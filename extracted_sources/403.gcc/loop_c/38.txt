static void
emit_prefetch_instructions (loop)
     struct loop *loop;
{
  int num_prefetches = 0;
  int num_real_prefetches = 0;
  int num_real_write_prefetches = 0;
  int ahead;
  int i;
  struct iv_class *bl;
  struct induction *iv;
  struct prefetch_info info[MAX_PREFETCHES];
  struct loop_ivs *ivs = LOOP_IVS (loop);

  if (!HAVE_prefetch)
    return;

  /* Consider only loops w/o calls.  When a call is done, the loop is probably
     slow enough to read the memory.  */
  if (PREFETCH_NO_CALL && LOOP_INFO (loop)->has_call)
    {
      if (loop_dump_stream)
	fprintf (loop_dump_stream, "Prefetch: ignoring loop - has call.\n");

      return;
    }

  if (PREFETCH_NO_LOW_LOOPCNT
      && LOOP_INFO (loop)->n_iterations
      && LOOP_INFO (loop)->n_iterations <= PREFETCH_LOW_LOOPCNT)
    {
      if (loop_dump_stream)
	fprintf (loop_dump_stream,
		 "Prefetch: ignoring loop - not enought iterations.\n");
      return;
    }

  /* Search all induction variables and pick those interesting for the prefetch
     machinery.  */
  for (bl = ivs->list; bl; bl = bl->next)
    {
      struct induction *biv = bl->biv, *biv1;
      int basestride = 0;

      biv1 = biv;

      /* Expect all BIVs to be executed in each iteration.  This makes our
	 analysis more conservative.  */
      while (biv1)
	{
	  /* Discard non-constant additions that we can't handle well yet, and
	     BIVs that are executed multiple times; such BIVs ought to be
	     handled in the nested loop.  We accept not_every_iteration BIVs,
	     since these only result in larger strides and make our
	     heuristics more conservative.
	     ??? What does the last sentence mean?  */
	  if (GET_CODE (biv->add_val) != CONST_INT)
	    {
	      if (loop_dump_stream)
		{
		  fprintf (loop_dump_stream,
			   "Prefetch: biv %i ignored: non-constant addition at insn %i:",
			   REGNO (biv->src_reg), INSN_UID (biv->insn));
		  print_rtl (loop_dump_stream, biv->add_val);
		  fprintf (loop_dump_stream, "\n");
		}
	      break;
	    }

	  if (biv->maybe_multiple)
	    {
	      if (loop_dump_stream)
		{
		  fprintf (loop_dump_stream,
			   "Prefetch: biv %i ignored: maybe_multiple at insn %i:",
			   REGNO (biv->src_reg), INSN_UID (biv->insn));
		  print_rtl (loop_dump_stream, biv->add_val);
		  fprintf (loop_dump_stream, "\n");
		}
	      break;
	    }

	  basestride += INTVAL (biv1->add_val);
	  biv1 = biv1->next_iv;
	}

      if (biv1 || !basestride)
	continue;

      for (iv = bl->giv; iv; iv = iv->next_iv)
	{
	  rtx address;
	  rtx temp;
	  HOST_WIDE_INT index = 0;
	  int add = 1;
	  HOST_WIDE_INT stride;
	  struct check_store_data d;
	  int size = GET_MODE_SIZE (GET_MODE (iv));

	  /* There are several reasons why an induction variable is not
	     interesting to us.  */
	  if (iv->giv_type != DEST_ADDR
	      /* We are interested only in constant stride memory references
		 in order to be able to compute density easily.  */
	      || GET_CODE (iv->mult_val) != CONST_INT
	      /* Don't handle reversed order prefetches, since they are usually
		 ineffective.  Later we may be able to reverse such BIVs.  */
	      || (PREFETCH_NO_REVERSE_ORDER
		  && (stride = INTVAL (iv->mult_val) * basestride) < 0)
	      /* Prefetching of accesses with such an extreme stride is probably
		 not worthwhile, either.  */
	      || (PREFETCH_NO_EXTREME_STRIDE
		  && stride > PREFETCH_EXTREME_STRIDE)
	      /* Ignore GIVs with varying add values; we can't predict the
		 value for the next iteration.  */
	      || !loop_invariant_p (loop, iv->add_val)
	      /* Ignore GIVs in the nested loops; they ought to have been
		 handled already.  */
	      || iv->maybe_multiple)
	    {
	      if (loop_dump_stream)
		fprintf (loop_dump_stream, "Prefetch: Ignoring giv at %i\n",
			 INSN_UID (iv->insn));
	      continue;
	    }

	  /* Determine the pointer to the basic array we are examining.  It is
	     the sum of the BIV's initial value and the GIV's add_val.  */
	  index = 0;

	  address = copy_rtx (iv->add_val);
	  temp = copy_rtx (bl->initial_value);

	  address = simplify_gen_binary (PLUS, Pmode, temp, address);
	  index = remove_constant_addition (&address);

	  index += size;
	  d.mem_write = 0;
	  d.mem_address = *iv->location;

	  /* When the GIV is not always executed, we might be better off by
	     not dirtying the cache pages.  */
	  if (PREFETCH_NOT_ALWAYS || iv->always_executed)
	    note_stores (PATTERN (iv->insn), check_store, &d);

	  /* Attempt to find another prefetch to the same array and see if we
	     can merge this one.  */
	  for (i = 0; i < num_prefetches; i++)
	    if (rtx_equal_for_prefetch_p (address, info[i].base_address)
		&& stride == info[i].stride)
	      {
		/* In case both access same array (same location
		   just with small difference in constant indexes), merge
		   the prefetches.  Just do the later and the earlier will
		   get prefetched from previous iteration.
		   4096 is artificial threshold.  It should not be too small,
		   but also not bigger than small portion of memory usually
		   traversed by single loop.  */
		if (index >= info[i].index && index - info[i].index < 4096)
		  {
		    info[i].write |= d.mem_write;
		    info[i].bytes_accesed += size;
		    info[i].index = index;
		    info[i].giv = iv;
		    info[i].class = bl;
		    info[num_prefetches].base_address = address;
		    add = 0;
		    break;
		  }

		if (index < info[i].index && info[i].index - index < 4096)
		  {
		    info[i].write |= d.mem_write;
		    info[i].bytes_accesed += size;
		    add = 0;
		    break;
		  }
	      }

	  /* Merging failed.  */
	  if (add)
	    {
	      info[num_prefetches].giv = iv;
	      info[num_prefetches].class = bl;
	      info[num_prefetches].index = index;
	      info[num_prefetches].stride = stride;
	      info[num_prefetches].base_address = address;
	      info[num_prefetches].write = d.mem_write;
	      info[num_prefetches].bytes_accesed = size;
	      num_prefetches++;
	      if (num_prefetches >= MAX_PREFETCHES)
		{
		  if (loop_dump_stream)
		    fprintf (loop_dump_stream,
			     "Maximal number of prefetches exceeded.\n");
		  return;
		}
	    }
	}
    }

  for (i = 0; i < num_prefetches; i++)
    {
      /* Attempt to calculate the number of bytes fetched by the loop.
	 Avoid overflow.  */
      if (LOOP_INFO (loop)->n_iterations
          && ((unsigned HOST_WIDE_INT) (0xffffffff / info[i].stride)
	      >= LOOP_INFO (loop)->n_iterations))
	info[i].total_bytes = info[i].stride * LOOP_INFO (loop)->n_iterations;
      else
	info[i].total_bytes = 0xffffffff;

      /* Prefetch is worthwhile only when the loads/stores are dense.  */
      if (PREFETCH_ONLY_DENSE_MEM
	  && info[i].bytes_accesed * 256 / info[i].stride > PREFETCH_DENSE_MEM
	  && (info[i].total_bytes / PREFETCH_BLOCK
	      >= PREFETCH_BLOCKS_BEFORE_LOOP_MIN))
	{
	  info[i].prefetch_before_loop = 1;
	  info[i].prefetch_in_loop
	    = (info[i].total_bytes / PREFETCH_BLOCK
	       > PREFETCH_BLOCKS_BEFORE_LOOP_MAX);
	}
      else
        info[i].prefetch_in_loop = 0, info[i].prefetch_before_loop = 0;

      if (info[i].prefetch_in_loop)
	{
	  num_real_prefetches += ((info[i].stride + PREFETCH_BLOCK - 1)
				  / PREFETCH_BLOCK);
	  if (info[i].write)
	    num_real_write_prefetches
	      += (info[i].stride + PREFETCH_BLOCK - 1) / PREFETCH_BLOCK;
	}
    }

  if (loop_dump_stream)
    {
      for (i = 0; i < num_prefetches; i++)
	{
	  fprintf (loop_dump_stream, "Prefetch insn %i address: ",
		   INSN_UID (info[i].giv->insn));
	  print_rtl (loop_dump_stream, info[i].base_address);
	  fprintf (loop_dump_stream, " Index: ");
	  fprintf (loop_dump_stream, HOST_WIDE_INT_PRINT_DEC, info[i].index);
	  fprintf (loop_dump_stream, " stride: ");
	  fprintf (loop_dump_stream, HOST_WIDE_INT_PRINT_DEC, info[i].stride);
	  fprintf (loop_dump_stream,
		   " density: %i%% total_bytes: %u%sin loop: %s before: %s\n",
		   (int) (info[i].bytes_accesed * 100 / info[i].stride),
		   info[i].total_bytes,
		   info[i].write ? " read/write " : " read only ",
		   info[i].prefetch_in_loop ? "yes" : "no",
		   info[i].prefetch_before_loop ? "yes" : "no");
	}

      fprintf (loop_dump_stream, "Real prefetches needed: %i (write: %i)\n",
	       num_real_prefetches, num_real_write_prefetches);
    }

  if (!num_real_prefetches)
    return;

  ahead = SIMULTANEOUS_PREFETCHES / num_real_prefetches;

  if (!ahead)
    return;

  for (i = 0; i < num_prefetches; i++)
    {
      if (info[i].prefetch_in_loop)
	{
	  int y;

	  for (y = 0; y < ((info[i].stride + PREFETCH_BLOCK - 1)
			   / PREFETCH_BLOCK); y++)
	    {
	      rtx loc = copy_rtx (*info[i].giv->location);
	      rtx insn;
	      int bytes_ahead = PREFETCH_BLOCK * (ahead + y);
	      rtx before_insn = info[i].giv->insn;
	      rtx prev_insn = PREV_INSN (info[i].giv->insn);
	      rtx seq;

	      /* We can save some effort by offsetting the address on
		 architectures with offsettable memory references.  */
	      if (offsettable_address_p (0, VOIDmode, loc))
		loc = plus_constant (loc, bytes_ahead);
	      else
		{
		  rtx reg = gen_reg_rtx (Pmode);
		  loop_iv_add_mult_emit_before (loop, loc, const1_rtx,
		      				GEN_INT (bytes_ahead), reg,
				  		0, before_insn);
		  loc = reg;
		}

	      start_sequence ();
	      /* Make sure the address operand is valid for prefetch.  */
	      if (! (*insn_data[(int)CODE_FOR_prefetch].operand[0].predicate)
		    (loc,
		     insn_data[(int)CODE_FOR_prefetch].operand[0].mode))
		loc = force_reg (Pmode, loc);
	      emit_insn (gen_prefetch (loc, GEN_INT (info[i].write),
		                       GEN_INT (3)));
	      seq = gen_sequence ();
	      end_sequence ();
	      emit_insn_before (seq, before_insn);

	      /* Check all insns emitted and record the new GIV
		 information.  */
	      insn = NEXT_INSN (prev_insn);
	      while (insn != before_insn)
		{
		  insn = check_insn_for_givs (loop, insn,
					      info[i].giv->always_executed,
					      info[i].giv->maybe_multiple);
		  insn = NEXT_INSN (insn);
		}
	    }
	}

      if (info[i].prefetch_before_loop)
	{
	  int y;

	  /* Emit INSNs before the loop to fetch the first cache lines.  */
	  for (y = 0;
	       (!info[i].prefetch_in_loop || y < ahead)
	       && y * PREFETCH_BLOCK < (int) info[i].total_bytes; y ++)
	    {
	      rtx reg = gen_reg_rtx (Pmode);
	      rtx loop_start = loop->start;
	      rtx add_val = simplify_gen_binary (PLUS, Pmode,
						 info[i].giv->add_val,
						 GEN_INT (y * PREFETCH_BLOCK));

	      loop_iv_add_mult_emit_before (loop, info[i].class->initial_value,
					    info[i].giv->mult_val,
				            add_val, reg, 0, loop_start);
	      emit_insn_before (gen_prefetch (reg, GEN_INT (info[i].write),
					      GEN_INT (3)),
				loop_start);
	    }
	}
    }

  return;
}
