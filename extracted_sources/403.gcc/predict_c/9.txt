void
estimate_probability (loops_info)
     struct loops *loops_info;
{
  sbitmap *dominators, *post_dominators;
  int i;
  int found_noreturn = 0;

  dominators = sbitmap_vector_alloc (n_basic_blocks, n_basic_blocks);
  post_dominators = sbitmap_vector_alloc (n_basic_blocks, n_basic_blocks);
  calculate_dominance_info (NULL, dominators, CDI_DOMINATORS);
  calculate_dominance_info (NULL, post_dominators, CDI_POST_DOMINATORS);

  /* Try to predict out blocks in a loop that are not part of a
     natural loop.  */
  for (i = 0; i < loops_info->num; i++)
    {
      int j;
      int exits;
      struct loop *loop = &loops_info->array[i];

      flow_loop_scan (loops_info, loop, LOOP_EXIT_EDGES);
      exits = loop->num_exits;

      for (j = loop->first->index; j <= loop->last->index; ++j)
	if (TEST_BIT (loop->nodes, j))
	  {
	    int header_found = 0;
	    edge e;

	    /* Loop branch heuristics - predict an edge back to a
	       loop's head as taken.  */
	    for (e = BASIC_BLOCK(j)->succ; e; e = e->succ_next)
	      if (e->dest == loop->header
		  && e->src == loop->latch)
		{
		  header_found = 1;
		  predict_edge_def (e, PRED_LOOP_BRANCH, TAKEN);
		}

	    /* Loop exit heuristics - predict an edge exiting the loop if the
	       conditinal has no loop header successors as not taken.  */
	    if (!header_found)
	      for (e = BASIC_BLOCK(j)->succ; e; e = e->succ_next)
		if (e->dest->index < 0
		    || !TEST_BIT (loop->nodes, e->dest->index))
		  predict_edge
		    (e, PRED_LOOP_EXIT,
		     (REG_BR_PROB_BASE
		      - predictor_info [(int) PRED_LOOP_EXIT].hitrate)
		     / exits);
	  }
    }

  /* Attempt to predict conditional jumps using a number of heuristics.  */
  for (i = 0; i < n_basic_blocks; i++)
    {
      basic_block bb = BASIC_BLOCK (i);
      rtx last_insn = bb->end;
      rtx cond, earliest;
      edge e;

      /* If block has no successor, predict all possible paths to it as
         improbable, as the block contains a call to a noreturn function and
         thus can be executed only once.  */
      if (bb->succ == NULL && !found_noreturn)
	{
	  int y;

	  /* ??? Postdominator claims each noreturn block to be postdominated
	     by each, so we need to run only once.  This needs to be changed
	     once postdominace algorithm is updated to say something more
	     sane.  */
	  found_noreturn = 1;
	  for (y = 0; y < n_basic_blocks; y++)
	    if (!TEST_BIT (post_dominators[y], i))
	      for (e = BASIC_BLOCK (y)->succ; e; e = e->succ_next)
		if (e->dest->index >= 0
		    && TEST_BIT (post_dominators[e->dest->index], i))
		  predict_edge_def (e, PRED_NORETURN, NOT_TAKEN);
	}

      if (GET_CODE (last_insn) != JUMP_INSN || ! any_condjump_p (last_insn))
	continue;

      for (e = bb->succ; e; e = e->succ_next)
	{
	  /* Predict edges to blocks that return immediately to be
	     improbable.  These are usually used to signal error states.  */
	  if (e->dest == EXIT_BLOCK_PTR
	      || (e->dest->succ && !e->dest->succ->succ_next
		  && e->dest->succ->dest == EXIT_BLOCK_PTR))
	    predict_edge_def (e, PRED_ERROR_RETURN, NOT_TAKEN);

	  /* Look for block we are guarding (ie we dominate it,
	     but it doesn't postdominate us).  */
	  if (e->dest != EXIT_BLOCK_PTR && e->dest != bb
	      && TEST_BIT (dominators[e->dest->index], e->src->index)
	      && !TEST_BIT (post_dominators[e->src->index], e->dest->index))
	    {
	      rtx insn;

	      /* The call heuristic claims that a guarded function call
		 is improbable.  This is because such calls are often used
		 to signal exceptional situations such as printing error
		 messages.  */
	      for (insn = e->dest->head; insn != NEXT_INSN (e->dest->end);
		   insn = NEXT_INSN (insn))
		if (GET_CODE (insn) == CALL_INSN
		    /* Constant and pure calls are hardly used to signalize
		       something exceptional.  */
		    && ! CONST_OR_PURE_CALL_P (insn))
		  {
		    predict_edge_def (e, PRED_CALL, NOT_TAKEN);
		    break;
		  }
	    }
	}

      cond = get_condition (last_insn, &earliest);
      if (! cond)
	continue;

      /* Try "pointer heuristic."
	 A comparison ptr == 0 is predicted as false.
	 Similarly, a comparison ptr1 == ptr2 is predicted as false.  */
      if (GET_RTX_CLASS (GET_CODE (cond)) == '<'
	  && ((REG_P (XEXP (cond, 0)) && REG_POINTER (XEXP (cond, 0)))
	      || (REG_P (XEXP (cond, 1)) && REG_POINTER (XEXP (cond, 1)))))
	{
	  if (GET_CODE (cond) == EQ)
	    predict_insn_def (last_insn, PRED_POINTER, NOT_TAKEN);
	  else if (GET_CODE (cond) == NE)
	    predict_insn_def (last_insn, PRED_POINTER, TAKEN);
	}
      else

      /* Try "opcode heuristic."
	 EQ tests are usually false and NE tests are usually true. Also,
	 most quantities are positive, so we can make the appropriate guesses
	 about signed comparisons against zero.  */
	switch (GET_CODE (cond))
	  {
	  case CONST_INT:
	    /* Unconditional branch.  */
	    predict_insn_def (last_insn, PRED_UNCONDITIONAL,
			      cond == const0_rtx ? NOT_TAKEN : TAKEN);
	    break;

	  case EQ:
	  case UNEQ:
	    /* Floating point comparisons appears to behave in a very
	       inpredictable way because of special role of = tests in
	       FP code.  */
	    if (FLOAT_MODE_P (GET_MODE (XEXP (cond, 0))))
	      ;
	    /* Comparisons with 0 are often used for booleans and there is
	       nothing usefull to predict about them.  */
	    else if (XEXP (cond, 1) == const0_rtx
		     || XEXP (cond, 0) == const0_rtx)
	      ;
	    else
	      predict_insn_def (last_insn, PRED_OPCODE_NONEQUAL, NOT_TAKEN);
	    break;

	  case NE:
	  case LTGT:
	    /* Floating point comparisons appears to behave in a very
	       inpredictable way because of special role of = tests in
	       FP code.  */
	    if (FLOAT_MODE_P (GET_MODE (XEXP (cond, 0))))
	      ;
	    /* Comparisons with 0 are often used for booleans and there is
	       nothing usefull to predict about them.  */
	    else if (XEXP (cond, 1) == const0_rtx
		     || XEXP (cond, 0) == const0_rtx)
	      ;
	    else
	      predict_insn_def (last_insn, PRED_OPCODE_NONEQUAL, TAKEN);
	    break;

	  case ORDERED:
	    predict_insn_def (last_insn, PRED_FPOPCODE, TAKEN);
	    break;

	  case UNORDERED:
	    predict_insn_def (last_insn, PRED_FPOPCODE, NOT_TAKEN);
	    break;

	  case LE:
	  case LT:
	    if (XEXP (cond, 1) == const0_rtx || XEXP (cond, 1) == const1_rtx
		|| XEXP (cond, 1) == constm1_rtx)
	      predict_insn_def (last_insn, PRED_OPCODE_POSITIVE, NOT_TAKEN);
	    break;

	  case GE:
	  case GT:
	    if (XEXP (cond, 1) == const0_rtx || XEXP (cond, 1) == const1_rtx
		|| XEXP (cond, 1) == constm1_rtx)
	      predict_insn_def (last_insn, PRED_OPCODE_POSITIVE, TAKEN);
	    break;

	  default:
	    break;
	  }
    }

  /* Attach the combined probability to each conditional jump.  */
  for (i = 0; i < n_basic_blocks; i++)
    if (GET_CODE (BLOCK_END (i)) == JUMP_INSN
	&& any_condjump_p (BLOCK_END (i)))
      combine_predictions_for_insn (BLOCK_END (i), BASIC_BLOCK (i));

  sbitmap_vector_free (post_dominators);
  sbitmap_vector_free (dominators);

  estimate_bb_frequencies (loops_info);
}
