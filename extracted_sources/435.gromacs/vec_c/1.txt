void vecrecip(real in[],real out[],int n)
{
/* No vectorized 1/x on alpha chips */

/* On IBM we should definitely use vectorized MASS if present. */ 
#ifdef HAVE_LIBMASSV_ANY
#  ifdef DOUBLE
   vrec(out,in,&n);
#  else /* SINGLE */
   vsrec(out,in,&n);
#  endif 
#else /* not IBM with MASS */


#ifdef SOFTWARE_RECIP
  const real  two=2.0;
  t_convert   result,bit_pattern;
  unsigned int exp,fract;
  float       lu,x;
#ifdef DOUBLE
  real        y;
#endif
#endif /* SOFTWARE_RECIP */
  int i;

#if (defined USE_X86_SSE_AND_3DNOW && !defined DOUBLE)
  static bool bFirst=TRUE;
  static int cpu_capabilities;

  if(bFirst) {
    cpu_capabilities=detect_cpu(NULL);
    bFirst=FALSE;
  }
  if((cpu_capabilities & X86_SSE_SUPPORT) && !((unsigned long int)in & 0x1f) && !((unsigned long int)out & 0x1f)) /* SSE data must be cache aligned */
    vecrecip_sse(in,out,n);
  else if(cpu_capabilities & X86_3DNOW_SUPPORT)
    vecrecip_3dnow(in,out,n);
  else
#endif /* no x86 optimizations */
#if (defined USE_X86_SSE2 && defined DOUBLE)
  static bool bFirst=TRUE;
  static int cpu_capabilities;

  if(bFirst) {
    cpu_capabilities=detect_cpu(NULL);
    bFirst=FALSE;
  }
  if((cpu_capabilities & X86_SSE2_SUPPORT) && !((unsigned long int)in & 0x1f) && !((unsigned long int)out & 0x1f)) /* SSE2 data must be cache aligned */
    vecrecip_sse2(in,out,n);
  else
#endif /* no sse2 optimizations */
#ifdef SOFTWARE_RECIP
    for(i=0;i<n;i++) {
      x=in[i];
      bit_pattern.fval=x;
      exp   = EXP_ADDR(bit_pattern.bval);
      fract = FRACT_ADDR(bit_pattern.bval);
      result.bval=crecipexptab[exp] | crecipfracttab[fract];
      lu    = result.fval;
      
#ifdef DOUBLE
      y=lu*(two-x*lu);
      out[i]=y*(two-x*y);
#else
      out[i]=lu*(two-x*lu);
#endif
    }
#else /* No gmx recip */ 
    for(i=0;i<n;i++)
      out[i]=1.0f/(in[i]);
#endif /* SOFTWARE_RECIP */
#endif
}
