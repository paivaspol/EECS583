define void @inl1030(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc) #0 {
entry:
  %0 = load i32* %iinr, align 4, !tbaa !0
  %idxprom = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds float* %charge, i64 %idxprom
  %1 = load float* %arrayidx1, align 4, !tbaa !3
  %add = add nsw i32 %0, 1
  %idxprom2 = sext i32 %add to i64
  %arrayidx3 = getelementptr inbounds float* %charge, i64 %idxprom2
  %2 = load float* %arrayidx3, align 4, !tbaa !3
  %mul = fmul float %1, %facel
  %mul4 = fmul float %1, %mul
  %mul6 = fmul float %mul, %2
  %mul7 = fmul float %2, %facel
  %mul8 = fmul float %2, %mul7
  %cmp861 = icmp sgt i32 %nri, 0
  br i1 %cmp861, label %for.body, label %for.end463

for.body:                                         ; preds = %entry, %for.end.for.body_crit_edge
  %3 = phi i32 [ %.pre, %for.end.for.body_crit_edge ], [ %0, %entry ]
  %indvars.iv863 = phi i64 [ %indvars.iv.next864, %for.end.for.body_crit_edge ], [ 0, %entry ]
  %arrayidx10 = getelementptr inbounds i32* %shift, i64 %indvars.iv863
  %4 = load i32* %arrayidx10, align 4, !tbaa !0
  %mul11 = mul nsw i32 %4, 3
  %idxprom12 = sext i32 %mul11 to i64
  %arrayidx13 = getelementptr inbounds float* %shiftvec, i64 %idxprom12
  %5 = load float* %arrayidx13, align 4, !tbaa !3
  %add14 = add nsw i32 %mul11, 1
  %idxprom15 = sext i32 %add14 to i64
  %arrayidx16 = getelementptr inbounds float* %shiftvec, i64 %idxprom15
  %6 = load float* %arrayidx16, align 4, !tbaa !3
  %add17 = add nsw i32 %mul11, 2
  %idxprom18 = sext i32 %add17 to i64
  %arrayidx19 = getelementptr inbounds float* %shiftvec, i64 %idxprom18
  %7 = load float* %arrayidx19, align 4, !tbaa !3
  %mul22 = mul nsw i32 %3, 3
  %arrayidx24 = getelementptr inbounds i32* %jindex, i64 %indvars.iv863
  %8 = load i32* %arrayidx24, align 4, !tbaa !0
  %indvars.iv.next864 = add i64 %indvars.iv863, 1
  %arrayidx27 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next864
  %9 = load i32* %arrayidx27, align 4, !tbaa !0
  %idxprom28 = sext i32 %mul22 to i64
  %arrayidx29 = getelementptr inbounds float* %pos, i64 %idxprom28
  %10 = load float* %arrayidx29, align 4, !tbaa !3
  %add30 = fadd float %5, %10
  %add31 = add nsw i32 %mul22, 1
  %idxprom32 = sext i32 %add31 to i64
  %arrayidx33 = getelementptr inbounds float* %pos, i64 %idxprom32
  %11 = load float* %arrayidx33, align 4, !tbaa !3
  %add34 = fadd float %6, %11
  %add35 = add nsw i32 %mul22, 2
  %idxprom36 = sext i32 %add35 to i64
  %arrayidx37 = getelementptr inbounds float* %pos, i64 %idxprom36
  %12 = load float* %arrayidx37, align 4, !tbaa !3
  %add38 = fadd float %7, %12
  %add39 = add nsw i32 %mul22, 3
  %idxprom40 = sext i32 %add39 to i64
  %arrayidx41 = getelementptr inbounds float* %pos, i64 %idxprom40
  %13 = load float* %arrayidx41, align 4, !tbaa !3
  %add42 = fadd float %5, %13
  %add43 = add nsw i32 %mul22, 4
  %idxprom44 = sext i32 %add43 to i64
  %arrayidx45 = getelementptr inbounds float* %pos, i64 %idxprom44
  %14 = load float* %arrayidx45, align 4, !tbaa !3
  %add46 = fadd float %6, %14
  %add47 = add nsw i32 %mul22, 5
  %idxprom48 = sext i32 %add47 to i64
  %arrayidx49 = getelementptr inbounds float* %pos, i64 %idxprom48
  %15 = load float* %arrayidx49, align 4, !tbaa !3
  %add50 = fadd float %7, %15
  %add51 = add nsw i32 %mul22, 6
  %idxprom52 = sext i32 %add51 to i64
  %arrayidx53 = getelementptr inbounds float* %pos, i64 %idxprom52
  %16 = load float* %arrayidx53, align 4, !tbaa !3
  %add54 = fadd float %5, %16
  %add55 = add nsw i32 %mul22, 7
  %idxprom56 = sext i32 %add55 to i64
  %arrayidx57 = getelementptr inbounds float* %pos, i64 %idxprom56
  %17 = load float* %arrayidx57, align 4, !tbaa !3
  %add58 = fadd float %6, %17
  %add59 = add nsw i32 %mul22, 8
  %idxprom60 = sext i32 %add59 to i64
  %arrayidx61 = getelementptr inbounds float* %pos, i64 %idxprom60
  %18 = load float* %arrayidx61, align 4, !tbaa !3
  %add62 = fadd float %7, %18
  %cmp64840 = icmp slt i32 %8, %9
  br i1 %cmp64840, label %for.body65.lr.ph, label %for.end

for.body65.lr.ph:                                 ; preds = %for.body
  %19 = sext i32 %8 to i64
  br label %for.body65

for.body65:                                       ; preds = %for.body65.lr.ph, %for.body65
  %indvars.iv = phi i64 [ %19, %for.body65.lr.ph ], [ %indvars.iv.next, %for.body65 ]
  %vctot.0850 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add349, %for.body65 ]
  %fix1.0849 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add249, %for.body65 ]
  %fiy1.0848 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add250, %for.body65 ]
  %fiz1.0847 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add251, %for.body65 ]
  %fix2.0846 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add297, %for.body65 ]
  %fiy2.0845 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add298, %for.body65 ]
  %fiz2.0844 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add299, %for.body65 ]
  %fix3.0843 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add353, %for.body65 ]
  %fiy3.0842 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add354, %for.body65 ]
  %fiz3.0841 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add355, %for.body65 ]
  %arrayidx67 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %20 = load i32* %arrayidx67, align 4, !tbaa !0
  %mul68 = mul nsw i32 %20, 3
  %idxprom69 = sext i32 %mul68 to i64
  %arrayidx70 = getelementptr inbounds float* %pos, i64 %idxprom69
  %21 = load float* %arrayidx70, align 4, !tbaa !3
  %add71 = add nsw i32 %mul68, 1
  %idxprom72 = sext i32 %add71 to i64
  %arrayidx73 = getelementptr inbounds float* %pos, i64 %idxprom72
  %22 = load float* %arrayidx73, align 4, !tbaa !3
  %add74 = add nsw i32 %mul68, 2
  %idxprom75 = sext i32 %add74 to i64
  %arrayidx76 = getelementptr inbounds float* %pos, i64 %idxprom75
  %23 = load float* %arrayidx76, align 4, !tbaa !3
  %add77 = add nsw i32 %mul68, 3
  %idxprom78 = sext i32 %add77 to i64
  %arrayidx79 = getelementptr inbounds float* %pos, i64 %idxprom78
  %24 = load float* %arrayidx79, align 4, !tbaa !3
  %add80 = add nsw i32 %mul68, 4
  %idxprom81 = sext i32 %add80 to i64
  %arrayidx82 = getelementptr inbounds float* %pos, i64 %idxprom81
  %25 = load float* %arrayidx82, align 4, !tbaa !3
  %add83 = add nsw i32 %mul68, 5
  %idxprom84 = sext i32 %add83 to i64
  %arrayidx85 = getelementptr inbounds float* %pos, i64 %idxprom84
  %26 = load float* %arrayidx85, align 4, !tbaa !3
  %add86 = add nsw i32 %mul68, 6
  %idxprom87 = sext i32 %add86 to i64
  %arrayidx88 = getelementptr inbounds float* %pos, i64 %idxprom87
  %27 = load float* %arrayidx88, align 4, !tbaa !3
  %add89 = add nsw i32 %mul68, 7
  %idxprom90 = sext i32 %add89 to i64
  %arrayidx91 = getelementptr inbounds float* %pos, i64 %idxprom90
  %28 = load float* %arrayidx91, align 4, !tbaa !3
  %add92 = add nsw i32 %mul68, 8
  %idxprom93 = sext i32 %add92 to i64
  %arrayidx94 = getelementptr inbounds float* %pos, i64 %idxprom93
  %29 = load float* %arrayidx94, align 4, !tbaa !3
  %sub = fsub float %add30, %21
  %sub95 = fsub float %add34, %22
  %sub96 = fsub float %add38, %23
  %mul97 = fmul float %sub, %sub
  %mul98 = fmul float %sub95, %sub95
  %add99 = fadd float %mul97, %mul98
  %mul100 = fmul float %sub96, %sub96
  %add101 = fadd float %add99, %mul100
  %sub102 = fsub float %add30, %24
  %sub103 = fsub float %add34, %25
  %sub104 = fsub float %add38, %26
  %mul105 = fmul float %sub102, %sub102
  %mul106 = fmul float %sub103, %sub103
  %add107 = fadd float %mul105, %mul106
  %mul108 = fmul float %sub104, %sub104
  %add109 = fadd float %add107, %mul108
  %sub110 = fsub float %add30, %27
  %sub111 = fsub float %add34, %28
  %sub112 = fsub float %add38, %29
  %mul113 = fmul float %sub110, %sub110
  %mul114 = fmul float %sub111, %sub111
  %add115 = fadd float %mul113, %mul114
  %mul116 = fmul float %sub112, %sub112
  %add117 = fadd float %add115, %mul116
  %sub118 = fsub float %add42, %21
  %sub119 = fsub float %add46, %22
  %sub120 = fsub float %add50, %23
  %mul121 = fmul float %sub118, %sub118
  %mul122 = fmul float %sub119, %sub119
  %add123 = fadd float %mul121, %mul122
  %mul124 = fmul float %sub120, %sub120
  %add125 = fadd float %add123, %mul124
  %sub126 = fsub float %add42, %24
  %sub127 = fsub float %add46, %25
  %sub128 = fsub float %add50, %26
  %mul129 = fmul float %sub126, %sub126
  %mul130 = fmul float %sub127, %sub127
  %add131 = fadd float %mul129, %mul130
  %mul132 = fmul float %sub128, %sub128
  %add133 = fadd float %add131, %mul132
  %sub134 = fsub float %add42, %27
  %sub135 = fsub float %add46, %28
  %sub136 = fsub float %add50, %29
  %mul137 = fmul float %sub134, %sub134
  %mul138 = fmul float %sub135, %sub135
  %add139 = fadd float %mul137, %mul138
  %mul140 = fmul float %sub136, %sub136
  %add141 = fadd float %add139, %mul140
  %sub142 = fsub float %add54, %21
  %sub143 = fsub float %add58, %22
  %sub144 = fsub float %add62, %23
  %mul145 = fmul float %sub142, %sub142
  %mul146 = fmul float %sub143, %sub143
  %add147 = fadd float %mul145, %mul146
  %mul148 = fmul float %sub144, %sub144
  %add149 = fadd float %add147, %mul148
  %sub150 = fsub float %add54, %24
  %sub151 = fsub float %add58, %25
  %sub152 = fsub float %add62, %26
  %mul153 = fmul float %sub150, %sub150
  %mul154 = fmul float %sub151, %sub151
  %add155 = fadd float %mul153, %mul154
  %mul156 = fmul float %sub152, %sub152
  %add157 = fadd float %add155, %mul156
  %sub158 = fsub float %add54, %27
  %sub159 = fsub float %add58, %28
  %sub160 = fsub float %add62, %29
  %mul161 = fmul float %sub158, %sub158
  %mul162 = fmul float %sub159, %sub159
  %add163 = fadd float %mul161, %mul162
  %mul164 = fmul float %sub160, %sub160
  %add165 = fadd float %add163, %mul164
  %conv = fpext float %add101 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv166 = fptrunc double %div to float
  %conv167 = fpext float %add125 to double
  %call168 = tail call double @sqrt(double %conv167) #2
  %div169 = fdiv double 1.000000e+00, %call168
  %conv170 = fptrunc double %div169 to float
  %conv171 = fpext float %add149 to double
  %call172 = tail call double @sqrt(double %conv171) #2
  %div173 = fdiv double 1.000000e+00, %call172
  %conv174 = fptrunc double %div173 to float
  %conv175 = fpext float %add109 to double
  %call176 = tail call double @sqrt(double %conv175) #2
  %div177 = fdiv double 1.000000e+00, %call176
  %conv178 = fptrunc double %div177 to float
  %conv179 = fpext float %add133 to double
  %call180 = tail call double @sqrt(double %conv179) #2
  %div181 = fdiv double 1.000000e+00, %call180
  %conv182 = fptrunc double %div181 to float
  %conv183 = fpext float %add157 to double
  %call184 = tail call double @sqrt(double %conv183) #2
  %div185 = fdiv double 1.000000e+00, %call184
  %conv186 = fptrunc double %div185 to float
  %conv187 = fpext float %add117 to double
  %call188 = tail call double @sqrt(double %conv187) #2
  %div189 = fdiv double 1.000000e+00, %call188
  %conv190 = fptrunc double %div189 to float
  %conv191 = fpext float %add141 to double
  %call192 = tail call double @sqrt(double %conv191) #2
  %div193 = fdiv double 1.000000e+00, %call192
  %conv194 = fptrunc double %div193 to float
  %conv195 = fpext float %add165 to double
  %call196 = tail call double @sqrt(double %conv195) #2
  %div197 = fdiv double 1.000000e+00, %call196
  %conv198 = fptrunc double %div197 to float
  %mul199 = fmul float %conv166, %conv166
  %mul200 = fmul float %mul4, %conv166
  %mul201 = fmul float %mul200, %mul199
  %add202 = fadd float %vctot.0850, %mul200
  %mul203 = fmul float %sub, %mul201
  %mul204 = fmul float %sub95, %mul201
  %mul205 = fmul float %sub96, %mul201
  %add206 = fadd float %fix1.0849, %mul203
  %add207 = fadd float %fiy1.0848, %mul204
  %add208 = fadd float %fiz1.0847, %mul205
  %arrayidx210 = getelementptr inbounds float* %faction, i64 %idxprom69
  %30 = load float* %arrayidx210, align 4, !tbaa !3
  %sub211 = fsub float %30, %mul203
  %arrayidx214 = getelementptr inbounds float* %faction, i64 %idxprom72
  %31 = load float* %arrayidx214, align 4, !tbaa !3
  %sub215 = fsub float %31, %mul204
  %arrayidx218 = getelementptr inbounds float* %faction, i64 %idxprom75
  %32 = load float* %arrayidx218, align 4, !tbaa !3
  %sub219 = fsub float %32, %mul205
  %mul220 = fmul float %conv178, %conv178
  %mul221 = fmul float %mul6, %conv178
  %mul222 = fmul float %mul221, %mul220
  %add223 = fadd float %add202, %mul221
  %mul224 = fmul float %sub102, %mul222
  %mul225 = fmul float %sub103, %mul222
  %mul226 = fmul float %sub104, %mul222
  %add227 = fadd float %add206, %mul224
  %add228 = fadd float %add207, %mul225
  %add229 = fadd float %add208, %mul226
  %arrayidx232 = getelementptr inbounds float* %faction, i64 %idxprom78
  %33 = load float* %arrayidx232, align 4, !tbaa !3
  %sub233 = fsub float %33, %mul224
  %arrayidx236 = getelementptr inbounds float* %faction, i64 %idxprom81
  %34 = load float* %arrayidx236, align 4, !tbaa !3
  %sub237 = fsub float %34, %mul225
  %arrayidx240 = getelementptr inbounds float* %faction, i64 %idxprom84
  %35 = load float* %arrayidx240, align 4, !tbaa !3
  %sub241 = fsub float %35, %mul226
  %mul242 = fmul float %conv190, %conv190
  %mul243 = fmul float %mul6, %conv190
  %mul244 = fmul float %mul243, %mul242
  %add245 = fadd float %add223, %mul243
  %mul246 = fmul float %sub110, %mul244
  %mul247 = fmul float %sub111, %mul244
  %mul248 = fmul float %sub112, %mul244
  %add249 = fadd float %add227, %mul246
  %add250 = fadd float %add228, %mul247
  %add251 = fadd float %add229, %mul248
  %arrayidx254 = getelementptr inbounds float* %faction, i64 %idxprom87
  %36 = load float* %arrayidx254, align 4, !tbaa !3
  %sub255 = fsub float %36, %mul246
  %arrayidx258 = getelementptr inbounds float* %faction, i64 %idxprom90
  %37 = load float* %arrayidx258, align 4, !tbaa !3
  %sub259 = fsub float %37, %mul247
  %arrayidx262 = getelementptr inbounds float* %faction, i64 %idxprom93
  %38 = load float* %arrayidx262, align 4, !tbaa !3
  %sub263 = fsub float %38, %mul248
  %mul264 = fmul float %conv170, %conv170
  %mul265 = fmul float %mul6, %conv170
  %mul266 = fmul float %mul265, %mul264
  %add267 = fadd float %mul265, %add245
  %mul268 = fmul float %sub118, %mul266
  %mul269 = fmul float %sub119, %mul266
  %mul270 = fmul float %sub120, %mul266
  %add271 = fadd float %fix2.0846, %mul268
  %add272 = fadd float %fiy2.0845, %mul269
  %add273 = fadd float %fiz2.0844, %mul270
  %sub274 = fsub float %sub211, %mul268
  %sub275 = fsub float %sub215, %mul269
  %sub276 = fsub float %sub219, %mul270
  %mul277 = fmul float %conv182, %conv182
  %mul278 = fmul float %mul8, %conv182
  %mul279 = fmul float %mul278, %mul277
  %add280 = fadd float %mul278, %add267
  %mul281 = fmul float %sub126, %mul279
  %mul282 = fmul float %sub127, %mul279
  %mul283 = fmul float %sub128, %mul279
  %add284 = fadd float %add271, %mul281
  %add285 = fadd float %add272, %mul282
  %add286 = fadd float %add273, %mul283
  %sub287 = fsub float %sub233, %mul281
  %sub288 = fsub float %sub237, %mul282
  %sub289 = fsub float %sub241, %mul283
  %mul290 = fmul float %conv194, %conv194
  %mul291 = fmul float %mul8, %conv194
  %mul292 = fmul float %mul291, %mul290
  %add293 = fadd float %mul291, %add280
  %mul294 = fmul float %sub134, %mul292
  %mul295 = fmul float %sub135, %mul292
  %mul296 = fmul float %sub136, %mul292
  %add297 = fadd float %add284, %mul294
  %add298 = fadd float %add285, %mul295
  %add299 = fadd float %add286, %mul296
  %sub300 = fsub float %sub255, %mul294
  %sub301 = fsub float %sub259, %mul295
  %sub302 = fsub float %sub263, %mul296
  %mul303 = fmul float %conv174, %conv174
  %mul304 = fmul float %mul6, %conv174
  %mul305 = fmul float %mul304, %mul303
  %add306 = fadd float %mul304, %add293
  %mul307 = fmul float %sub142, %mul305
  %mul308 = fmul float %sub143, %mul305
  %mul309 = fmul float %sub144, %mul305
  %add310 = fadd float %fix3.0843, %mul307
  %add311 = fadd float %fiy3.0842, %mul308
  %add312 = fadd float %fiz3.0841, %mul309
  %sub313 = fsub float %sub274, %mul307
  store float %sub313, float* %arrayidx210, align 4, !tbaa !3
  %sub316 = fsub float %sub275, %mul308
  store float %sub316, float* %arrayidx214, align 4, !tbaa !3
  %sub320 = fsub float %sub276, %mul309
  store float %sub320, float* %arrayidx218, align 4, !tbaa !3
  %mul324 = fmul float %conv186, %conv186
  %mul325 = fmul float %mul8, %conv186
  %mul326 = fmul float %mul325, %mul324
  %add327 = fadd float %mul325, %add306
  %mul328 = fmul float %sub150, %mul326
  %mul329 = fmul float %sub151, %mul326
  %mul330 = fmul float %sub152, %mul326
  %add331 = fadd float %add310, %mul328
  %add332 = fadd float %add311, %mul329
  %add333 = fadd float %add312, %mul330
  %sub334 = fsub float %sub287, %mul328
  store float %sub334, float* %arrayidx232, align 4, !tbaa !3
  %sub338 = fsub float %sub288, %mul329
  store float %sub338, float* %arrayidx236, align 4, !tbaa !3
  %sub342 = fsub float %sub289, %mul330
  store float %sub342, float* %arrayidx240, align 4, !tbaa !3
  %mul346 = fmul float %conv198, %conv198
  %mul347 = fmul float %mul8, %conv198
  %mul348 = fmul float %mul347, %mul346
  %add349 = fadd float %mul347, %add327
  %mul350 = fmul float %sub158, %mul348
  %mul351 = fmul float %sub159, %mul348
  %mul352 = fmul float %sub160, %mul348
  %add353 = fadd float %add331, %mul350
  %add354 = fadd float %add332, %mul351
  %add355 = fadd float %add333, %mul352
  %sub356 = fsub float %sub300, %mul350
  store float %sub356, float* %arrayidx254, align 4, !tbaa !3
  %sub360 = fsub float %sub301, %mul351
  store float %sub360, float* %arrayidx258, align 4, !tbaa !3
  %sub364 = fsub float %sub302, %mul352
  store float %sub364, float* %arrayidx262, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %39 = trunc i64 %indvars.iv.next to i32
  %cmp64 = icmp slt i32 %39, %9
  br i1 %cmp64, label %for.body65, label %for.end

for.end:                                          ; preds = %for.body65, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add349, %for.body65 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add249, %for.body65 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add250, %for.body65 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add251, %for.body65 ]
  %fix2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add297, %for.body65 ]
  %fiy2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add298, %for.body65 ]
  %fiz2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add299, %for.body65 ]
  %fix3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add353, %for.body65 ]
  %fiy3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add354, %for.body65 ]
  %fiz3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add355, %for.body65 ]
  %arrayidx369 = getelementptr inbounds float* %faction, i64 %idxprom28
  %40 = load float* %arrayidx369, align 4, !tbaa !3
  %add370 = fadd float %fix1.0.lcssa, %40
  store float %add370, float* %arrayidx369, align 4, !tbaa !3
  %arrayidx375 = getelementptr inbounds float* %faction, i64 %idxprom32
  %41 = load float* %arrayidx375, align 4, !tbaa !3
  %add376 = fadd float %fiy1.0.lcssa, %41
  store float %add376, float* %arrayidx375, align 4, !tbaa !3
  %arrayidx382 = getelementptr inbounds float* %faction, i64 %idxprom36
  %42 = load float* %arrayidx382, align 4, !tbaa !3
  %add383 = fadd float %fiz1.0.lcssa, %42
  store float %add383, float* %arrayidx382, align 4, !tbaa !3
  %arrayidx389 = getelementptr inbounds float* %faction, i64 %idxprom40
  %43 = load float* %arrayidx389, align 4, !tbaa !3
  %add390 = fadd float %fix2.0.lcssa, %43
  store float %add390, float* %arrayidx389, align 4, !tbaa !3
  %arrayidx396 = getelementptr inbounds float* %faction, i64 %idxprom44
  %44 = load float* %arrayidx396, align 4, !tbaa !3
  %add397 = fadd float %fiy2.0.lcssa, %44
  store float %add397, float* %arrayidx396, align 4, !tbaa !3
  %arrayidx403 = getelementptr inbounds float* %faction, i64 %idxprom48
  %45 = load float* %arrayidx403, align 4, !tbaa !3
  %add404 = fadd float %fiz2.0.lcssa, %45
  store float %add404, float* %arrayidx403, align 4, !tbaa !3
  %arrayidx410 = getelementptr inbounds float* %faction, i64 %idxprom52
  %46 = load float* %arrayidx410, align 4, !tbaa !3
  %add411 = fadd float %fix3.0.lcssa, %46
  store float %add411, float* %arrayidx410, align 4, !tbaa !3
  %arrayidx417 = getelementptr inbounds float* %faction, i64 %idxprom56
  %47 = load float* %arrayidx417, align 4, !tbaa !3
  %add418 = fadd float %fiy3.0.lcssa, %47
  store float %add418, float* %arrayidx417, align 4, !tbaa !3
  %arrayidx424 = getelementptr inbounds float* %faction, i64 %idxprom60
  %48 = load float* %arrayidx424, align 4, !tbaa !3
  %add425 = fadd float %fiz3.0.lcssa, %48
  store float %add425, float* %arrayidx424, align 4, !tbaa !3
  %arrayidx430 = getelementptr inbounds float* %fshift, i64 %idxprom12
  %49 = load float* %arrayidx430, align 4, !tbaa !3
  %add431 = fadd float %fix1.0.lcssa, %49
  %add432 = fadd float %fix2.0.lcssa, %add431
  %add433 = fadd float %fix3.0.lcssa, %add432
  store float %add433, float* %arrayidx430, align 4, !tbaa !3
  %arrayidx438 = getelementptr inbounds float* %fshift, i64 %idxprom15
  %50 = load float* %arrayidx438, align 4, !tbaa !3
  %add439 = fadd float %fiy1.0.lcssa, %50
  %add440 = fadd float %fiy2.0.lcssa, %add439
  %add441 = fadd float %fiy3.0.lcssa, %add440
  store float %add441, float* %arrayidx438, align 4, !tbaa !3
  %arrayidx447 = getelementptr inbounds float* %fshift, i64 %idxprom18
  %51 = load float* %arrayidx447, align 4, !tbaa !3
  %add448 = fadd float %fiz1.0.lcssa, %51
  %add449 = fadd float %fiz2.0.lcssa, %add448
  %add450 = fadd float %fiz3.0.lcssa, %add449
  store float %add450, float* %arrayidx447, align 4, !tbaa !3
  %arrayidx455 = getelementptr inbounds i32* %gid, i64 %indvars.iv863
  %52 = load i32* %arrayidx455, align 4, !tbaa !0
  %idxprom456 = sext i32 %52 to i64
  %arrayidx457 = getelementptr inbounds float* %Vc, i64 %idxprom456
  %53 = load float* %arrayidx457, align 4, !tbaa !3
  %add458 = fadd float %vctot.0.lcssa, %53
  store float %add458, float* %arrayidx457, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next864 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end463, label %for.end.for.body_crit_edge

for.end.for.body_crit_edge:                       ; preds = %for.end
  %arrayidx21.phi.trans.insert = getelementptr inbounds i32* %iinr, i64 %indvars.iv.next864
  %.pre = load i32* %arrayidx21.phi.trans.insert, align 4, !tbaa !0
  br label %for.body

for.end463:                                       ; preds = %for.end, %entry
  ret void
}
