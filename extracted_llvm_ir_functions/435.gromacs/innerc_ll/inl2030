define void @inl2030(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc, float %krf, float %crf) #0 {
entry:
  %0 = load i32* %iinr, align 4, !tbaa !0
  %idxprom = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds float* %charge, i64 %idxprom
  %1 = load float* %arrayidx1, align 4, !tbaa !3
  %add = add nsw i32 %0, 1
  %idxprom2 = sext i32 %add to i64
  %arrayidx3 = getelementptr inbounds float* %charge, i64 %idxprom2
  %2 = load float* %arrayidx3, align 4, !tbaa !3
  %mul = fmul float %1, %facel
  %mul4 = fmul float %1, %mul
  %mul6 = fmul float %mul, %2
  %mul7 = fmul float %2, %facel
  %mul8 = fmul float %2, %mul7
  %cmp966 = icmp sgt i32 %nri, 0
  br i1 %cmp966, label %for.body, label %for.end517

for.body:                                         ; preds = %entry, %for.end.for.body_crit_edge
  %3 = phi i32 [ %.pre, %for.end.for.body_crit_edge ], [ %0, %entry ]
  %indvars.iv968 = phi i64 [ %indvars.iv.next969, %for.end.for.body_crit_edge ], [ 0, %entry ]
  %arrayidx10 = getelementptr inbounds i32* %shift, i64 %indvars.iv968
  %4 = load i32* %arrayidx10, align 4, !tbaa !0
  %mul11 = mul nsw i32 %4, 3
  %idxprom12 = sext i32 %mul11 to i64
  %arrayidx13 = getelementptr inbounds float* %shiftvec, i64 %idxprom12
  %5 = load float* %arrayidx13, align 4, !tbaa !3
  %add14 = add nsw i32 %mul11, 1
  %idxprom15 = sext i32 %add14 to i64
  %arrayidx16 = getelementptr inbounds float* %shiftvec, i64 %idxprom15
  %6 = load float* %arrayidx16, align 4, !tbaa !3
  %add17 = add nsw i32 %mul11, 2
  %idxprom18 = sext i32 %add17 to i64
  %arrayidx19 = getelementptr inbounds float* %shiftvec, i64 %idxprom18
  %7 = load float* %arrayidx19, align 4, !tbaa !3
  %mul22 = mul nsw i32 %3, 3
  %arrayidx24 = getelementptr inbounds i32* %jindex, i64 %indvars.iv968
  %8 = load i32* %arrayidx24, align 4, !tbaa !0
  %indvars.iv.next969 = add i64 %indvars.iv968, 1
  %arrayidx27 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next969
  %9 = load i32* %arrayidx27, align 4, !tbaa !0
  %idxprom28 = sext i32 %mul22 to i64
  %arrayidx29 = getelementptr inbounds float* %pos, i64 %idxprom28
  %10 = load float* %arrayidx29, align 4, !tbaa !3
  %add30 = fadd float %5, %10
  %add31 = add nsw i32 %mul22, 1
  %idxprom32 = sext i32 %add31 to i64
  %arrayidx33 = getelementptr inbounds float* %pos, i64 %idxprom32
  %11 = load float* %arrayidx33, align 4, !tbaa !3
  %add34 = fadd float %6, %11
  %add35 = add nsw i32 %mul22, 2
  %idxprom36 = sext i32 %add35 to i64
  %arrayidx37 = getelementptr inbounds float* %pos, i64 %idxprom36
  %12 = load float* %arrayidx37, align 4, !tbaa !3
  %add38 = fadd float %7, %12
  %add39 = add nsw i32 %mul22, 3
  %idxprom40 = sext i32 %add39 to i64
  %arrayidx41 = getelementptr inbounds float* %pos, i64 %idxprom40
  %13 = load float* %arrayidx41, align 4, !tbaa !3
  %add42 = fadd float %5, %13
  %add43 = add nsw i32 %mul22, 4
  %idxprom44 = sext i32 %add43 to i64
  %arrayidx45 = getelementptr inbounds float* %pos, i64 %idxprom44
  %14 = load float* %arrayidx45, align 4, !tbaa !3
  %add46 = fadd float %6, %14
  %add47 = add nsw i32 %mul22, 5
  %idxprom48 = sext i32 %add47 to i64
  %arrayidx49 = getelementptr inbounds float* %pos, i64 %idxprom48
  %15 = load float* %arrayidx49, align 4, !tbaa !3
  %add50 = fadd float %7, %15
  %add51 = add nsw i32 %mul22, 6
  %idxprom52 = sext i32 %add51 to i64
  %arrayidx53 = getelementptr inbounds float* %pos, i64 %idxprom52
  %16 = load float* %arrayidx53, align 4, !tbaa !3
  %add54 = fadd float %5, %16
  %add55 = add nsw i32 %mul22, 7
  %idxprom56 = sext i32 %add55 to i64
  %arrayidx57 = getelementptr inbounds float* %pos, i64 %idxprom56
  %17 = load float* %arrayidx57, align 4, !tbaa !3
  %add58 = fadd float %6, %17
  %add59 = add nsw i32 %mul22, 8
  %idxprom60 = sext i32 %add59 to i64
  %arrayidx61 = getelementptr inbounds float* %pos, i64 %idxprom60
  %18 = load float* %arrayidx61, align 4, !tbaa !3
  %add62 = fadd float %7, %18
  %cmp64945 = icmp slt i32 %8, %9
  br i1 %cmp64945, label %for.body65.lr.ph, label %for.end

for.body65.lr.ph:                                 ; preds = %for.body
  %19 = sext i32 %8 to i64
  br label %for.body65

for.body65:                                       ; preds = %for.body65.lr.ph, %for.body65
  %indvars.iv = phi i64 [ %19, %for.body65.lr.ph ], [ %indvars.iv.next, %for.body65 ]
  %vctot.0955 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add403, %for.body65 ]
  %fix1.0954 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add267, %for.body65 ]
  %fiy1.0953 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add268, %for.body65 ]
  %fiz1.0952 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add269, %for.body65 ]
  %fix2.0951 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add333, %for.body65 ]
  %fiy2.0950 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add334, %for.body65 ]
  %fiz2.0949 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add335, %for.body65 ]
  %fix3.0948 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add407, %for.body65 ]
  %fiy3.0947 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add408, %for.body65 ]
  %fiz3.0946 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add409, %for.body65 ]
  %arrayidx67 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %20 = load i32* %arrayidx67, align 4, !tbaa !0
  %mul68 = mul nsw i32 %20, 3
  %idxprom69 = sext i32 %mul68 to i64
  %arrayidx70 = getelementptr inbounds float* %pos, i64 %idxprom69
  %21 = load float* %arrayidx70, align 4, !tbaa !3
  %add71 = add nsw i32 %mul68, 1
  %idxprom72 = sext i32 %add71 to i64
  %arrayidx73 = getelementptr inbounds float* %pos, i64 %idxprom72
  %22 = load float* %arrayidx73, align 4, !tbaa !3
  %add74 = add nsw i32 %mul68, 2
  %idxprom75 = sext i32 %add74 to i64
  %arrayidx76 = getelementptr inbounds float* %pos, i64 %idxprom75
  %23 = load float* %arrayidx76, align 4, !tbaa !3
  %add77 = add nsw i32 %mul68, 3
  %idxprom78 = sext i32 %add77 to i64
  %arrayidx79 = getelementptr inbounds float* %pos, i64 %idxprom78
  %24 = load float* %arrayidx79, align 4, !tbaa !3
  %add80 = add nsw i32 %mul68, 4
  %idxprom81 = sext i32 %add80 to i64
  %arrayidx82 = getelementptr inbounds float* %pos, i64 %idxprom81
  %25 = load float* %arrayidx82, align 4, !tbaa !3
  %add83 = add nsw i32 %mul68, 5
  %idxprom84 = sext i32 %add83 to i64
  %arrayidx85 = getelementptr inbounds float* %pos, i64 %idxprom84
  %26 = load float* %arrayidx85, align 4, !tbaa !3
  %add86 = add nsw i32 %mul68, 6
  %idxprom87 = sext i32 %add86 to i64
  %arrayidx88 = getelementptr inbounds float* %pos, i64 %idxprom87
  %27 = load float* %arrayidx88, align 4, !tbaa !3
  %add89 = add nsw i32 %mul68, 7
  %idxprom90 = sext i32 %add89 to i64
  %arrayidx91 = getelementptr inbounds float* %pos, i64 %idxprom90
  %28 = load float* %arrayidx91, align 4, !tbaa !3
  %add92 = add nsw i32 %mul68, 8
  %idxprom93 = sext i32 %add92 to i64
  %arrayidx94 = getelementptr inbounds float* %pos, i64 %idxprom93
  %29 = load float* %arrayidx94, align 4, !tbaa !3
  %sub = fsub float %add30, %21
  %sub95 = fsub float %add34, %22
  %sub96 = fsub float %add38, %23
  %mul97 = fmul float %sub, %sub
  %mul98 = fmul float %sub95, %sub95
  %add99 = fadd float %mul97, %mul98
  %mul100 = fmul float %sub96, %sub96
  %add101 = fadd float %add99, %mul100
  %sub102 = fsub float %add30, %24
  %sub103 = fsub float %add34, %25
  %sub104 = fsub float %add38, %26
  %mul105 = fmul float %sub102, %sub102
  %mul106 = fmul float %sub103, %sub103
  %add107 = fadd float %mul105, %mul106
  %mul108 = fmul float %sub104, %sub104
  %add109 = fadd float %add107, %mul108
  %sub110 = fsub float %add30, %27
  %sub111 = fsub float %add34, %28
  %sub112 = fsub float %add38, %29
  %mul113 = fmul float %sub110, %sub110
  %mul114 = fmul float %sub111, %sub111
  %add115 = fadd float %mul113, %mul114
  %mul116 = fmul float %sub112, %sub112
  %add117 = fadd float %add115, %mul116
  %sub118 = fsub float %add42, %21
  %sub119 = fsub float %add46, %22
  %sub120 = fsub float %add50, %23
  %mul121 = fmul float %sub118, %sub118
  %mul122 = fmul float %sub119, %sub119
  %add123 = fadd float %mul121, %mul122
  %mul124 = fmul float %sub120, %sub120
  %add125 = fadd float %add123, %mul124
  %sub126 = fsub float %add42, %24
  %sub127 = fsub float %add46, %25
  %sub128 = fsub float %add50, %26
  %mul129 = fmul float %sub126, %sub126
  %mul130 = fmul float %sub127, %sub127
  %add131 = fadd float %mul129, %mul130
  %mul132 = fmul float %sub128, %sub128
  %add133 = fadd float %add131, %mul132
  %sub134 = fsub float %add42, %27
  %sub135 = fsub float %add46, %28
  %sub136 = fsub float %add50, %29
  %mul137 = fmul float %sub134, %sub134
  %mul138 = fmul float %sub135, %sub135
  %add139 = fadd float %mul137, %mul138
  %mul140 = fmul float %sub136, %sub136
  %add141 = fadd float %add139, %mul140
  %sub142 = fsub float %add54, %21
  %sub143 = fsub float %add58, %22
  %sub144 = fsub float %add62, %23
  %mul145 = fmul float %sub142, %sub142
  %mul146 = fmul float %sub143, %sub143
  %add147 = fadd float %mul145, %mul146
  %mul148 = fmul float %sub144, %sub144
  %add149 = fadd float %add147, %mul148
  %sub150 = fsub float %add54, %24
  %sub151 = fsub float %add58, %25
  %sub152 = fsub float %add62, %26
  %mul153 = fmul float %sub150, %sub150
  %mul154 = fmul float %sub151, %sub151
  %add155 = fadd float %mul153, %mul154
  %mul156 = fmul float %sub152, %sub152
  %add157 = fadd float %add155, %mul156
  %sub158 = fsub float %add54, %27
  %sub159 = fsub float %add58, %28
  %sub160 = fsub float %add62, %29
  %mul161 = fmul float %sub158, %sub158
  %mul162 = fmul float %sub159, %sub159
  %add163 = fadd float %mul161, %mul162
  %mul164 = fmul float %sub160, %sub160
  %add165 = fadd float %add163, %mul164
  %conv = fpext float %add101 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv166 = fptrunc double %div to float
  %conv167 = fpext float %add125 to double
  %call168 = tail call double @sqrt(double %conv167) #2
  %div169 = fdiv double 1.000000e+00, %call168
  %conv170 = fptrunc double %div169 to float
  %conv171 = fpext float %add149 to double
  %call172 = tail call double @sqrt(double %conv171) #2
  %div173 = fdiv double 1.000000e+00, %call172
  %conv174 = fptrunc double %div173 to float
  %conv175 = fpext float %add109 to double
  %call176 = tail call double @sqrt(double %conv175) #2
  %div177 = fdiv double 1.000000e+00, %call176
  %conv178 = fptrunc double %div177 to float
  %conv179 = fpext float %add133 to double
  %call180 = tail call double @sqrt(double %conv179) #2
  %div181 = fdiv double 1.000000e+00, %call180
  %conv182 = fptrunc double %div181 to float
  %conv183 = fpext float %add157 to double
  %call184 = tail call double @sqrt(double %conv183) #2
  %div185 = fdiv double 1.000000e+00, %call184
  %conv186 = fptrunc double %div185 to float
  %conv187 = fpext float %add117 to double
  %call188 = tail call double @sqrt(double %conv187) #2
  %div189 = fdiv double 1.000000e+00, %call188
  %conv190 = fptrunc double %div189 to float
  %conv191 = fpext float %add141 to double
  %call192 = tail call double @sqrt(double %conv191) #2
  %div193 = fdiv double 1.000000e+00, %call192
  %conv194 = fptrunc double %div193 to float
  %conv195 = fpext float %add165 to double
  %call196 = tail call double @sqrt(double %conv195) #2
  %div197 = fdiv double 1.000000e+00, %call196
  %conv198 = fptrunc double %div197 to float
  %mul199 = fmul float %conv166, %conv166
  %mul200 = fmul float %add101, %krf
  %add201 = fadd float %mul200, %conv166
  %sub202 = fsub float %add201, %crf
  %mul203 = fmul float %mul4, %sub202
  %mul204 = fmul float %mul200, 2.000000e+00
  %sub205 = fsub float %conv166, %mul204
  %mul206 = fmul float %mul4, %sub205
  %mul207 = fmul float %mul199, %mul206
  %add208 = fadd float %vctot.0955, %mul203
  %mul209 = fmul float %sub, %mul207
  %mul210 = fmul float %sub95, %mul207
  %mul211 = fmul float %sub96, %mul207
  %add212 = fadd float %fix1.0954, %mul209
  %add213 = fadd float %fiy1.0953, %mul210
  %add214 = fadd float %fiz1.0952, %mul211
  %arrayidx216 = getelementptr inbounds float* %faction, i64 %idxprom69
  %30 = load float* %arrayidx216, align 4, !tbaa !3
  %sub217 = fsub float %30, %mul209
  %arrayidx220 = getelementptr inbounds float* %faction, i64 %idxprom72
  %31 = load float* %arrayidx220, align 4, !tbaa !3
  %sub221 = fsub float %31, %mul210
  %arrayidx224 = getelementptr inbounds float* %faction, i64 %idxprom75
  %32 = load float* %arrayidx224, align 4, !tbaa !3
  %sub225 = fsub float %32, %mul211
  %mul226 = fmul float %conv178, %conv178
  %mul227 = fmul float %add109, %krf
  %add228 = fadd float %mul227, %conv178
  %sub229 = fsub float %add228, %crf
  %mul230 = fmul float %mul6, %sub229
  %mul231 = fmul float %mul227, 2.000000e+00
  %sub232 = fsub float %conv178, %mul231
  %mul233 = fmul float %mul6, %sub232
  %mul234 = fmul float %mul226, %mul233
  %add235 = fadd float %add208, %mul230
  %mul236 = fmul float %sub102, %mul234
  %mul237 = fmul float %sub103, %mul234
  %mul238 = fmul float %sub104, %mul234
  %add239 = fadd float %add212, %mul236
  %add240 = fadd float %add213, %mul237
  %add241 = fadd float %add214, %mul238
  %arrayidx244 = getelementptr inbounds float* %faction, i64 %idxprom78
  %33 = load float* %arrayidx244, align 4, !tbaa !3
  %sub245 = fsub float %33, %mul236
  %arrayidx248 = getelementptr inbounds float* %faction, i64 %idxprom81
  %34 = load float* %arrayidx248, align 4, !tbaa !3
  %sub249 = fsub float %34, %mul237
  %arrayidx252 = getelementptr inbounds float* %faction, i64 %idxprom84
  %35 = load float* %arrayidx252, align 4, !tbaa !3
  %sub253 = fsub float %35, %mul238
  %mul254 = fmul float %conv190, %conv190
  %mul255 = fmul float %add117, %krf
  %add256 = fadd float %mul255, %conv190
  %sub257 = fsub float %add256, %crf
  %mul258 = fmul float %mul6, %sub257
  %mul259 = fmul float %mul255, 2.000000e+00
  %sub260 = fsub float %conv190, %mul259
  %mul261 = fmul float %mul6, %sub260
  %mul262 = fmul float %mul254, %mul261
  %add263 = fadd float %add235, %mul258
  %mul264 = fmul float %sub110, %mul262
  %mul265 = fmul float %sub111, %mul262
  %mul266 = fmul float %sub112, %mul262
  %add267 = fadd float %add239, %mul264
  %add268 = fadd float %add240, %mul265
  %add269 = fadd float %add241, %mul266
  %arrayidx272 = getelementptr inbounds float* %faction, i64 %idxprom87
  %36 = load float* %arrayidx272, align 4, !tbaa !3
  %sub273 = fsub float %36, %mul264
  %arrayidx276 = getelementptr inbounds float* %faction, i64 %idxprom90
  %37 = load float* %arrayidx276, align 4, !tbaa !3
  %sub277 = fsub float %37, %mul265
  %arrayidx280 = getelementptr inbounds float* %faction, i64 %idxprom93
  %38 = load float* %arrayidx280, align 4, !tbaa !3
  %sub281 = fsub float %38, %mul266
  %mul282 = fmul float %conv170, %conv170
  %mul283 = fmul float %add125, %krf
  %add284 = fadd float %mul283, %conv170
  %sub285 = fsub float %add284, %crf
  %mul286 = fmul float %mul6, %sub285
  %mul287 = fmul float %mul283, 2.000000e+00
  %sub288 = fsub float %conv170, %mul287
  %mul289 = fmul float %mul6, %sub288
  %mul290 = fmul float %mul282, %mul289
  %add291 = fadd float %mul286, %add263
  %mul292 = fmul float %sub118, %mul290
  %mul293 = fmul float %sub119, %mul290
  %mul294 = fmul float %sub120, %mul290
  %add295 = fadd float %fix2.0951, %mul292
  %add296 = fadd float %fiy2.0950, %mul293
  %add297 = fadd float %fiz2.0949, %mul294
  %sub298 = fsub float %sub217, %mul292
  %sub299 = fsub float %sub221, %mul293
  %sub300 = fsub float %sub225, %mul294
  %mul301 = fmul float %conv182, %conv182
  %mul302 = fmul float %add133, %krf
  %add303 = fadd float %mul302, %conv182
  %sub304 = fsub float %add303, %crf
  %mul305 = fmul float %mul8, %sub304
  %mul306 = fmul float %mul302, 2.000000e+00
  %sub307 = fsub float %conv182, %mul306
  %mul308 = fmul float %mul8, %sub307
  %mul309 = fmul float %mul301, %mul308
  %add310 = fadd float %mul305, %add291
  %mul311 = fmul float %sub126, %mul309
  %mul312 = fmul float %sub127, %mul309
  %mul313 = fmul float %sub128, %mul309
  %add314 = fadd float %add295, %mul311
  %add315 = fadd float %add296, %mul312
  %add316 = fadd float %add297, %mul313
  %sub317 = fsub float %sub245, %mul311
  %sub318 = fsub float %sub249, %mul312
  %sub319 = fsub float %sub253, %mul313
  %mul320 = fmul float %conv194, %conv194
  %mul321 = fmul float %add141, %krf
  %add322 = fadd float %mul321, %conv194
  %sub323 = fsub float %add322, %crf
  %mul324 = fmul float %mul8, %sub323
  %mul325 = fmul float %mul321, 2.000000e+00
  %sub326 = fsub float %conv194, %mul325
  %mul327 = fmul float %mul8, %sub326
  %mul328 = fmul float %mul320, %mul327
  %add329 = fadd float %mul324, %add310
  %mul330 = fmul float %sub134, %mul328
  %mul331 = fmul float %sub135, %mul328
  %mul332 = fmul float %sub136, %mul328
  %add333 = fadd float %add314, %mul330
  %add334 = fadd float %add315, %mul331
  %add335 = fadd float %add316, %mul332
  %sub336 = fsub float %sub273, %mul330
  %sub337 = fsub float %sub277, %mul331
  %sub338 = fsub float %sub281, %mul332
  %mul339 = fmul float %conv174, %conv174
  %mul340 = fmul float %add149, %krf
  %add341 = fadd float %mul340, %conv174
  %sub342 = fsub float %add341, %crf
  %mul343 = fmul float %mul6, %sub342
  %mul344 = fmul float %mul340, 2.000000e+00
  %sub345 = fsub float %conv174, %mul344
  %mul346 = fmul float %mul6, %sub345
  %mul347 = fmul float %mul339, %mul346
  %add348 = fadd float %mul343, %add329
  %mul349 = fmul float %sub142, %mul347
  %mul350 = fmul float %sub143, %mul347
  %mul351 = fmul float %sub144, %mul347
  %add352 = fadd float %fix3.0948, %mul349
  %add353 = fadd float %fiy3.0947, %mul350
  %add354 = fadd float %fiz3.0946, %mul351
  %sub355 = fsub float %sub298, %mul349
  store float %sub355, float* %arrayidx216, align 4, !tbaa !3
  %sub358 = fsub float %sub299, %mul350
  store float %sub358, float* %arrayidx220, align 4, !tbaa !3
  %sub362 = fsub float %sub300, %mul351
  store float %sub362, float* %arrayidx224, align 4, !tbaa !3
  %mul366 = fmul float %conv186, %conv186
  %mul367 = fmul float %add157, %krf
  %add368 = fadd float %mul367, %conv186
  %sub369 = fsub float %add368, %crf
  %mul370 = fmul float %mul8, %sub369
  %mul371 = fmul float %mul367, 2.000000e+00
  %sub372 = fsub float %conv186, %mul371
  %mul373 = fmul float %mul8, %sub372
  %mul374 = fmul float %mul366, %mul373
  %add375 = fadd float %mul370, %add348
  %mul376 = fmul float %sub150, %mul374
  %mul377 = fmul float %sub151, %mul374
  %mul378 = fmul float %sub152, %mul374
  %add379 = fadd float %add352, %mul376
  %add380 = fadd float %add353, %mul377
  %add381 = fadd float %add354, %mul378
  %sub382 = fsub float %sub317, %mul376
  store float %sub382, float* %arrayidx244, align 4, !tbaa !3
  %sub386 = fsub float %sub318, %mul377
  store float %sub386, float* %arrayidx248, align 4, !tbaa !3
  %sub390 = fsub float %sub319, %mul378
  store float %sub390, float* %arrayidx252, align 4, !tbaa !3
  %mul394 = fmul float %conv198, %conv198
  %mul395 = fmul float %add165, %krf
  %add396 = fadd float %mul395, %conv198
  %sub397 = fsub float %add396, %crf
  %mul398 = fmul float %mul8, %sub397
  %mul399 = fmul float %mul395, 2.000000e+00
  %sub400 = fsub float %conv198, %mul399
  %mul401 = fmul float %mul8, %sub400
  %mul402 = fmul float %mul394, %mul401
  %add403 = fadd float %mul398, %add375
  %mul404 = fmul float %sub158, %mul402
  %mul405 = fmul float %sub159, %mul402
  %mul406 = fmul float %sub160, %mul402
  %add407 = fadd float %add379, %mul404
  %add408 = fadd float %add380, %mul405
  %add409 = fadd float %add381, %mul406
  %sub410 = fsub float %sub336, %mul404
  store float %sub410, float* %arrayidx272, align 4, !tbaa !3
  %sub414 = fsub float %sub337, %mul405
  store float %sub414, float* %arrayidx276, align 4, !tbaa !3
  %sub418 = fsub float %sub338, %mul406
  store float %sub418, float* %arrayidx280, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %39 = trunc i64 %indvars.iv.next to i32
  %cmp64 = icmp slt i32 %39, %9
  br i1 %cmp64, label %for.body65, label %for.end

for.end:                                          ; preds = %for.body65, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add403, %for.body65 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add267, %for.body65 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add268, %for.body65 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add269, %for.body65 ]
  %fix2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add333, %for.body65 ]
  %fiy2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add334, %for.body65 ]
  %fiz2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add335, %for.body65 ]
  %fix3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add407, %for.body65 ]
  %fiy3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add408, %for.body65 ]
  %fiz3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add409, %for.body65 ]
  %arrayidx423 = getelementptr inbounds float* %faction, i64 %idxprom28
  %40 = load float* %arrayidx423, align 4, !tbaa !3
  %add424 = fadd float %fix1.0.lcssa, %40
  store float %add424, float* %arrayidx423, align 4, !tbaa !3
  %arrayidx429 = getelementptr inbounds float* %faction, i64 %idxprom32
  %41 = load float* %arrayidx429, align 4, !tbaa !3
  %add430 = fadd float %fiy1.0.lcssa, %41
  store float %add430, float* %arrayidx429, align 4, !tbaa !3
  %arrayidx436 = getelementptr inbounds float* %faction, i64 %idxprom36
  %42 = load float* %arrayidx436, align 4, !tbaa !3
  %add437 = fadd float %fiz1.0.lcssa, %42
  store float %add437, float* %arrayidx436, align 4, !tbaa !3
  %arrayidx443 = getelementptr inbounds float* %faction, i64 %idxprom40
  %43 = load float* %arrayidx443, align 4, !tbaa !3
  %add444 = fadd float %fix2.0.lcssa, %43
  store float %add444, float* %arrayidx443, align 4, !tbaa !3
  %arrayidx450 = getelementptr inbounds float* %faction, i64 %idxprom44
  %44 = load float* %arrayidx450, align 4, !tbaa !3
  %add451 = fadd float %fiy2.0.lcssa, %44
  store float %add451, float* %arrayidx450, align 4, !tbaa !3
  %arrayidx457 = getelementptr inbounds float* %faction, i64 %idxprom48
  %45 = load float* %arrayidx457, align 4, !tbaa !3
  %add458 = fadd float %fiz2.0.lcssa, %45
  store float %add458, float* %arrayidx457, align 4, !tbaa !3
  %arrayidx464 = getelementptr inbounds float* %faction, i64 %idxprom52
  %46 = load float* %arrayidx464, align 4, !tbaa !3
  %add465 = fadd float %fix3.0.lcssa, %46
  store float %add465, float* %arrayidx464, align 4, !tbaa !3
  %arrayidx471 = getelementptr inbounds float* %faction, i64 %idxprom56
  %47 = load float* %arrayidx471, align 4, !tbaa !3
  %add472 = fadd float %fiy3.0.lcssa, %47
  store float %add472, float* %arrayidx471, align 4, !tbaa !3
  %arrayidx478 = getelementptr inbounds float* %faction, i64 %idxprom60
  %48 = load float* %arrayidx478, align 4, !tbaa !3
  %add479 = fadd float %fiz3.0.lcssa, %48
  store float %add479, float* %arrayidx478, align 4, !tbaa !3
  %arrayidx484 = getelementptr inbounds float* %fshift, i64 %idxprom12
  %49 = load float* %arrayidx484, align 4, !tbaa !3
  %add485 = fadd float %fix1.0.lcssa, %49
  %add486 = fadd float %fix2.0.lcssa, %add485
  %add487 = fadd float %fix3.0.lcssa, %add486
  store float %add487, float* %arrayidx484, align 4, !tbaa !3
  %arrayidx492 = getelementptr inbounds float* %fshift, i64 %idxprom15
  %50 = load float* %arrayidx492, align 4, !tbaa !3
  %add493 = fadd float %fiy1.0.lcssa, %50
  %add494 = fadd float %fiy2.0.lcssa, %add493
  %add495 = fadd float %fiy3.0.lcssa, %add494
  store float %add495, float* %arrayidx492, align 4, !tbaa !3
  %arrayidx501 = getelementptr inbounds float* %fshift, i64 %idxprom18
  %51 = load float* %arrayidx501, align 4, !tbaa !3
  %add502 = fadd float %fiz1.0.lcssa, %51
  %add503 = fadd float %fiz2.0.lcssa, %add502
  %add504 = fadd float %fiz3.0.lcssa, %add503
  store float %add504, float* %arrayidx501, align 4, !tbaa !3
  %arrayidx509 = getelementptr inbounds i32* %gid, i64 %indvars.iv968
  %52 = load i32* %arrayidx509, align 4, !tbaa !0
  %idxprom510 = sext i32 %52 to i64
  %arrayidx511 = getelementptr inbounds float* %Vc, i64 %idxprom510
  %53 = load float* %arrayidx511, align 4, !tbaa !3
  %add512 = fadd float %vctot.0.lcssa, %53
  store float %add512, float* %arrayidx511, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next969 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end517, label %for.end.for.body_crit_edge

for.end.for.body_crit_edge:                       ; preds = %for.end
  %arrayidx21.phi.trans.insert = getelementptr inbounds i32* %iinr, i64 %indvars.iv.next969
  %.pre = load i32* %arrayidx21.phi.trans.insert, align 4, !tbaa !0
  br label %for.body

for.end517:                                       ; preds = %for.end, %entry
  ret void
}
