define void @inl3430(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc, i32* nocapture %type, i32 %ntype, float* nocapture %nbfp, float* nocapture %Vnb, float %tabscale, float* %VFtab, float %exptabscale) #0 {
entry:
  %0 = load i32* %iinr, align 4, !tbaa !0
  %idxprom = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds float* %charge, i64 %idxprom
  %1 = load float* %arrayidx1, align 4, !tbaa !3
  %add = add nsw i32 %0, 1
  %idxprom2 = sext i32 %add to i64
  %arrayidx3 = getelementptr inbounds float* %charge, i64 %idxprom2
  %2 = load float* %arrayidx3, align 4, !tbaa !3
  %mul = fmul float %1, %facel
  %mul4 = fmul float %1, %mul
  %mul6 = fmul float %mul, %2
  %mul7 = fmul float %2, %facel
  %mul8 = fmul float %2, %mul7
  %mul9 = mul nsw i32 %ntype, 3
  %arrayidx11 = getelementptr inbounds i32* %type, i64 %idxprom
  %3 = load i32* %arrayidx11, align 4, !tbaa !0
  %mul121510 = add i32 %mul9, 3
  %add16 = mul i32 %3, %mul121510
  %idxprom17 = sext i32 %add16 to i64
  %arrayidx18 = getelementptr inbounds float* %nbfp, i64 %idxprom17
  %4 = load float* %arrayidx18, align 4, !tbaa !3
  %add19 = add nsw i32 %add16, 1
  %idxprom20 = sext i32 %add19 to i64
  %arrayidx21 = getelementptr inbounds float* %nbfp, i64 %idxprom20
  %5 = load float* %arrayidx21, align 4, !tbaa !3
  %add22 = add nsw i32 %add16, 2
  %idxprom23 = sext i32 %add22 to i64
  %arrayidx24 = getelementptr inbounds float* %nbfp, i64 %idxprom23
  %6 = load float* %arrayidx24, align 4, !tbaa !3
  %cmp1561 = icmp sgt i32 %nri, 0
  br i1 %cmp1561, label %for.body.lr.ph, label %for.end804

for.body.lr.ph:                                   ; preds = %entry
  %mul296 = fmul float %5, %6
  br label %for.body

for.body:                                         ; preds = %for.end.for.body_crit_edge, %for.body.lr.ph
  %7 = phi i32 [ %0, %for.body.lr.ph ], [ %.pre, %for.end.for.body_crit_edge ]
  %indvars.iv1563 = phi i64 [ 0, %for.body.lr.ph ], [ %indvars.iv.next1564, %for.end.for.body_crit_edge ]
  %arrayidx26 = getelementptr inbounds i32* %shift, i64 %indvars.iv1563
  %8 = load i32* %arrayidx26, align 4, !tbaa !0
  %mul27 = mul nsw i32 %8, 3
  %idxprom28 = sext i32 %mul27 to i64
  %arrayidx29 = getelementptr inbounds float* %shiftvec, i64 %idxprom28
  %9 = load float* %arrayidx29, align 4, !tbaa !3
  %add30 = add nsw i32 %mul27, 1
  %idxprom31 = sext i32 %add30 to i64
  %arrayidx32 = getelementptr inbounds float* %shiftvec, i64 %idxprom31
  %10 = load float* %arrayidx32, align 4, !tbaa !3
  %add33 = add nsw i32 %mul27, 2
  %idxprom34 = sext i32 %add33 to i64
  %arrayidx35 = getelementptr inbounds float* %shiftvec, i64 %idxprom34
  %11 = load float* %arrayidx35, align 4, !tbaa !3
  %mul38 = mul nsw i32 %7, 3
  %arrayidx40 = getelementptr inbounds i32* %jindex, i64 %indvars.iv1563
  %12 = load i32* %arrayidx40, align 4, !tbaa !0
  %indvars.iv.next1564 = add i64 %indvars.iv1563, 1
  %arrayidx43 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next1564
  %13 = load i32* %arrayidx43, align 4, !tbaa !0
  %idxprom44 = sext i32 %mul38 to i64
  %arrayidx45 = getelementptr inbounds float* %pos, i64 %idxprom44
  %14 = load float* %arrayidx45, align 4, !tbaa !3
  %add46 = fadd float %9, %14
  %add47 = add nsw i32 %mul38, 1
  %idxprom48 = sext i32 %add47 to i64
  %arrayidx49 = getelementptr inbounds float* %pos, i64 %idxprom48
  %15 = load float* %arrayidx49, align 4, !tbaa !3
  %add50 = fadd float %10, %15
  %add51 = add nsw i32 %mul38, 2
  %idxprom52 = sext i32 %add51 to i64
  %arrayidx53 = getelementptr inbounds float* %pos, i64 %idxprom52
  %16 = load float* %arrayidx53, align 4, !tbaa !3
  %add54 = fadd float %11, %16
  %add55 = add nsw i32 %mul38, 3
  %idxprom56 = sext i32 %add55 to i64
  %arrayidx57 = getelementptr inbounds float* %pos, i64 %idxprom56
  %17 = load float* %arrayidx57, align 4, !tbaa !3
  %add58 = fadd float %9, %17
  %add59 = add nsw i32 %mul38, 4
  %idxprom60 = sext i32 %add59 to i64
  %arrayidx61 = getelementptr inbounds float* %pos, i64 %idxprom60
  %18 = load float* %arrayidx61, align 4, !tbaa !3
  %add62 = fadd float %10, %18
  %add63 = add nsw i32 %mul38, 5
  %idxprom64 = sext i32 %add63 to i64
  %arrayidx65 = getelementptr inbounds float* %pos, i64 %idxprom64
  %19 = load float* %arrayidx65, align 4, !tbaa !3
  %add66 = fadd float %11, %19
  %add67 = add nsw i32 %mul38, 6
  %idxprom68 = sext i32 %add67 to i64
  %arrayidx69 = getelementptr inbounds float* %pos, i64 %idxprom68
  %20 = load float* %arrayidx69, align 4, !tbaa !3
  %add70 = fadd float %9, %20
  %add71 = add nsw i32 %mul38, 7
  %idxprom72 = sext i32 %add71 to i64
  %arrayidx73 = getelementptr inbounds float* %pos, i64 %idxprom72
  %21 = load float* %arrayidx73, align 4, !tbaa !3
  %add74 = fadd float %10, %21
  %add75 = add nsw i32 %mul38, 8
  %idxprom76 = sext i32 %add75 to i64
  %arrayidx77 = getelementptr inbounds float* %pos, i64 %idxprom76
  %22 = load float* %arrayidx77, align 4, !tbaa !3
  %add78 = fadd float %11, %22
  %cmp801538 = icmp slt i32 %12, %13
  br i1 %cmp801538, label %for.body81.lr.ph, label %for.end

for.body81.lr.ph:                                 ; preds = %for.body
  %23 = sext i32 %12 to i64
  br label %for.body81

for.body81:                                       ; preds = %for.body81.lr.ph, %for.body81
  %indvars.iv = phi i64 [ %23, %for.body81.lr.ph ], [ %indvars.iv.next, %for.body81 ]
  %vctot.01549 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add685, %for.body81 ]
  %vnbtot.01548 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add299, %for.body81 ]
  %fix1.01547 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add411, %for.body81 ]
  %fiy1.01546 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add412, %for.body81 ]
  %fiz1.01545 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add413, %for.body81 ]
  %fix2.01544 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add546, %for.body81 ]
  %fiy2.01543 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add547, %for.body81 ]
  %fiz2.01542 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add548, %for.body81 ]
  %fix3.01541 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add689, %for.body81 ]
  %fiy3.01540 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add690, %for.body81 ]
  %fiz3.01539 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add691, %for.body81 ]
  %arrayidx83 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %24 = load i32* %arrayidx83, align 4, !tbaa !0
  %mul84 = mul nsw i32 %24, 3
  %idxprom85 = sext i32 %mul84 to i64
  %arrayidx86 = getelementptr inbounds float* %pos, i64 %idxprom85
  %25 = load float* %arrayidx86, align 4, !tbaa !3
  %add87 = add nsw i32 %mul84, 1
  %idxprom88 = sext i32 %add87 to i64
  %arrayidx89 = getelementptr inbounds float* %pos, i64 %idxprom88
  %26 = load float* %arrayidx89, align 4, !tbaa !3
  %add90 = add nsw i32 %mul84, 2
  %idxprom91 = sext i32 %add90 to i64
  %arrayidx92 = getelementptr inbounds float* %pos, i64 %idxprom91
  %27 = load float* %arrayidx92, align 4, !tbaa !3
  %add93 = add nsw i32 %mul84, 3
  %idxprom94 = sext i32 %add93 to i64
  %arrayidx95 = getelementptr inbounds float* %pos, i64 %idxprom94
  %28 = load float* %arrayidx95, align 4, !tbaa !3
  %add96 = add nsw i32 %mul84, 4
  %idxprom97 = sext i32 %add96 to i64
  %arrayidx98 = getelementptr inbounds float* %pos, i64 %idxprom97
  %29 = load float* %arrayidx98, align 4, !tbaa !3
  %add99 = add nsw i32 %mul84, 5
  %idxprom100 = sext i32 %add99 to i64
  %arrayidx101 = getelementptr inbounds float* %pos, i64 %idxprom100
  %30 = load float* %arrayidx101, align 4, !tbaa !3
  %add102 = add nsw i32 %mul84, 6
  %idxprom103 = sext i32 %add102 to i64
  %arrayidx104 = getelementptr inbounds float* %pos, i64 %idxprom103
  %31 = load float* %arrayidx104, align 4, !tbaa !3
  %add105 = add nsw i32 %mul84, 7
  %idxprom106 = sext i32 %add105 to i64
  %arrayidx107 = getelementptr inbounds float* %pos, i64 %idxprom106
  %32 = load float* %arrayidx107, align 4, !tbaa !3
  %add108 = add nsw i32 %mul84, 8
  %idxprom109 = sext i32 %add108 to i64
  %arrayidx110 = getelementptr inbounds float* %pos, i64 %idxprom109
  %33 = load float* %arrayidx110, align 4, !tbaa !3
  %sub = fsub float %add46, %25
  %sub111 = fsub float %add50, %26
  %sub112 = fsub float %add54, %27
  %mul113 = fmul float %sub, %sub
  %mul114 = fmul float %sub111, %sub111
  %add115 = fadd float %mul113, %mul114
  %mul116 = fmul float %sub112, %sub112
  %add117 = fadd float %add115, %mul116
  %sub118 = fsub float %add46, %28
  %sub119 = fsub float %add50, %29
  %sub120 = fsub float %add54, %30
  %mul121 = fmul float %sub118, %sub118
  %mul122 = fmul float %sub119, %sub119
  %add123 = fadd float %mul121, %mul122
  %mul124 = fmul float %sub120, %sub120
  %add125 = fadd float %add123, %mul124
  %sub126 = fsub float %add46, %31
  %sub127 = fsub float %add50, %32
  %sub128 = fsub float %add54, %33
  %mul129 = fmul float %sub126, %sub126
  %mul130 = fmul float %sub127, %sub127
  %add131 = fadd float %mul129, %mul130
  %mul132 = fmul float %sub128, %sub128
  %add133 = fadd float %add131, %mul132
  %sub134 = fsub float %add58, %25
  %sub135 = fsub float %add62, %26
  %sub136 = fsub float %add66, %27
  %mul137 = fmul float %sub134, %sub134
  %mul138 = fmul float %sub135, %sub135
  %add139 = fadd float %mul137, %mul138
  %mul140 = fmul float %sub136, %sub136
  %add141 = fadd float %add139, %mul140
  %sub142 = fsub float %add58, %28
  %sub143 = fsub float %add62, %29
  %sub144 = fsub float %add66, %30
  %mul145 = fmul float %sub142, %sub142
  %mul146 = fmul float %sub143, %sub143
  %add147 = fadd float %mul145, %mul146
  %mul148 = fmul float %sub144, %sub144
  %add149 = fadd float %add147, %mul148
  %sub150 = fsub float %add58, %31
  %sub151 = fsub float %add62, %32
  %sub152 = fsub float %add66, %33
  %mul153 = fmul float %sub150, %sub150
  %mul154 = fmul float %sub151, %sub151
  %add155 = fadd float %mul153, %mul154
  %mul156 = fmul float %sub152, %sub152
  %add157 = fadd float %add155, %mul156
  %sub158 = fsub float %add70, %25
  %sub159 = fsub float %add74, %26
  %sub160 = fsub float %add78, %27
  %mul161 = fmul float %sub158, %sub158
  %mul162 = fmul float %sub159, %sub159
  %add163 = fadd float %mul161, %mul162
  %mul164 = fmul float %sub160, %sub160
  %add165 = fadd float %add163, %mul164
  %sub166 = fsub float %add70, %28
  %sub167 = fsub float %add74, %29
  %sub168 = fsub float %add78, %30
  %mul169 = fmul float %sub166, %sub166
  %mul170 = fmul float %sub167, %sub167
  %add171 = fadd float %mul169, %mul170
  %mul172 = fmul float %sub168, %sub168
  %add173 = fadd float %add171, %mul172
  %sub174 = fsub float %add70, %31
  %sub175 = fsub float %add74, %32
  %sub176 = fsub float %add78, %33
  %mul177 = fmul float %sub174, %sub174
  %mul178 = fmul float %sub175, %sub175
  %add179 = fadd float %mul177, %mul178
  %mul180 = fmul float %sub176, %sub176
  %add181 = fadd float %add179, %mul180
  %conv = fpext float %add117 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv182 = fptrunc double %div to float
  %conv183 = fpext float %add141 to double
  %call184 = tail call double @sqrt(double %conv183) #2
  %div185 = fdiv double 1.000000e+00, %call184
  %conv186 = fptrunc double %div185 to float
  %conv187 = fpext float %add165 to double
  %call188 = tail call double @sqrt(double %conv187) #2
  %div189 = fdiv double 1.000000e+00, %call188
  %conv190 = fptrunc double %div189 to float
  %conv191 = fpext float %add125 to double
  %call192 = tail call double @sqrt(double %conv191) #2
  %div193 = fdiv double 1.000000e+00, %call192
  %conv194 = fptrunc double %div193 to float
  %conv195 = fpext float %add149 to double
  %call196 = tail call double @sqrt(double %conv195) #2
  %div197 = fdiv double 1.000000e+00, %call196
  %conv198 = fptrunc double %div197 to float
  %conv199 = fpext float %add173 to double
  %call200 = tail call double @sqrt(double %conv199) #2
  %div201 = fdiv double 1.000000e+00, %call200
  %conv202 = fptrunc double %div201 to float
  %conv203 = fpext float %add133 to double
  %call204 = tail call double @sqrt(double %conv203) #2
  %div205 = fdiv double 1.000000e+00, %call204
  %conv206 = fptrunc double %div205 to float
  %conv207 = fpext float %add157 to double
  %call208 = tail call double @sqrt(double %conv207) #2
  %div209 = fdiv double 1.000000e+00, %call208
  %conv210 = fptrunc double %div209 to float
  %conv211 = fpext float %add181 to double
  %call212 = tail call double @sqrt(double %conv211) #2
  %div213 = fdiv double 1.000000e+00, %call212
  %conv214 = fptrunc double %div213 to float
  %mul215 = fmul float %add117, %conv182
  %mul216 = fmul float %mul215, %tabscale
  %conv217 = fptosi float %mul216 to i32
  %conv218 = sitofp i32 %conv217 to float
  %sub219 = fsub float %mul216, %conv218
  %mul220 = fmul float %sub219, %sub219
  %mul221 = mul nsw i32 %conv217, 12
  %idxprom222 = sext i32 %mul221 to i64
  %arrayidx223 = getelementptr inbounds float* %VFtab, i64 %idxprom222
  %34 = load float* %arrayidx223, align 4, !tbaa !3
  %add2241511 = or i32 %mul221, 1
  %idxprom225 = sext i32 %add2241511 to i64
  %arrayidx226 = getelementptr inbounds float* %VFtab, i64 %idxprom225
  %35 = load float* %arrayidx226, align 4, !tbaa !3
  %add2271512 = or i32 %mul221, 2
  %idxprom228 = sext i32 %add2271512 to i64
  %arrayidx229 = getelementptr inbounds float* %VFtab, i64 %idxprom228
  %36 = load float* %arrayidx229, align 4, !tbaa !3
  %mul230 = fmul float %sub219, %36
  %add2311513 = or i32 %mul221, 3
  %idxprom232 = sext i32 %add2311513 to i64
  %arrayidx233 = getelementptr inbounds float* %VFtab, i64 %idxprom232
  %37 = load float* %arrayidx233, align 4, !tbaa !3
  %mul234 = fmul float %mul220, %37
  %add235 = fadd float %35, %mul230
  %add236 = fadd float %add235, %mul234
  %mul237 = fmul float %sub219, %add236
  %add238 = fadd float %34, %mul237
  %add239 = fadd float %mul230, %add236
  %mul240 = fmul float %mul234, 2.000000e+00
  %add241 = fadd float %mul240, %add239
  %mul242 = fmul float %mul4, %add238
  %mul243 = fmul float %mul4, %add241
  %add244 = add nsw i32 %mul221, 4
  %idxprom245 = sext i32 %add244 to i64
  %arrayidx246 = getelementptr inbounds float* %VFtab, i64 %idxprom245
  %38 = load float* %arrayidx246, align 4, !tbaa !3
  %add247 = add nsw i32 %mul221, 5
  %idxprom248 = sext i32 %add247 to i64
  %arrayidx249 = getelementptr inbounds float* %VFtab, i64 %idxprom248
  %39 = load float* %arrayidx249, align 4, !tbaa !3
  %add250 = add nsw i32 %mul221, 6
  %idxprom251 = sext i32 %add250 to i64
  %arrayidx252 = getelementptr inbounds float* %VFtab, i64 %idxprom251
  %40 = load float* %arrayidx252, align 4, !tbaa !3
  %mul253 = fmul float %sub219, %40
  %add254 = add nsw i32 %mul221, 7
  %idxprom255 = sext i32 %add254 to i64
  %arrayidx256 = getelementptr inbounds float* %VFtab, i64 %idxprom255
  %41 = load float* %arrayidx256, align 4, !tbaa !3
  %mul257 = fmul float %mul220, %41
  %add258 = fadd float %39, %mul253
  %add259 = fadd float %add258, %mul257
  %mul260 = fmul float %sub219, %add259
  %add261 = fadd float %38, %mul260
  %add262 = fadd float %mul253, %add259
  %mul263 = fmul float %mul257, 2.000000e+00
  %add264 = fadd float %mul263, %add262
  %mul265 = fmul float %4, %add261
  %mul266 = fmul float %4, %add264
  %mul267 = fmul float %6, %mul215
  %mul268 = fmul float %mul267, %exptabscale
  %conv269 = fptosi float %mul268 to i32
  %conv270 = sitofp i32 %conv269 to float
  %sub271 = fsub float %mul268, %conv270
  %mul272 = fmul float %sub271, %sub271
  %mul273 = mul nsw i32 %conv269, 12
  %add274 = add nsw i32 %mul273, 8
  %idxprom275 = sext i32 %add274 to i64
  %arrayidx276 = getelementptr inbounds float* %VFtab, i64 %idxprom275
  %42 = load float* %arrayidx276, align 4, !tbaa !3
  %add277 = add nsw i32 %mul273, 9
  %idxprom278 = sext i32 %add277 to i64
  %arrayidx279 = getelementptr inbounds float* %VFtab, i64 %idxprom278
  %43 = load float* %arrayidx279, align 4, !tbaa !3
  %add280 = add nsw i32 %mul273, 10
  %idxprom281 = sext i32 %add280 to i64
  %arrayidx282 = getelementptr inbounds float* %VFtab, i64 %idxprom281
  %44 = load float* %arrayidx282, align 4, !tbaa !3
  %mul283 = fmul float %sub271, %44
  %add284 = add nsw i32 %mul273, 11
  %idxprom285 = sext i32 %add284 to i64
  %arrayidx286 = getelementptr inbounds float* %VFtab, i64 %idxprom285
  %45 = load float* %arrayidx286, align 4, !tbaa !3
  %mul287 = fmul float %mul272, %45
  %add288 = fadd float %43, %mul283
  %add289 = fadd float %add288, %mul287
  %mul290 = fmul float %sub271, %add289
  %add291 = fadd float %42, %mul290
  %add292 = fadd float %mul283, %add289
  %mul293 = fmul float %mul287, 2.000000e+00
  %add294 = fadd float %mul293, %add292
  %mul295 = fmul float %5, %add291
  %mul297 = fmul float %mul296, %add294
  %add298 = fadd float %vnbtot.01548, %mul265
  %add299 = fadd float %add298, %mul295
  %add300 = fadd float %mul243, %mul266
  %mul301 = fmul float %add300, %tabscale
  %mul302 = fmul float %mul297, %exptabscale
  %add303 = fadd float %mul301, %mul302
  %46 = fmul float %conv182, %add303
  %mul305 = fsub float -0.000000e+00, %46
  %add306 = fadd float %vctot.01549, %mul242
  %mul307 = fmul float %sub, %mul305
  %mul308 = fmul float %sub111, %mul305
  %mul309 = fmul float %sub112, %mul305
  %add310 = fadd float %fix1.01547, %mul307
  %add311 = fadd float %fiy1.01546, %mul308
  %add312 = fadd float %fiz1.01545, %mul309
  %arrayidx314 = getelementptr inbounds float* %faction, i64 %idxprom85
  %47 = load float* %arrayidx314, align 4, !tbaa !3
  %sub315 = fsub float %47, %mul307
  %arrayidx318 = getelementptr inbounds float* %faction, i64 %idxprom88
  %48 = load float* %arrayidx318, align 4, !tbaa !3
  %sub319 = fsub float %48, %mul308
  %arrayidx322 = getelementptr inbounds float* %faction, i64 %idxprom91
  %49 = load float* %arrayidx322, align 4, !tbaa !3
  %sub323 = fsub float %49, %mul309
  %mul324 = fmul float %add125, %conv194
  %mul325 = fmul float %mul324, %tabscale
  %conv326 = fptosi float %mul325 to i32
  %conv327 = sitofp i32 %conv326 to float
  %sub328 = fsub float %mul325, %conv327
  %mul329 = fmul float %sub328, %sub328
  %mul330 = mul nsw i32 %conv326, 12
  %idxprom331 = sext i32 %mul330 to i64
  %arrayidx332 = getelementptr inbounds float* %VFtab, i64 %idxprom331
  %50 = load float* %arrayidx332, align 4, !tbaa !3
  %add3331514 = or i32 %mul330, 1
  %idxprom334 = sext i32 %add3331514 to i64
  %arrayidx335 = getelementptr inbounds float* %VFtab, i64 %idxprom334
  %51 = load float* %arrayidx335, align 4, !tbaa !3
  %add3361515 = or i32 %mul330, 2
  %idxprom337 = sext i32 %add3361515 to i64
  %arrayidx338 = getelementptr inbounds float* %VFtab, i64 %idxprom337
  %52 = load float* %arrayidx338, align 4, !tbaa !3
  %mul339 = fmul float %sub328, %52
  %add3401516 = or i32 %mul330, 3
  %idxprom341 = sext i32 %add3401516 to i64
  %arrayidx342 = getelementptr inbounds float* %VFtab, i64 %idxprom341
  %53 = load float* %arrayidx342, align 4, !tbaa !3
  %mul343 = fmul float %mul329, %53
  %add344 = fadd float %51, %mul339
  %add345 = fadd float %add344, %mul343
  %mul346 = fmul float %sub328, %add345
  %add347 = fadd float %50, %mul346
  %add348 = fadd float %mul339, %add345
  %mul349 = fmul float %mul343, 2.000000e+00
  %add350 = fadd float %mul349, %add348
  %mul351 = fmul float %mul6, %add347
  %mul352 = fmul float %mul6, %add350
  %mul353 = fmul float %mul352, %tabscale
  %54 = fmul float %conv194, %mul353
  %mul355 = fsub float -0.000000e+00, %54
  %add356 = fadd float %add306, %mul351
  %mul357 = fmul float %sub118, %mul355
  %mul358 = fmul float %sub119, %mul355
  %mul359 = fmul float %sub120, %mul355
  %add360 = fadd float %add310, %mul357
  %add361 = fadd float %add311, %mul358
  %add362 = fadd float %add312, %mul359
  %arrayidx365 = getelementptr inbounds float* %faction, i64 %idxprom94
  %55 = load float* %arrayidx365, align 4, !tbaa !3
  %sub366 = fsub float %55, %mul357
  %arrayidx369 = getelementptr inbounds float* %faction, i64 %idxprom97
  %56 = load float* %arrayidx369, align 4, !tbaa !3
  %sub370 = fsub float %56, %mul358
  %arrayidx373 = getelementptr inbounds float* %faction, i64 %idxprom100
  %57 = load float* %arrayidx373, align 4, !tbaa !3
  %sub374 = fsub float %57, %mul359
  %mul375 = fmul float %add133, %conv206
  %mul376 = fmul float %mul375, %tabscale
  %conv377 = fptosi float %mul376 to i32
  %conv378 = sitofp i32 %conv377 to float
  %sub379 = fsub float %mul376, %conv378
  %mul380 = fmul float %sub379, %sub379
  %mul381 = mul nsw i32 %conv377, 12
  %idxprom382 = sext i32 %mul381 to i64
  %arrayidx383 = getelementptr inbounds float* %VFtab, i64 %idxprom382
  %58 = load float* %arrayidx383, align 4, !tbaa !3
  %add3841517 = or i32 %mul381, 1
  %idxprom385 = sext i32 %add3841517 to i64
  %arrayidx386 = getelementptr inbounds float* %VFtab, i64 %idxprom385
  %59 = load float* %arrayidx386, align 4, !tbaa !3
  %add3871518 = or i32 %mul381, 2
  %idxprom388 = sext i32 %add3871518 to i64
  %arrayidx389 = getelementptr inbounds float* %VFtab, i64 %idxprom388
  %60 = load float* %arrayidx389, align 4, !tbaa !3
  %mul390 = fmul float %sub379, %60
  %add3911519 = or i32 %mul381, 3
  %idxprom392 = sext i32 %add3911519 to i64
  %arrayidx393 = getelementptr inbounds float* %VFtab, i64 %idxprom392
  %61 = load float* %arrayidx393, align 4, !tbaa !3
  %mul394 = fmul float %mul380, %61
  %add395 = fadd float %59, %mul390
  %add396 = fadd float %add395, %mul394
  %mul397 = fmul float %sub379, %add396
  %add398 = fadd float %58, %mul397
  %add399 = fadd float %mul390, %add396
  %mul400 = fmul float %mul394, 2.000000e+00
  %add401 = fadd float %mul400, %add399
  %mul402 = fmul float %mul6, %add398
  %mul403 = fmul float %mul6, %add401
  %mul404 = fmul float %mul403, %tabscale
  %62 = fmul float %conv206, %mul404
  %mul406 = fsub float -0.000000e+00, %62
  %add407 = fadd float %add356, %mul402
  %mul408 = fmul float %sub126, %mul406
  %mul409 = fmul float %sub127, %mul406
  %mul410 = fmul float %sub128, %mul406
  %add411 = fadd float %add360, %mul408
  %add412 = fadd float %add361, %mul409
  %add413 = fadd float %add362, %mul410
  %arrayidx416 = getelementptr inbounds float* %faction, i64 %idxprom103
  %63 = load float* %arrayidx416, align 4, !tbaa !3
  %sub417 = fsub float %63, %mul408
  %arrayidx420 = getelementptr inbounds float* %faction, i64 %idxprom106
  %64 = load float* %arrayidx420, align 4, !tbaa !3
  %sub421 = fsub float %64, %mul409
  %arrayidx424 = getelementptr inbounds float* %faction, i64 %idxprom109
  %65 = load float* %arrayidx424, align 4, !tbaa !3
  %sub425 = fsub float %65, %mul410
  %mul426 = fmul float %add141, %conv186
  %mul427 = fmul float %mul426, %tabscale
  %conv428 = fptosi float %mul427 to i32
  %conv429 = sitofp i32 %conv428 to float
  %sub430 = fsub float %mul427, %conv429
  %mul431 = fmul float %sub430, %sub430
  %mul432 = mul nsw i32 %conv428, 12
  %idxprom433 = sext i32 %mul432 to i64
  %arrayidx434 = getelementptr inbounds float* %VFtab, i64 %idxprom433
  %66 = load float* %arrayidx434, align 4, !tbaa !3
  %add4351520 = or i32 %mul432, 1
  %idxprom436 = sext i32 %add4351520 to i64
  %arrayidx437 = getelementptr inbounds float* %VFtab, i64 %idxprom436
  %67 = load float* %arrayidx437, align 4, !tbaa !3
  %add4381521 = or i32 %mul432, 2
  %idxprom439 = sext i32 %add4381521 to i64
  %arrayidx440 = getelementptr inbounds float* %VFtab, i64 %idxprom439
  %68 = load float* %arrayidx440, align 4, !tbaa !3
  %mul441 = fmul float %sub430, %68
  %add4421522 = or i32 %mul432, 3
  %idxprom443 = sext i32 %add4421522 to i64
  %arrayidx444 = getelementptr inbounds float* %VFtab, i64 %idxprom443
  %69 = load float* %arrayidx444, align 4, !tbaa !3
  %mul445 = fmul float %mul431, %69
  %add446 = fadd float %67, %mul441
  %add447 = fadd float %add446, %mul445
  %mul448 = fmul float %sub430, %add447
  %add449 = fadd float %66, %mul448
  %add450 = fadd float %mul441, %add447
  %mul451 = fmul float %mul445, 2.000000e+00
  %add452 = fadd float %mul451, %add450
  %mul453 = fmul float %mul6, %add449
  %mul454 = fmul float %mul6, %add452
  %mul455 = fmul float %mul454, %tabscale
  %70 = fmul float %conv186, %mul455
  %mul457 = fsub float -0.000000e+00, %70
  %add458 = fadd float %add407, %mul453
  %mul459 = fmul float %sub134, %mul457
  %mul460 = fmul float %sub135, %mul457
  %mul461 = fmul float %sub136, %mul457
  %add462 = fadd float %fix2.01544, %mul459
  %add463 = fadd float %fiy2.01543, %mul460
  %add464 = fadd float %fiz2.01542, %mul461
  %sub465 = fsub float %sub315, %mul459
  %sub466 = fsub float %sub319, %mul460
  %sub467 = fsub float %sub323, %mul461
  %mul468 = fmul float %add149, %conv198
  %mul469 = fmul float %mul468, %tabscale
  %conv470 = fptosi float %mul469 to i32
  %conv471 = sitofp i32 %conv470 to float
  %sub472 = fsub float %mul469, %conv471
  %mul473 = fmul float %sub472, %sub472
  %mul474 = mul nsw i32 %conv470, 12
  %idxprom475 = sext i32 %mul474 to i64
  %arrayidx476 = getelementptr inbounds float* %VFtab, i64 %idxprom475
  %71 = load float* %arrayidx476, align 4, !tbaa !3
  %add4771523 = or i32 %mul474, 1
  %idxprom478 = sext i32 %add4771523 to i64
  %arrayidx479 = getelementptr inbounds float* %VFtab, i64 %idxprom478
  %72 = load float* %arrayidx479, align 4, !tbaa !3
  %add4801524 = or i32 %mul474, 2
  %idxprom481 = sext i32 %add4801524 to i64
  %arrayidx482 = getelementptr inbounds float* %VFtab, i64 %idxprom481
  %73 = load float* %arrayidx482, align 4, !tbaa !3
  %mul483 = fmul float %sub472, %73
  %add4841525 = or i32 %mul474, 3
  %idxprom485 = sext i32 %add4841525 to i64
  %arrayidx486 = getelementptr inbounds float* %VFtab, i64 %idxprom485
  %74 = load float* %arrayidx486, align 4, !tbaa !3
  %mul487 = fmul float %mul473, %74
  %add488 = fadd float %72, %mul483
  %add489 = fadd float %add488, %mul487
  %mul490 = fmul float %sub472, %add489
  %add491 = fadd float %71, %mul490
  %add492 = fadd float %mul483, %add489
  %mul493 = fmul float %mul487, 2.000000e+00
  %add494 = fadd float %mul493, %add492
  %mul495 = fmul float %mul8, %add491
  %mul496 = fmul float %mul8, %add494
  %mul497 = fmul float %mul496, %tabscale
  %75 = fmul float %conv198, %mul497
  %mul499 = fsub float -0.000000e+00, %75
  %add500 = fadd float %add458, %mul495
  %mul501 = fmul float %sub142, %mul499
  %mul502 = fmul float %sub143, %mul499
  %mul503 = fmul float %sub144, %mul499
  %add504 = fadd float %add462, %mul501
  %add505 = fadd float %add463, %mul502
  %add506 = fadd float %add464, %mul503
  %sub507 = fsub float %sub366, %mul501
  %sub508 = fsub float %sub370, %mul502
  %sub509 = fsub float %sub374, %mul503
  %mul510 = fmul float %add157, %conv210
  %mul511 = fmul float %mul510, %tabscale
  %conv512 = fptosi float %mul511 to i32
  %conv513 = sitofp i32 %conv512 to float
  %sub514 = fsub float %mul511, %conv513
  %mul515 = fmul float %sub514, %sub514
  %mul516 = mul nsw i32 %conv512, 12
  %idxprom517 = sext i32 %mul516 to i64
  %arrayidx518 = getelementptr inbounds float* %VFtab, i64 %idxprom517
  %76 = load float* %arrayidx518, align 4, !tbaa !3
  %add5191526 = or i32 %mul516, 1
  %idxprom520 = sext i32 %add5191526 to i64
  %arrayidx521 = getelementptr inbounds float* %VFtab, i64 %idxprom520
  %77 = load float* %arrayidx521, align 4, !tbaa !3
  %add5221527 = or i32 %mul516, 2
  %idxprom523 = sext i32 %add5221527 to i64
  %arrayidx524 = getelementptr inbounds float* %VFtab, i64 %idxprom523
  %78 = load float* %arrayidx524, align 4, !tbaa !3
  %mul525 = fmul float %sub514, %78
  %add5261528 = or i32 %mul516, 3
  %idxprom527 = sext i32 %add5261528 to i64
  %arrayidx528 = getelementptr inbounds float* %VFtab, i64 %idxprom527
  %79 = load float* %arrayidx528, align 4, !tbaa !3
  %mul529 = fmul float %mul515, %79
  %add530 = fadd float %77, %mul525
  %add531 = fadd float %add530, %mul529
  %mul532 = fmul float %sub514, %add531
  %add533 = fadd float %76, %mul532
  %add534 = fadd float %mul525, %add531
  %mul535 = fmul float %mul529, 2.000000e+00
  %add536 = fadd float %mul535, %add534
  %mul537 = fmul float %mul8, %add533
  %mul538 = fmul float %mul8, %add536
  %mul539 = fmul float %mul538, %tabscale
  %80 = fmul float %conv210, %mul539
  %mul541 = fsub float -0.000000e+00, %80
  %add542 = fadd float %add500, %mul537
  %mul543 = fmul float %sub150, %mul541
  %mul544 = fmul float %sub151, %mul541
  %mul545 = fmul float %sub152, %mul541
  %add546 = fadd float %add504, %mul543
  %add547 = fadd float %add505, %mul544
  %add548 = fadd float %add506, %mul545
  %sub549 = fsub float %sub417, %mul543
  %sub550 = fsub float %sub421, %mul544
  %sub551 = fsub float %sub425, %mul545
  %mul552 = fmul float %add165, %conv190
  %mul553 = fmul float %mul552, %tabscale
  %conv554 = fptosi float %mul553 to i32
  %conv555 = sitofp i32 %conv554 to float
  %sub556 = fsub float %mul553, %conv555
  %mul557 = fmul float %sub556, %sub556
  %mul558 = mul nsw i32 %conv554, 12
  %idxprom559 = sext i32 %mul558 to i64
  %arrayidx560 = getelementptr inbounds float* %VFtab, i64 %idxprom559
  %81 = load float* %arrayidx560, align 4, !tbaa !3
  %add5611529 = or i32 %mul558, 1
  %idxprom562 = sext i32 %add5611529 to i64
  %arrayidx563 = getelementptr inbounds float* %VFtab, i64 %idxprom562
  %82 = load float* %arrayidx563, align 4, !tbaa !3
  %add5641530 = or i32 %mul558, 2
  %idxprom565 = sext i32 %add5641530 to i64
  %arrayidx566 = getelementptr inbounds float* %VFtab, i64 %idxprom565
  %83 = load float* %arrayidx566, align 4, !tbaa !3
  %mul567 = fmul float %sub556, %83
  %add5681531 = or i32 %mul558, 3
  %idxprom569 = sext i32 %add5681531 to i64
  %arrayidx570 = getelementptr inbounds float* %VFtab, i64 %idxprom569
  %84 = load float* %arrayidx570, align 4, !tbaa !3
  %mul571 = fmul float %mul557, %84
  %add572 = fadd float %82, %mul567
  %add573 = fadd float %add572, %mul571
  %mul574 = fmul float %sub556, %add573
  %add575 = fadd float %81, %mul574
  %add576 = fadd float %mul567, %add573
  %mul577 = fmul float %mul571, 2.000000e+00
  %add578 = fadd float %mul577, %add576
  %mul579 = fmul float %mul6, %add575
  %mul580 = fmul float %mul6, %add578
  %mul581 = fmul float %mul580, %tabscale
  %85 = fmul float %conv190, %mul581
  %mul583 = fsub float -0.000000e+00, %85
  %add584 = fadd float %add542, %mul579
  %mul585 = fmul float %sub158, %mul583
  %mul586 = fmul float %sub159, %mul583
  %mul587 = fmul float %sub160, %mul583
  %add588 = fadd float %fix3.01541, %mul585
  %add589 = fadd float %fiy3.01540, %mul586
  %add590 = fadd float %fiz3.01539, %mul587
  %sub591 = fsub float %sub465, %mul585
  store float %sub591, float* %arrayidx314, align 4, !tbaa !3
  %sub594 = fsub float %sub466, %mul586
  store float %sub594, float* %arrayidx318, align 4, !tbaa !3
  %sub598 = fsub float %sub467, %mul587
  store float %sub598, float* %arrayidx322, align 4, !tbaa !3
  %mul602 = fmul float %add173, %conv202
  %mul603 = fmul float %mul602, %tabscale
  %conv604 = fptosi float %mul603 to i32
  %conv605 = sitofp i32 %conv604 to float
  %sub606 = fsub float %mul603, %conv605
  %mul607 = fmul float %sub606, %sub606
  %mul608 = mul nsw i32 %conv604, 12
  %idxprom609 = sext i32 %mul608 to i64
  %arrayidx610 = getelementptr inbounds float* %VFtab, i64 %idxprom609
  %86 = load float* %arrayidx610, align 4, !tbaa !3
  %add6111532 = or i32 %mul608, 1
  %idxprom612 = sext i32 %add6111532 to i64
  %arrayidx613 = getelementptr inbounds float* %VFtab, i64 %idxprom612
  %87 = load float* %arrayidx613, align 4, !tbaa !3
  %add6141533 = or i32 %mul608, 2
  %idxprom615 = sext i32 %add6141533 to i64
  %arrayidx616 = getelementptr inbounds float* %VFtab, i64 %idxprom615
  %88 = load float* %arrayidx616, align 4, !tbaa !3
  %mul617 = fmul float %sub606, %88
  %add6181534 = or i32 %mul608, 3
  %idxprom619 = sext i32 %add6181534 to i64
  %arrayidx620 = getelementptr inbounds float* %VFtab, i64 %idxprom619
  %89 = load float* %arrayidx620, align 4, !tbaa !3
  %mul621 = fmul float %mul607, %89
  %add622 = fadd float %87, %mul617
  %add623 = fadd float %add622, %mul621
  %mul624 = fmul float %sub606, %add623
  %add625 = fadd float %86, %mul624
  %add626 = fadd float %mul617, %add623
  %mul627 = fmul float %mul621, 2.000000e+00
  %add628 = fadd float %mul627, %add626
  %mul629 = fmul float %mul8, %add625
  %mul630 = fmul float %mul8, %add628
  %mul631 = fmul float %mul630, %tabscale
  %90 = fmul float %conv202, %mul631
  %mul633 = fsub float -0.000000e+00, %90
  %add634 = fadd float %add584, %mul629
  %mul635 = fmul float %sub166, %mul633
  %mul636 = fmul float %sub167, %mul633
  %mul637 = fmul float %sub168, %mul633
  %add638 = fadd float %add588, %mul635
  %add639 = fadd float %add589, %mul636
  %add640 = fadd float %add590, %mul637
  %sub641 = fsub float %sub507, %mul635
  store float %sub641, float* %arrayidx365, align 4, !tbaa !3
  %sub645 = fsub float %sub508, %mul636
  store float %sub645, float* %arrayidx369, align 4, !tbaa !3
  %sub649 = fsub float %sub509, %mul637
  store float %sub649, float* %arrayidx373, align 4, !tbaa !3
  %mul653 = fmul float %add181, %conv214
  %mul654 = fmul float %mul653, %tabscale
  %conv655 = fptosi float %mul654 to i32
  %conv656 = sitofp i32 %conv655 to float
  %sub657 = fsub float %mul654, %conv656
  %mul658 = fmul float %sub657, %sub657
  %mul659 = mul nsw i32 %conv655, 12
  %idxprom660 = sext i32 %mul659 to i64
  %arrayidx661 = getelementptr inbounds float* %VFtab, i64 %idxprom660
  %91 = load float* %arrayidx661, align 4, !tbaa !3
  %add6621535 = or i32 %mul659, 1
  %idxprom663 = sext i32 %add6621535 to i64
  %arrayidx664 = getelementptr inbounds float* %VFtab, i64 %idxprom663
  %92 = load float* %arrayidx664, align 4, !tbaa !3
  %add6651536 = or i32 %mul659, 2
  %idxprom666 = sext i32 %add6651536 to i64
  %arrayidx667 = getelementptr inbounds float* %VFtab, i64 %idxprom666
  %93 = load float* %arrayidx667, align 4, !tbaa !3
  %mul668 = fmul float %sub657, %93
  %add6691537 = or i32 %mul659, 3
  %idxprom670 = sext i32 %add6691537 to i64
  %arrayidx671 = getelementptr inbounds float* %VFtab, i64 %idxprom670
  %94 = load float* %arrayidx671, align 4, !tbaa !3
  %mul672 = fmul float %mul658, %94
  %add673 = fadd float %92, %mul668
  %add674 = fadd float %add673, %mul672
  %mul675 = fmul float %sub657, %add674
  %add676 = fadd float %91, %mul675
  %add677 = fadd float %mul668, %add674
  %mul678 = fmul float %mul672, 2.000000e+00
  %add679 = fadd float %mul678, %add677
  %mul680 = fmul float %mul8, %add676
  %mul681 = fmul float %mul8, %add679
  %mul682 = fmul float %mul681, %tabscale
  %95 = fmul float %conv214, %mul682
  %mul684 = fsub float -0.000000e+00, %95
  %add685 = fadd float %add634, %mul680
  %mul686 = fmul float %sub174, %mul684
  %mul687 = fmul float %sub175, %mul684
  %mul688 = fmul float %sub176, %mul684
  %add689 = fadd float %add638, %mul686
  %add690 = fadd float %add639, %mul687
  %add691 = fadd float %add640, %mul688
  %sub692 = fsub float %sub549, %mul686
  store float %sub692, float* %arrayidx416, align 4, !tbaa !3
  %sub696 = fsub float %sub550, %mul687
  store float %sub696, float* %arrayidx420, align 4, !tbaa !3
  %sub700 = fsub float %sub551, %mul688
  store float %sub700, float* %arrayidx424, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %96 = trunc i64 %indvars.iv.next to i32
  %cmp80 = icmp slt i32 %96, %13
  br i1 %cmp80, label %for.body81, label %for.end

for.end:                                          ; preds = %for.body81, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add685, %for.body81 ]
  %vnbtot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add299, %for.body81 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add411, %for.body81 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add412, %for.body81 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add413, %for.body81 ]
  %fix2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add546, %for.body81 ]
  %fiy2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add547, %for.body81 ]
  %fiz2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add548, %for.body81 ]
  %fix3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add689, %for.body81 ]
  %fiy3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add690, %for.body81 ]
  %fiz3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add691, %for.body81 ]
  %arrayidx705 = getelementptr inbounds float* %faction, i64 %idxprom44
  %97 = load float* %arrayidx705, align 4, !tbaa !3
  %add706 = fadd float %fix1.0.lcssa, %97
  store float %add706, float* %arrayidx705, align 4, !tbaa !3
  %arrayidx711 = getelementptr inbounds float* %faction, i64 %idxprom48
  %98 = load float* %arrayidx711, align 4, !tbaa !3
  %add712 = fadd float %fiy1.0.lcssa, %98
  store float %add712, float* %arrayidx711, align 4, !tbaa !3
  %arrayidx718 = getelementptr inbounds float* %faction, i64 %idxprom52
  %99 = load float* %arrayidx718, align 4, !tbaa !3
  %add719 = fadd float %fiz1.0.lcssa, %99
  store float %add719, float* %arrayidx718, align 4, !tbaa !3
  %arrayidx725 = getelementptr inbounds float* %faction, i64 %idxprom56
  %100 = load float* %arrayidx725, align 4, !tbaa !3
  %add726 = fadd float %fix2.0.lcssa, %100
  store float %add726, float* %arrayidx725, align 4, !tbaa !3
  %arrayidx732 = getelementptr inbounds float* %faction, i64 %idxprom60
  %101 = load float* %arrayidx732, align 4, !tbaa !3
  %add733 = fadd float %fiy2.0.lcssa, %101
  store float %add733, float* %arrayidx732, align 4, !tbaa !3
  %arrayidx739 = getelementptr inbounds float* %faction, i64 %idxprom64
  %102 = load float* %arrayidx739, align 4, !tbaa !3
  %add740 = fadd float %fiz2.0.lcssa, %102
  store float %add740, float* %arrayidx739, align 4, !tbaa !3
  %arrayidx746 = getelementptr inbounds float* %faction, i64 %idxprom68
  %103 = load float* %arrayidx746, align 4, !tbaa !3
  %add747 = fadd float %fix3.0.lcssa, %103
  store float %add747, float* %arrayidx746, align 4, !tbaa !3
  %arrayidx753 = getelementptr inbounds float* %faction, i64 %idxprom72
  %104 = load float* %arrayidx753, align 4, !tbaa !3
  %add754 = fadd float %fiy3.0.lcssa, %104
  store float %add754, float* %arrayidx753, align 4, !tbaa !3
  %arrayidx760 = getelementptr inbounds float* %faction, i64 %idxprom76
  %105 = load float* %arrayidx760, align 4, !tbaa !3
  %add761 = fadd float %fiz3.0.lcssa, %105
  store float %add761, float* %arrayidx760, align 4, !tbaa !3
  %arrayidx766 = getelementptr inbounds float* %fshift, i64 %idxprom28
  %106 = load float* %arrayidx766, align 4, !tbaa !3
  %add767 = fadd float %fix1.0.lcssa, %106
  %add768 = fadd float %fix2.0.lcssa, %add767
  %add769 = fadd float %fix3.0.lcssa, %add768
  store float %add769, float* %arrayidx766, align 4, !tbaa !3
  %arrayidx774 = getelementptr inbounds float* %fshift, i64 %idxprom31
  %107 = load float* %arrayidx774, align 4, !tbaa !3
  %add775 = fadd float %fiy1.0.lcssa, %107
  %add776 = fadd float %fiy2.0.lcssa, %add775
  %add777 = fadd float %fiy3.0.lcssa, %add776
  store float %add777, float* %arrayidx774, align 4, !tbaa !3
  %arrayidx783 = getelementptr inbounds float* %fshift, i64 %idxprom34
  %108 = load float* %arrayidx783, align 4, !tbaa !3
  %add784 = fadd float %fiz1.0.lcssa, %108
  %add785 = fadd float %fiz2.0.lcssa, %add784
  %add786 = fadd float %fiz3.0.lcssa, %add785
  store float %add786, float* %arrayidx783, align 4, !tbaa !3
  %arrayidx791 = getelementptr inbounds i32* %gid, i64 %indvars.iv1563
  %109 = load i32* %arrayidx791, align 4, !tbaa !0
  %idxprom792 = sext i32 %109 to i64
  %arrayidx793 = getelementptr inbounds float* %Vc, i64 %idxprom792
  %110 = load float* %arrayidx793, align 4, !tbaa !3
  %add794 = fadd float %vctot.0.lcssa, %110
  store float %add794, float* %arrayidx793, align 4, !tbaa !3
  %arrayidx798 = getelementptr inbounds float* %Vnb, i64 %idxprom792
  %111 = load float* %arrayidx798, align 4, !tbaa !3
  %add799 = fadd float %vnbtot.0.lcssa, %111
  store float %add799, float* %arrayidx798, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next1564 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end804, label %for.end.for.body_crit_edge

for.end.for.body_crit_edge:                       ; preds = %for.end
  %arrayidx37.phi.trans.insert = getelementptr inbounds i32* %iinr, i64 %indvars.iv.next1564
  %.pre = load i32* %arrayidx37.phi.trans.insert, align 4, !tbaa !0
  br label %for.body

for.end804:                                       ; preds = %for.end, %entry
  ret void
}
