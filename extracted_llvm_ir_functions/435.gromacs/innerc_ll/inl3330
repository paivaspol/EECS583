define void @inl3330(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc, i32* nocapture %type, i32 %ntype, float* nocapture %nbfp, float* nocapture %Vnb, float %tabscale, float* %VFtab) #0 {
entry:
  %0 = load i32* %iinr, align 4, !tbaa !0
  %idxprom = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds float* %charge, i64 %idxprom
  %1 = load float* %arrayidx1, align 4, !tbaa !3
  %add = add nsw i32 %0, 1
  %idxprom2 = sext i32 %add to i64
  %arrayidx3 = getelementptr inbounds float* %charge, i64 %idxprom2
  %2 = load float* %arrayidx3, align 4, !tbaa !3
  %mul = fmul float %1, %facel
  %mul4 = fmul float %1, %mul
  %mul6 = fmul float %mul, %2
  %mul7 = fmul float %2, %facel
  %mul8 = fmul float %2, %mul7
  %mul9 = shl i32 %ntype, 1
  %arrayidx11 = getelementptr inbounds i32* %type, i64 %idxprom
  %3 = load i32* %arrayidx11, align 4, !tbaa !0
  %mul12 = mul nsw i32 %mul9, %3
  %mul15 = shl nsw i32 %3, 1
  %add16 = add nsw i32 %mul12, %mul15
  %idxprom17 = sext i32 %add16 to i64
  %arrayidx18 = getelementptr inbounds float* %nbfp, i64 %idxprom17
  %4 = load float* %arrayidx18, align 4, !tbaa !3
  %add191487 = or i32 %add16, 1
  %idxprom20 = sext i32 %add191487 to i64
  %arrayidx21 = getelementptr inbounds float* %nbfp, i64 %idxprom20
  %5 = load float* %arrayidx21, align 4, !tbaa !3
  %cmp1538 = icmp sgt i32 %nri, 0
  br i1 %cmp1538, label %for.body, label %for.end792

for.body:                                         ; preds = %entry, %for.end.for.body_crit_edge
  %6 = phi i32 [ %.pre, %for.end.for.body_crit_edge ], [ %0, %entry ]
  %indvars.iv1540 = phi i64 [ %indvars.iv.next1541, %for.end.for.body_crit_edge ], [ 0, %entry ]
  %arrayidx23 = getelementptr inbounds i32* %shift, i64 %indvars.iv1540
  %7 = load i32* %arrayidx23, align 4, !tbaa !0
  %mul24 = mul nsw i32 %7, 3
  %idxprom25 = sext i32 %mul24 to i64
  %arrayidx26 = getelementptr inbounds float* %shiftvec, i64 %idxprom25
  %8 = load float* %arrayidx26, align 4, !tbaa !3
  %add27 = add nsw i32 %mul24, 1
  %idxprom28 = sext i32 %add27 to i64
  %arrayidx29 = getelementptr inbounds float* %shiftvec, i64 %idxprom28
  %9 = load float* %arrayidx29, align 4, !tbaa !3
  %add30 = add nsw i32 %mul24, 2
  %idxprom31 = sext i32 %add30 to i64
  %arrayidx32 = getelementptr inbounds float* %shiftvec, i64 %idxprom31
  %10 = load float* %arrayidx32, align 4, !tbaa !3
  %mul35 = mul nsw i32 %6, 3
  %arrayidx37 = getelementptr inbounds i32* %jindex, i64 %indvars.iv1540
  %11 = load i32* %arrayidx37, align 4, !tbaa !0
  %indvars.iv.next1541 = add i64 %indvars.iv1540, 1
  %arrayidx40 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next1541
  %12 = load i32* %arrayidx40, align 4, !tbaa !0
  %idxprom41 = sext i32 %mul35 to i64
  %arrayidx42 = getelementptr inbounds float* %pos, i64 %idxprom41
  %13 = load float* %arrayidx42, align 4, !tbaa !3
  %add43 = fadd float %8, %13
  %add44 = add nsw i32 %mul35, 1
  %idxprom45 = sext i32 %add44 to i64
  %arrayidx46 = getelementptr inbounds float* %pos, i64 %idxprom45
  %14 = load float* %arrayidx46, align 4, !tbaa !3
  %add47 = fadd float %9, %14
  %add48 = add nsw i32 %mul35, 2
  %idxprom49 = sext i32 %add48 to i64
  %arrayidx50 = getelementptr inbounds float* %pos, i64 %idxprom49
  %15 = load float* %arrayidx50, align 4, !tbaa !3
  %add51 = fadd float %10, %15
  %add52 = add nsw i32 %mul35, 3
  %idxprom53 = sext i32 %add52 to i64
  %arrayidx54 = getelementptr inbounds float* %pos, i64 %idxprom53
  %16 = load float* %arrayidx54, align 4, !tbaa !3
  %add55 = fadd float %8, %16
  %add56 = add nsw i32 %mul35, 4
  %idxprom57 = sext i32 %add56 to i64
  %arrayidx58 = getelementptr inbounds float* %pos, i64 %idxprom57
  %17 = load float* %arrayidx58, align 4, !tbaa !3
  %add59 = fadd float %9, %17
  %add60 = add nsw i32 %mul35, 5
  %idxprom61 = sext i32 %add60 to i64
  %arrayidx62 = getelementptr inbounds float* %pos, i64 %idxprom61
  %18 = load float* %arrayidx62, align 4, !tbaa !3
  %add63 = fadd float %10, %18
  %add64 = add nsw i32 %mul35, 6
  %idxprom65 = sext i32 %add64 to i64
  %arrayidx66 = getelementptr inbounds float* %pos, i64 %idxprom65
  %19 = load float* %arrayidx66, align 4, !tbaa !3
  %add67 = fadd float %8, %19
  %add68 = add nsw i32 %mul35, 7
  %idxprom69 = sext i32 %add68 to i64
  %arrayidx70 = getelementptr inbounds float* %pos, i64 %idxprom69
  %20 = load float* %arrayidx70, align 4, !tbaa !3
  %add71 = fadd float %9, %20
  %add72 = add nsw i32 %mul35, 8
  %idxprom73 = sext i32 %add72 to i64
  %arrayidx74 = getelementptr inbounds float* %pos, i64 %idxprom73
  %21 = load float* %arrayidx74, align 4, !tbaa !3
  %add75 = fadd float %10, %21
  %cmp771515 = icmp slt i32 %11, %12
  br i1 %cmp771515, label %for.body78.lr.ph, label %for.end

for.body78.lr.ph:                                 ; preds = %for.body
  %22 = sext i32 %11 to i64
  br label %for.body78

for.body78:                                       ; preds = %for.body78.lr.ph, %for.body78
  %indvars.iv = phi i64 [ %22, %for.body78.lr.ph ], [ %indvars.iv.next, %for.body78 ]
  %vctot.01526 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add673, %for.body78 ]
  %vnbtot.01525 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add288, %for.body78 ]
  %fix1.01524 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add399, %for.body78 ]
  %fiy1.01523 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add400, %for.body78 ]
  %fiz1.01522 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add401, %for.body78 ]
  %fix2.01521 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add534, %for.body78 ]
  %fiy2.01520 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add535, %for.body78 ]
  %fiz2.01519 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add536, %for.body78 ]
  %fix3.01518 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add677, %for.body78 ]
  %fiy3.01517 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add678, %for.body78 ]
  %fiz3.01516 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add679, %for.body78 ]
  %arrayidx80 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %23 = load i32* %arrayidx80, align 4, !tbaa !0
  %mul81 = mul nsw i32 %23, 3
  %idxprom82 = sext i32 %mul81 to i64
  %arrayidx83 = getelementptr inbounds float* %pos, i64 %idxprom82
  %24 = load float* %arrayidx83, align 4, !tbaa !3
  %add84 = add nsw i32 %mul81, 1
  %idxprom85 = sext i32 %add84 to i64
  %arrayidx86 = getelementptr inbounds float* %pos, i64 %idxprom85
  %25 = load float* %arrayidx86, align 4, !tbaa !3
  %add87 = add nsw i32 %mul81, 2
  %idxprom88 = sext i32 %add87 to i64
  %arrayidx89 = getelementptr inbounds float* %pos, i64 %idxprom88
  %26 = load float* %arrayidx89, align 4, !tbaa !3
  %add90 = add nsw i32 %mul81, 3
  %idxprom91 = sext i32 %add90 to i64
  %arrayidx92 = getelementptr inbounds float* %pos, i64 %idxprom91
  %27 = load float* %arrayidx92, align 4, !tbaa !3
  %add93 = add nsw i32 %mul81, 4
  %idxprom94 = sext i32 %add93 to i64
  %arrayidx95 = getelementptr inbounds float* %pos, i64 %idxprom94
  %28 = load float* %arrayidx95, align 4, !tbaa !3
  %add96 = add nsw i32 %mul81, 5
  %idxprom97 = sext i32 %add96 to i64
  %arrayidx98 = getelementptr inbounds float* %pos, i64 %idxprom97
  %29 = load float* %arrayidx98, align 4, !tbaa !3
  %add99 = add nsw i32 %mul81, 6
  %idxprom100 = sext i32 %add99 to i64
  %arrayidx101 = getelementptr inbounds float* %pos, i64 %idxprom100
  %30 = load float* %arrayidx101, align 4, !tbaa !3
  %add102 = add nsw i32 %mul81, 7
  %idxprom103 = sext i32 %add102 to i64
  %arrayidx104 = getelementptr inbounds float* %pos, i64 %idxprom103
  %31 = load float* %arrayidx104, align 4, !tbaa !3
  %add105 = add nsw i32 %mul81, 8
  %idxprom106 = sext i32 %add105 to i64
  %arrayidx107 = getelementptr inbounds float* %pos, i64 %idxprom106
  %32 = load float* %arrayidx107, align 4, !tbaa !3
  %sub = fsub float %add43, %24
  %sub108 = fsub float %add47, %25
  %sub109 = fsub float %add51, %26
  %mul110 = fmul float %sub, %sub
  %mul111 = fmul float %sub108, %sub108
  %add112 = fadd float %mul110, %mul111
  %mul113 = fmul float %sub109, %sub109
  %add114 = fadd float %add112, %mul113
  %sub115 = fsub float %add43, %27
  %sub116 = fsub float %add47, %28
  %sub117 = fsub float %add51, %29
  %mul118 = fmul float %sub115, %sub115
  %mul119 = fmul float %sub116, %sub116
  %add120 = fadd float %mul118, %mul119
  %mul121 = fmul float %sub117, %sub117
  %add122 = fadd float %add120, %mul121
  %sub123 = fsub float %add43, %30
  %sub124 = fsub float %add47, %31
  %sub125 = fsub float %add51, %32
  %mul126 = fmul float %sub123, %sub123
  %mul127 = fmul float %sub124, %sub124
  %add128 = fadd float %mul126, %mul127
  %mul129 = fmul float %sub125, %sub125
  %add130 = fadd float %add128, %mul129
  %sub131 = fsub float %add55, %24
  %sub132 = fsub float %add59, %25
  %sub133 = fsub float %add63, %26
  %mul134 = fmul float %sub131, %sub131
  %mul135 = fmul float %sub132, %sub132
  %add136 = fadd float %mul134, %mul135
  %mul137 = fmul float %sub133, %sub133
  %add138 = fadd float %add136, %mul137
  %sub139 = fsub float %add55, %27
  %sub140 = fsub float %add59, %28
  %sub141 = fsub float %add63, %29
  %mul142 = fmul float %sub139, %sub139
  %mul143 = fmul float %sub140, %sub140
  %add144 = fadd float %mul142, %mul143
  %mul145 = fmul float %sub141, %sub141
  %add146 = fadd float %add144, %mul145
  %sub147 = fsub float %add55, %30
  %sub148 = fsub float %add59, %31
  %sub149 = fsub float %add63, %32
  %mul150 = fmul float %sub147, %sub147
  %mul151 = fmul float %sub148, %sub148
  %add152 = fadd float %mul150, %mul151
  %mul153 = fmul float %sub149, %sub149
  %add154 = fadd float %add152, %mul153
  %sub155 = fsub float %add67, %24
  %sub156 = fsub float %add71, %25
  %sub157 = fsub float %add75, %26
  %mul158 = fmul float %sub155, %sub155
  %mul159 = fmul float %sub156, %sub156
  %add160 = fadd float %mul158, %mul159
  %mul161 = fmul float %sub157, %sub157
  %add162 = fadd float %add160, %mul161
  %sub163 = fsub float %add67, %27
  %sub164 = fsub float %add71, %28
  %sub165 = fsub float %add75, %29
  %mul166 = fmul float %sub163, %sub163
  %mul167 = fmul float %sub164, %sub164
  %add168 = fadd float %mul166, %mul167
  %mul169 = fmul float %sub165, %sub165
  %add170 = fadd float %add168, %mul169
  %sub171 = fsub float %add67, %30
  %sub172 = fsub float %add71, %31
  %sub173 = fsub float %add75, %32
  %mul174 = fmul float %sub171, %sub171
  %mul175 = fmul float %sub172, %sub172
  %add176 = fadd float %mul174, %mul175
  %mul177 = fmul float %sub173, %sub173
  %add178 = fadd float %add176, %mul177
  %conv = fpext float %add114 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv179 = fptrunc double %div to float
  %conv180 = fpext float %add138 to double
  %call181 = tail call double @sqrt(double %conv180) #2
  %div182 = fdiv double 1.000000e+00, %call181
  %conv183 = fptrunc double %div182 to float
  %conv184 = fpext float %add162 to double
  %call185 = tail call double @sqrt(double %conv184) #2
  %div186 = fdiv double 1.000000e+00, %call185
  %conv187 = fptrunc double %div186 to float
  %conv188 = fpext float %add122 to double
  %call189 = tail call double @sqrt(double %conv188) #2
  %div190 = fdiv double 1.000000e+00, %call189
  %conv191 = fptrunc double %div190 to float
  %conv192 = fpext float %add146 to double
  %call193 = tail call double @sqrt(double %conv192) #2
  %div194 = fdiv double 1.000000e+00, %call193
  %conv195 = fptrunc double %div194 to float
  %conv196 = fpext float %add170 to double
  %call197 = tail call double @sqrt(double %conv196) #2
  %div198 = fdiv double 1.000000e+00, %call197
  %conv199 = fptrunc double %div198 to float
  %conv200 = fpext float %add130 to double
  %call201 = tail call double @sqrt(double %conv200) #2
  %div202 = fdiv double 1.000000e+00, %call201
  %conv203 = fptrunc double %div202 to float
  %conv204 = fpext float %add154 to double
  %call205 = tail call double @sqrt(double %conv204) #2
  %div206 = fdiv double 1.000000e+00, %call205
  %conv207 = fptrunc double %div206 to float
  %conv208 = fpext float %add178 to double
  %call209 = tail call double @sqrt(double %conv208) #2
  %div210 = fdiv double 1.000000e+00, %call209
  %conv211 = fptrunc double %div210 to float
  %mul212 = fmul float %add114, %conv179
  %mul213 = fmul float %mul212, %tabscale
  %conv214 = fptosi float %mul213 to i32
  %conv215 = sitofp i32 %conv214 to float
  %sub216 = fsub float %mul213, %conv215
  %mul217 = fmul float %sub216, %sub216
  %mul218 = mul nsw i32 %conv214, 12
  %idxprom219 = sext i32 %mul218 to i64
  %arrayidx220 = getelementptr inbounds float* %VFtab, i64 %idxprom219
  %33 = load float* %arrayidx220, align 4, !tbaa !3
  %add2211488 = or i32 %mul218, 1
  %idxprom222 = sext i32 %add2211488 to i64
  %arrayidx223 = getelementptr inbounds float* %VFtab, i64 %idxprom222
  %34 = load float* %arrayidx223, align 4, !tbaa !3
  %add2241489 = or i32 %mul218, 2
  %idxprom225 = sext i32 %add2241489 to i64
  %arrayidx226 = getelementptr inbounds float* %VFtab, i64 %idxprom225
  %35 = load float* %arrayidx226, align 4, !tbaa !3
  %mul227 = fmul float %sub216, %35
  %add2281490 = or i32 %mul218, 3
  %idxprom229 = sext i32 %add2281490 to i64
  %arrayidx230 = getelementptr inbounds float* %VFtab, i64 %idxprom229
  %36 = load float* %arrayidx230, align 4, !tbaa !3
  %mul231 = fmul float %mul217, %36
  %add232 = fadd float %34, %mul227
  %add233 = fadd float %add232, %mul231
  %mul234 = fmul float %sub216, %add233
  %add235 = fadd float %33, %mul234
  %add236 = fadd float %mul227, %add233
  %mul237 = fmul float %mul231, 2.000000e+00
  %add238 = fadd float %mul237, %add236
  %mul239 = fmul float %mul4, %add235
  %mul240 = fmul float %mul4, %add238
  %add241 = add nsw i32 %mul218, 4
  %idxprom242 = sext i32 %add241 to i64
  %arrayidx243 = getelementptr inbounds float* %VFtab, i64 %idxprom242
  %37 = load float* %arrayidx243, align 4, !tbaa !3
  %add244 = add nsw i32 %mul218, 5
  %idxprom245 = sext i32 %add244 to i64
  %arrayidx246 = getelementptr inbounds float* %VFtab, i64 %idxprom245
  %38 = load float* %arrayidx246, align 4, !tbaa !3
  %add247 = add nsw i32 %mul218, 6
  %idxprom248 = sext i32 %add247 to i64
  %arrayidx249 = getelementptr inbounds float* %VFtab, i64 %idxprom248
  %39 = load float* %arrayidx249, align 4, !tbaa !3
  %mul250 = fmul float %sub216, %39
  %add251 = add nsw i32 %mul218, 7
  %idxprom252 = sext i32 %add251 to i64
  %arrayidx253 = getelementptr inbounds float* %VFtab, i64 %idxprom252
  %40 = load float* %arrayidx253, align 4, !tbaa !3
  %mul254 = fmul float %mul217, %40
  %add255 = fadd float %38, %mul250
  %add256 = fadd float %add255, %mul254
  %mul257 = fmul float %sub216, %add256
  %add258 = fadd float %37, %mul257
  %add259 = fadd float %mul250, %add256
  %mul260 = fmul float %mul254, 2.000000e+00
  %add261 = fadd float %mul260, %add259
  %mul262 = fmul float %4, %add258
  %mul263 = fmul float %4, %add261
  %add264 = add nsw i32 %mul218, 8
  %idxprom265 = sext i32 %add264 to i64
  %arrayidx266 = getelementptr inbounds float* %VFtab, i64 %idxprom265
  %41 = load float* %arrayidx266, align 4, !tbaa !3
  %add267 = add nsw i32 %mul218, 9
  %idxprom268 = sext i32 %add267 to i64
  %arrayidx269 = getelementptr inbounds float* %VFtab, i64 %idxprom268
  %42 = load float* %arrayidx269, align 4, !tbaa !3
  %add270 = add nsw i32 %mul218, 10
  %idxprom271 = sext i32 %add270 to i64
  %arrayidx272 = getelementptr inbounds float* %VFtab, i64 %idxprom271
  %43 = load float* %arrayidx272, align 4, !tbaa !3
  %mul273 = fmul float %sub216, %43
  %add274 = add nsw i32 %mul218, 11
  %idxprom275 = sext i32 %add274 to i64
  %arrayidx276 = getelementptr inbounds float* %VFtab, i64 %idxprom275
  %44 = load float* %arrayidx276, align 4, !tbaa !3
  %mul277 = fmul float %mul217, %44
  %add278 = fadd float %42, %mul273
  %add279 = fadd float %add278, %mul277
  %mul280 = fmul float %sub216, %add279
  %add281 = fadd float %41, %mul280
  %add282 = fadd float %mul273, %add279
  %mul283 = fmul float %mul277, 2.000000e+00
  %add284 = fadd float %mul283, %add282
  %mul285 = fmul float %5, %add281
  %mul286 = fmul float %5, %add284
  %add287 = fadd float %vnbtot.01525, %mul262
  %add288 = fadd float %add287, %mul285
  %add289 = fadd float %mul240, %mul263
  %add290 = fadd float %add289, %mul286
  %mul291 = fmul float %add290, %tabscale
  %45 = fmul float %conv179, %mul291
  %mul293 = fsub float -0.000000e+00, %45
  %add294 = fadd float %vctot.01526, %mul239
  %mul295 = fmul float %sub, %mul293
  %mul296 = fmul float %sub108, %mul293
  %mul297 = fmul float %sub109, %mul293
  %add298 = fadd float %fix1.01524, %mul295
  %add299 = fadd float %fiy1.01523, %mul296
  %add300 = fadd float %fiz1.01522, %mul297
  %arrayidx302 = getelementptr inbounds float* %faction, i64 %idxprom82
  %46 = load float* %arrayidx302, align 4, !tbaa !3
  %sub303 = fsub float %46, %mul295
  %arrayidx306 = getelementptr inbounds float* %faction, i64 %idxprom85
  %47 = load float* %arrayidx306, align 4, !tbaa !3
  %sub307 = fsub float %47, %mul296
  %arrayidx310 = getelementptr inbounds float* %faction, i64 %idxprom88
  %48 = load float* %arrayidx310, align 4, !tbaa !3
  %sub311 = fsub float %48, %mul297
  %mul312 = fmul float %add122, %conv191
  %mul313 = fmul float %mul312, %tabscale
  %conv314 = fptosi float %mul313 to i32
  %conv315 = sitofp i32 %conv314 to float
  %sub316 = fsub float %mul313, %conv315
  %mul317 = fmul float %sub316, %sub316
  %mul318 = mul nsw i32 %conv314, 12
  %idxprom319 = sext i32 %mul318 to i64
  %arrayidx320 = getelementptr inbounds float* %VFtab, i64 %idxprom319
  %49 = load float* %arrayidx320, align 4, !tbaa !3
  %add3211491 = or i32 %mul318, 1
  %idxprom322 = sext i32 %add3211491 to i64
  %arrayidx323 = getelementptr inbounds float* %VFtab, i64 %idxprom322
  %50 = load float* %arrayidx323, align 4, !tbaa !3
  %add3241492 = or i32 %mul318, 2
  %idxprom325 = sext i32 %add3241492 to i64
  %arrayidx326 = getelementptr inbounds float* %VFtab, i64 %idxprom325
  %51 = load float* %arrayidx326, align 4, !tbaa !3
  %mul327 = fmul float %sub316, %51
  %add3281493 = or i32 %mul318, 3
  %idxprom329 = sext i32 %add3281493 to i64
  %arrayidx330 = getelementptr inbounds float* %VFtab, i64 %idxprom329
  %52 = load float* %arrayidx330, align 4, !tbaa !3
  %mul331 = fmul float %mul317, %52
  %add332 = fadd float %50, %mul327
  %add333 = fadd float %add332, %mul331
  %mul334 = fmul float %sub316, %add333
  %add335 = fadd float %49, %mul334
  %add336 = fadd float %mul327, %add333
  %mul337 = fmul float %mul331, 2.000000e+00
  %add338 = fadd float %mul337, %add336
  %mul339 = fmul float %mul6, %add335
  %mul340 = fmul float %mul6, %add338
  %mul341 = fmul float %mul340, %tabscale
  %53 = fmul float %conv191, %mul341
  %mul343 = fsub float -0.000000e+00, %53
  %add344 = fadd float %add294, %mul339
  %mul345 = fmul float %sub115, %mul343
  %mul346 = fmul float %sub116, %mul343
  %mul347 = fmul float %sub117, %mul343
  %add348 = fadd float %add298, %mul345
  %add349 = fadd float %add299, %mul346
  %add350 = fadd float %add300, %mul347
  %arrayidx353 = getelementptr inbounds float* %faction, i64 %idxprom91
  %54 = load float* %arrayidx353, align 4, !tbaa !3
  %sub354 = fsub float %54, %mul345
  %arrayidx357 = getelementptr inbounds float* %faction, i64 %idxprom94
  %55 = load float* %arrayidx357, align 4, !tbaa !3
  %sub358 = fsub float %55, %mul346
  %arrayidx361 = getelementptr inbounds float* %faction, i64 %idxprom97
  %56 = load float* %arrayidx361, align 4, !tbaa !3
  %sub362 = fsub float %56, %mul347
  %mul363 = fmul float %add130, %conv203
  %mul364 = fmul float %mul363, %tabscale
  %conv365 = fptosi float %mul364 to i32
  %conv366 = sitofp i32 %conv365 to float
  %sub367 = fsub float %mul364, %conv366
  %mul368 = fmul float %sub367, %sub367
  %mul369 = mul nsw i32 %conv365, 12
  %idxprom370 = sext i32 %mul369 to i64
  %arrayidx371 = getelementptr inbounds float* %VFtab, i64 %idxprom370
  %57 = load float* %arrayidx371, align 4, !tbaa !3
  %add3721494 = or i32 %mul369, 1
  %idxprom373 = sext i32 %add3721494 to i64
  %arrayidx374 = getelementptr inbounds float* %VFtab, i64 %idxprom373
  %58 = load float* %arrayidx374, align 4, !tbaa !3
  %add3751495 = or i32 %mul369, 2
  %idxprom376 = sext i32 %add3751495 to i64
  %arrayidx377 = getelementptr inbounds float* %VFtab, i64 %idxprom376
  %59 = load float* %arrayidx377, align 4, !tbaa !3
  %mul378 = fmul float %sub367, %59
  %add3791496 = or i32 %mul369, 3
  %idxprom380 = sext i32 %add3791496 to i64
  %arrayidx381 = getelementptr inbounds float* %VFtab, i64 %idxprom380
  %60 = load float* %arrayidx381, align 4, !tbaa !3
  %mul382 = fmul float %mul368, %60
  %add383 = fadd float %58, %mul378
  %add384 = fadd float %add383, %mul382
  %mul385 = fmul float %sub367, %add384
  %add386 = fadd float %57, %mul385
  %add387 = fadd float %mul378, %add384
  %mul388 = fmul float %mul382, 2.000000e+00
  %add389 = fadd float %mul388, %add387
  %mul390 = fmul float %mul6, %add386
  %mul391 = fmul float %mul6, %add389
  %mul392 = fmul float %mul391, %tabscale
  %61 = fmul float %conv203, %mul392
  %mul394 = fsub float -0.000000e+00, %61
  %add395 = fadd float %add344, %mul390
  %mul396 = fmul float %sub123, %mul394
  %mul397 = fmul float %sub124, %mul394
  %mul398 = fmul float %sub125, %mul394
  %add399 = fadd float %add348, %mul396
  %add400 = fadd float %add349, %mul397
  %add401 = fadd float %add350, %mul398
  %arrayidx404 = getelementptr inbounds float* %faction, i64 %idxprom100
  %62 = load float* %arrayidx404, align 4, !tbaa !3
  %sub405 = fsub float %62, %mul396
  %arrayidx408 = getelementptr inbounds float* %faction, i64 %idxprom103
  %63 = load float* %arrayidx408, align 4, !tbaa !3
  %sub409 = fsub float %63, %mul397
  %arrayidx412 = getelementptr inbounds float* %faction, i64 %idxprom106
  %64 = load float* %arrayidx412, align 4, !tbaa !3
  %sub413 = fsub float %64, %mul398
  %mul414 = fmul float %add138, %conv183
  %mul415 = fmul float %mul414, %tabscale
  %conv416 = fptosi float %mul415 to i32
  %conv417 = sitofp i32 %conv416 to float
  %sub418 = fsub float %mul415, %conv417
  %mul419 = fmul float %sub418, %sub418
  %mul420 = mul nsw i32 %conv416, 12
  %idxprom421 = sext i32 %mul420 to i64
  %arrayidx422 = getelementptr inbounds float* %VFtab, i64 %idxprom421
  %65 = load float* %arrayidx422, align 4, !tbaa !3
  %add4231497 = or i32 %mul420, 1
  %idxprom424 = sext i32 %add4231497 to i64
  %arrayidx425 = getelementptr inbounds float* %VFtab, i64 %idxprom424
  %66 = load float* %arrayidx425, align 4, !tbaa !3
  %add4261498 = or i32 %mul420, 2
  %idxprom427 = sext i32 %add4261498 to i64
  %arrayidx428 = getelementptr inbounds float* %VFtab, i64 %idxprom427
  %67 = load float* %arrayidx428, align 4, !tbaa !3
  %mul429 = fmul float %sub418, %67
  %add4301499 = or i32 %mul420, 3
  %idxprom431 = sext i32 %add4301499 to i64
  %arrayidx432 = getelementptr inbounds float* %VFtab, i64 %idxprom431
  %68 = load float* %arrayidx432, align 4, !tbaa !3
  %mul433 = fmul float %mul419, %68
  %add434 = fadd float %66, %mul429
  %add435 = fadd float %add434, %mul433
  %mul436 = fmul float %sub418, %add435
  %add437 = fadd float %65, %mul436
  %add438 = fadd float %mul429, %add435
  %mul439 = fmul float %mul433, 2.000000e+00
  %add440 = fadd float %mul439, %add438
  %mul441 = fmul float %mul6, %add437
  %mul442 = fmul float %mul6, %add440
  %mul443 = fmul float %mul442, %tabscale
  %69 = fmul float %conv183, %mul443
  %mul445 = fsub float -0.000000e+00, %69
  %add446 = fadd float %add395, %mul441
  %mul447 = fmul float %sub131, %mul445
  %mul448 = fmul float %sub132, %mul445
  %mul449 = fmul float %sub133, %mul445
  %add450 = fadd float %fix2.01521, %mul447
  %add451 = fadd float %fiy2.01520, %mul448
  %add452 = fadd float %fiz2.01519, %mul449
  %sub453 = fsub float %sub303, %mul447
  %sub454 = fsub float %sub307, %mul448
  %sub455 = fsub float %sub311, %mul449
  %mul456 = fmul float %add146, %conv195
  %mul457 = fmul float %mul456, %tabscale
  %conv458 = fptosi float %mul457 to i32
  %conv459 = sitofp i32 %conv458 to float
  %sub460 = fsub float %mul457, %conv459
  %mul461 = fmul float %sub460, %sub460
  %mul462 = mul nsw i32 %conv458, 12
  %idxprom463 = sext i32 %mul462 to i64
  %arrayidx464 = getelementptr inbounds float* %VFtab, i64 %idxprom463
  %70 = load float* %arrayidx464, align 4, !tbaa !3
  %add4651500 = or i32 %mul462, 1
  %idxprom466 = sext i32 %add4651500 to i64
  %arrayidx467 = getelementptr inbounds float* %VFtab, i64 %idxprom466
  %71 = load float* %arrayidx467, align 4, !tbaa !3
  %add4681501 = or i32 %mul462, 2
  %idxprom469 = sext i32 %add4681501 to i64
  %arrayidx470 = getelementptr inbounds float* %VFtab, i64 %idxprom469
  %72 = load float* %arrayidx470, align 4, !tbaa !3
  %mul471 = fmul float %sub460, %72
  %add4721502 = or i32 %mul462, 3
  %idxprom473 = sext i32 %add4721502 to i64
  %arrayidx474 = getelementptr inbounds float* %VFtab, i64 %idxprom473
  %73 = load float* %arrayidx474, align 4, !tbaa !3
  %mul475 = fmul float %mul461, %73
  %add476 = fadd float %71, %mul471
  %add477 = fadd float %add476, %mul475
  %mul478 = fmul float %sub460, %add477
  %add479 = fadd float %70, %mul478
  %add480 = fadd float %mul471, %add477
  %mul481 = fmul float %mul475, 2.000000e+00
  %add482 = fadd float %mul481, %add480
  %mul483 = fmul float %mul8, %add479
  %mul484 = fmul float %mul8, %add482
  %mul485 = fmul float %mul484, %tabscale
  %74 = fmul float %conv195, %mul485
  %mul487 = fsub float -0.000000e+00, %74
  %add488 = fadd float %add446, %mul483
  %mul489 = fmul float %sub139, %mul487
  %mul490 = fmul float %sub140, %mul487
  %mul491 = fmul float %sub141, %mul487
  %add492 = fadd float %add450, %mul489
  %add493 = fadd float %add451, %mul490
  %add494 = fadd float %add452, %mul491
  %sub495 = fsub float %sub354, %mul489
  %sub496 = fsub float %sub358, %mul490
  %sub497 = fsub float %sub362, %mul491
  %mul498 = fmul float %add154, %conv207
  %mul499 = fmul float %mul498, %tabscale
  %conv500 = fptosi float %mul499 to i32
  %conv501 = sitofp i32 %conv500 to float
  %sub502 = fsub float %mul499, %conv501
  %mul503 = fmul float %sub502, %sub502
  %mul504 = mul nsw i32 %conv500, 12
  %idxprom505 = sext i32 %mul504 to i64
  %arrayidx506 = getelementptr inbounds float* %VFtab, i64 %idxprom505
  %75 = load float* %arrayidx506, align 4, !tbaa !3
  %add5071503 = or i32 %mul504, 1
  %idxprom508 = sext i32 %add5071503 to i64
  %arrayidx509 = getelementptr inbounds float* %VFtab, i64 %idxprom508
  %76 = load float* %arrayidx509, align 4, !tbaa !3
  %add5101504 = or i32 %mul504, 2
  %idxprom511 = sext i32 %add5101504 to i64
  %arrayidx512 = getelementptr inbounds float* %VFtab, i64 %idxprom511
  %77 = load float* %arrayidx512, align 4, !tbaa !3
  %mul513 = fmul float %sub502, %77
  %add5141505 = or i32 %mul504, 3
  %idxprom515 = sext i32 %add5141505 to i64
  %arrayidx516 = getelementptr inbounds float* %VFtab, i64 %idxprom515
  %78 = load float* %arrayidx516, align 4, !tbaa !3
  %mul517 = fmul float %mul503, %78
  %add518 = fadd float %76, %mul513
  %add519 = fadd float %add518, %mul517
  %mul520 = fmul float %sub502, %add519
  %add521 = fadd float %75, %mul520
  %add522 = fadd float %mul513, %add519
  %mul523 = fmul float %mul517, 2.000000e+00
  %add524 = fadd float %mul523, %add522
  %mul525 = fmul float %mul8, %add521
  %mul526 = fmul float %mul8, %add524
  %mul527 = fmul float %mul526, %tabscale
  %79 = fmul float %conv207, %mul527
  %mul529 = fsub float -0.000000e+00, %79
  %add530 = fadd float %add488, %mul525
  %mul531 = fmul float %sub147, %mul529
  %mul532 = fmul float %sub148, %mul529
  %mul533 = fmul float %sub149, %mul529
  %add534 = fadd float %add492, %mul531
  %add535 = fadd float %add493, %mul532
  %add536 = fadd float %add494, %mul533
  %sub537 = fsub float %sub405, %mul531
  %sub538 = fsub float %sub409, %mul532
  %sub539 = fsub float %sub413, %mul533
  %mul540 = fmul float %add162, %conv187
  %mul541 = fmul float %mul540, %tabscale
  %conv542 = fptosi float %mul541 to i32
  %conv543 = sitofp i32 %conv542 to float
  %sub544 = fsub float %mul541, %conv543
  %mul545 = fmul float %sub544, %sub544
  %mul546 = mul nsw i32 %conv542, 12
  %idxprom547 = sext i32 %mul546 to i64
  %arrayidx548 = getelementptr inbounds float* %VFtab, i64 %idxprom547
  %80 = load float* %arrayidx548, align 4, !tbaa !3
  %add5491506 = or i32 %mul546, 1
  %idxprom550 = sext i32 %add5491506 to i64
  %arrayidx551 = getelementptr inbounds float* %VFtab, i64 %idxprom550
  %81 = load float* %arrayidx551, align 4, !tbaa !3
  %add5521507 = or i32 %mul546, 2
  %idxprom553 = sext i32 %add5521507 to i64
  %arrayidx554 = getelementptr inbounds float* %VFtab, i64 %idxprom553
  %82 = load float* %arrayidx554, align 4, !tbaa !3
  %mul555 = fmul float %sub544, %82
  %add5561508 = or i32 %mul546, 3
  %idxprom557 = sext i32 %add5561508 to i64
  %arrayidx558 = getelementptr inbounds float* %VFtab, i64 %idxprom557
  %83 = load float* %arrayidx558, align 4, !tbaa !3
  %mul559 = fmul float %mul545, %83
  %add560 = fadd float %81, %mul555
  %add561 = fadd float %add560, %mul559
  %mul562 = fmul float %sub544, %add561
  %add563 = fadd float %80, %mul562
  %add564 = fadd float %mul555, %add561
  %mul565 = fmul float %mul559, 2.000000e+00
  %add566 = fadd float %mul565, %add564
  %mul567 = fmul float %mul6, %add563
  %mul568 = fmul float %mul6, %add566
  %mul569 = fmul float %mul568, %tabscale
  %84 = fmul float %conv187, %mul569
  %mul571 = fsub float -0.000000e+00, %84
  %add572 = fadd float %add530, %mul567
  %mul573 = fmul float %sub155, %mul571
  %mul574 = fmul float %sub156, %mul571
  %mul575 = fmul float %sub157, %mul571
  %add576 = fadd float %fix3.01518, %mul573
  %add577 = fadd float %fiy3.01517, %mul574
  %add578 = fadd float %fiz3.01516, %mul575
  %sub579 = fsub float %sub453, %mul573
  store float %sub579, float* %arrayidx302, align 4, !tbaa !3
  %sub582 = fsub float %sub454, %mul574
  store float %sub582, float* %arrayidx306, align 4, !tbaa !3
  %sub586 = fsub float %sub455, %mul575
  store float %sub586, float* %arrayidx310, align 4, !tbaa !3
  %mul590 = fmul float %add170, %conv199
  %mul591 = fmul float %mul590, %tabscale
  %conv592 = fptosi float %mul591 to i32
  %conv593 = sitofp i32 %conv592 to float
  %sub594 = fsub float %mul591, %conv593
  %mul595 = fmul float %sub594, %sub594
  %mul596 = mul nsw i32 %conv592, 12
  %idxprom597 = sext i32 %mul596 to i64
  %arrayidx598 = getelementptr inbounds float* %VFtab, i64 %idxprom597
  %85 = load float* %arrayidx598, align 4, !tbaa !3
  %add5991509 = or i32 %mul596, 1
  %idxprom600 = sext i32 %add5991509 to i64
  %arrayidx601 = getelementptr inbounds float* %VFtab, i64 %idxprom600
  %86 = load float* %arrayidx601, align 4, !tbaa !3
  %add6021510 = or i32 %mul596, 2
  %idxprom603 = sext i32 %add6021510 to i64
  %arrayidx604 = getelementptr inbounds float* %VFtab, i64 %idxprom603
  %87 = load float* %arrayidx604, align 4, !tbaa !3
  %mul605 = fmul float %sub594, %87
  %add6061511 = or i32 %mul596, 3
  %idxprom607 = sext i32 %add6061511 to i64
  %arrayidx608 = getelementptr inbounds float* %VFtab, i64 %idxprom607
  %88 = load float* %arrayidx608, align 4, !tbaa !3
  %mul609 = fmul float %mul595, %88
  %add610 = fadd float %86, %mul605
  %add611 = fadd float %add610, %mul609
  %mul612 = fmul float %sub594, %add611
  %add613 = fadd float %85, %mul612
  %add614 = fadd float %mul605, %add611
  %mul615 = fmul float %mul609, 2.000000e+00
  %add616 = fadd float %mul615, %add614
  %mul617 = fmul float %mul8, %add613
  %mul618 = fmul float %mul8, %add616
  %mul619 = fmul float %mul618, %tabscale
  %89 = fmul float %conv199, %mul619
  %mul621 = fsub float -0.000000e+00, %89
  %add622 = fadd float %add572, %mul617
  %mul623 = fmul float %sub163, %mul621
  %mul624 = fmul float %sub164, %mul621
  %mul625 = fmul float %sub165, %mul621
  %add626 = fadd float %add576, %mul623
  %add627 = fadd float %add577, %mul624
  %add628 = fadd float %add578, %mul625
  %sub629 = fsub float %sub495, %mul623
  store float %sub629, float* %arrayidx353, align 4, !tbaa !3
  %sub633 = fsub float %sub496, %mul624
  store float %sub633, float* %arrayidx357, align 4, !tbaa !3
  %sub637 = fsub float %sub497, %mul625
  store float %sub637, float* %arrayidx361, align 4, !tbaa !3
  %mul641 = fmul float %add178, %conv211
  %mul642 = fmul float %mul641, %tabscale
  %conv643 = fptosi float %mul642 to i32
  %conv644 = sitofp i32 %conv643 to float
  %sub645 = fsub float %mul642, %conv644
  %mul646 = fmul float %sub645, %sub645
  %mul647 = mul nsw i32 %conv643, 12
  %idxprom648 = sext i32 %mul647 to i64
  %arrayidx649 = getelementptr inbounds float* %VFtab, i64 %idxprom648
  %90 = load float* %arrayidx649, align 4, !tbaa !3
  %add6501512 = or i32 %mul647, 1
  %idxprom651 = sext i32 %add6501512 to i64
  %arrayidx652 = getelementptr inbounds float* %VFtab, i64 %idxprom651
  %91 = load float* %arrayidx652, align 4, !tbaa !3
  %add6531513 = or i32 %mul647, 2
  %idxprom654 = sext i32 %add6531513 to i64
  %arrayidx655 = getelementptr inbounds float* %VFtab, i64 %idxprom654
  %92 = load float* %arrayidx655, align 4, !tbaa !3
  %mul656 = fmul float %sub645, %92
  %add6571514 = or i32 %mul647, 3
  %idxprom658 = sext i32 %add6571514 to i64
  %arrayidx659 = getelementptr inbounds float* %VFtab, i64 %idxprom658
  %93 = load float* %arrayidx659, align 4, !tbaa !3
  %mul660 = fmul float %mul646, %93
  %add661 = fadd float %91, %mul656
  %add662 = fadd float %add661, %mul660
  %mul663 = fmul float %sub645, %add662
  %add664 = fadd float %90, %mul663
  %add665 = fadd float %mul656, %add662
  %mul666 = fmul float %mul660, 2.000000e+00
  %add667 = fadd float %mul666, %add665
  %mul668 = fmul float %mul8, %add664
  %mul669 = fmul float %mul8, %add667
  %mul670 = fmul float %mul669, %tabscale
  %94 = fmul float %conv211, %mul670
  %mul672 = fsub float -0.000000e+00, %94
  %add673 = fadd float %add622, %mul668
  %mul674 = fmul float %sub171, %mul672
  %mul675 = fmul float %sub172, %mul672
  %mul676 = fmul float %sub173, %mul672
  %add677 = fadd float %add626, %mul674
  %add678 = fadd float %add627, %mul675
  %add679 = fadd float %add628, %mul676
  %sub680 = fsub float %sub537, %mul674
  store float %sub680, float* %arrayidx404, align 4, !tbaa !3
  %sub684 = fsub float %sub538, %mul675
  store float %sub684, float* %arrayidx408, align 4, !tbaa !3
  %sub688 = fsub float %sub539, %mul676
  store float %sub688, float* %arrayidx412, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %95 = trunc i64 %indvars.iv.next to i32
  %cmp77 = icmp slt i32 %95, %12
  br i1 %cmp77, label %for.body78, label %for.end

for.end:                                          ; preds = %for.body78, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add673, %for.body78 ]
  %vnbtot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add288, %for.body78 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add399, %for.body78 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add400, %for.body78 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add401, %for.body78 ]
  %fix2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add534, %for.body78 ]
  %fiy2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add535, %for.body78 ]
  %fiz2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add536, %for.body78 ]
  %fix3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add677, %for.body78 ]
  %fiy3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add678, %for.body78 ]
  %fiz3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add679, %for.body78 ]
  %arrayidx693 = getelementptr inbounds float* %faction, i64 %idxprom41
  %96 = load float* %arrayidx693, align 4, !tbaa !3
  %add694 = fadd float %fix1.0.lcssa, %96
  store float %add694, float* %arrayidx693, align 4, !tbaa !3
  %arrayidx699 = getelementptr inbounds float* %faction, i64 %idxprom45
  %97 = load float* %arrayidx699, align 4, !tbaa !3
  %add700 = fadd float %fiy1.0.lcssa, %97
  store float %add700, float* %arrayidx699, align 4, !tbaa !3
  %arrayidx706 = getelementptr inbounds float* %faction, i64 %idxprom49
  %98 = load float* %arrayidx706, align 4, !tbaa !3
  %add707 = fadd float %fiz1.0.lcssa, %98
  store float %add707, float* %arrayidx706, align 4, !tbaa !3
  %arrayidx713 = getelementptr inbounds float* %faction, i64 %idxprom53
  %99 = load float* %arrayidx713, align 4, !tbaa !3
  %add714 = fadd float %fix2.0.lcssa, %99
  store float %add714, float* %arrayidx713, align 4, !tbaa !3
  %arrayidx720 = getelementptr inbounds float* %faction, i64 %idxprom57
  %100 = load float* %arrayidx720, align 4, !tbaa !3
  %add721 = fadd float %fiy2.0.lcssa, %100
  store float %add721, float* %arrayidx720, align 4, !tbaa !3
  %arrayidx727 = getelementptr inbounds float* %faction, i64 %idxprom61
  %101 = load float* %arrayidx727, align 4, !tbaa !3
  %add728 = fadd float %fiz2.0.lcssa, %101
  store float %add728, float* %arrayidx727, align 4, !tbaa !3
  %arrayidx734 = getelementptr inbounds float* %faction, i64 %idxprom65
  %102 = load float* %arrayidx734, align 4, !tbaa !3
  %add735 = fadd float %fix3.0.lcssa, %102
  store float %add735, float* %arrayidx734, align 4, !tbaa !3
  %arrayidx741 = getelementptr inbounds float* %faction, i64 %idxprom69
  %103 = load float* %arrayidx741, align 4, !tbaa !3
  %add742 = fadd float %fiy3.0.lcssa, %103
  store float %add742, float* %arrayidx741, align 4, !tbaa !3
  %arrayidx748 = getelementptr inbounds float* %faction, i64 %idxprom73
  %104 = load float* %arrayidx748, align 4, !tbaa !3
  %add749 = fadd float %fiz3.0.lcssa, %104
  store float %add749, float* %arrayidx748, align 4, !tbaa !3
  %arrayidx754 = getelementptr inbounds float* %fshift, i64 %idxprom25
  %105 = load float* %arrayidx754, align 4, !tbaa !3
  %add755 = fadd float %fix1.0.lcssa, %105
  %add756 = fadd float %fix2.0.lcssa, %add755
  %add757 = fadd float %fix3.0.lcssa, %add756
  store float %add757, float* %arrayidx754, align 4, !tbaa !3
  %arrayidx762 = getelementptr inbounds float* %fshift, i64 %idxprom28
  %106 = load float* %arrayidx762, align 4, !tbaa !3
  %add763 = fadd float %fiy1.0.lcssa, %106
  %add764 = fadd float %fiy2.0.lcssa, %add763
  %add765 = fadd float %fiy3.0.lcssa, %add764
  store float %add765, float* %arrayidx762, align 4, !tbaa !3
  %arrayidx771 = getelementptr inbounds float* %fshift, i64 %idxprom31
  %107 = load float* %arrayidx771, align 4, !tbaa !3
  %add772 = fadd float %fiz1.0.lcssa, %107
  %add773 = fadd float %fiz2.0.lcssa, %add772
  %add774 = fadd float %fiz3.0.lcssa, %add773
  store float %add774, float* %arrayidx771, align 4, !tbaa !3
  %arrayidx779 = getelementptr inbounds i32* %gid, i64 %indvars.iv1540
  %108 = load i32* %arrayidx779, align 4, !tbaa !0
  %idxprom780 = sext i32 %108 to i64
  %arrayidx781 = getelementptr inbounds float* %Vc, i64 %idxprom780
  %109 = load float* %arrayidx781, align 4, !tbaa !3
  %add782 = fadd float %vctot.0.lcssa, %109
  store float %add782, float* %arrayidx781, align 4, !tbaa !3
  %arrayidx786 = getelementptr inbounds float* %Vnb, i64 %idxprom780
  %110 = load float* %arrayidx786, align 4, !tbaa !3
  %add787 = fadd float %vnbtot.0.lcssa, %110
  store float %add787, float* %arrayidx786, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next1541 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end792, label %for.end.for.body_crit_edge

for.end.for.body_crit_edge:                       ; preds = %for.end
  %arrayidx34.phi.trans.insert = getelementptr inbounds i32* %iinr, i64 %indvars.iv.next1541
  %.pre = load i32* %arrayidx34.phi.trans.insert, align 4, !tbaa !0
  br label %for.body

for.end792:                                       ; preds = %for.end, %entry
  ret void
}
