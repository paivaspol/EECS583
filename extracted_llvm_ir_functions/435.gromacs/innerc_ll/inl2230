define void @inl2230(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc, float %krf, float %crf, i32* nocapture %type, i32 %ntype, float* nocapture %nbfp, float* nocapture %Vnb) #0 {
entry:
  %0 = load i32* %iinr, align 4, !tbaa !0
  %idxprom = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds float* %charge, i64 %idxprom
  %1 = load float* %arrayidx1, align 4, !tbaa !3
  %add = add nsw i32 %0, 1
  %idxprom2 = sext i32 %add to i64
  %arrayidx3 = getelementptr inbounds float* %charge, i64 %idxprom2
  %2 = load float* %arrayidx3, align 4, !tbaa !3
  %mul = fmul float %1, %facel
  %mul4 = fmul float %1, %mul
  %mul6 = fmul float %mul, %2
  %mul7 = fmul float %2, %facel
  %mul8 = fmul float %2, %mul7
  %mul9 = mul nsw i32 %ntype, 3
  %arrayidx11 = getelementptr inbounds i32* %type, i64 %idxprom
  %3 = load i32* %arrayidx11, align 4, !tbaa !0
  %mul121002 = add i32 %mul9, 3
  %add16 = mul i32 %3, %mul121002
  %idxprom17 = sext i32 %add16 to i64
  %arrayidx18 = getelementptr inbounds float* %nbfp, i64 %idxprom17
  %4 = load float* %arrayidx18, align 4, !tbaa !3
  %add22 = add nsw i32 %add16, 2
  %idxprom23 = sext i32 %add22 to i64
  %arrayidx24 = getelementptr inbounds float* %nbfp, i64 %idxprom23
  %5 = load float* %arrayidx24, align 4, !tbaa !3
  %cmp1026 = icmp sgt i32 %nri, 0
  br i1 %cmp1026, label %for.body.lr.ph, label %for.end555

for.body.lr.ph:                                   ; preds = %entry
  %add19 = add nsw i32 %add16, 1
  %idxprom20 = sext i32 %add19 to i64
  %arrayidx21 = getelementptr inbounds float* %nbfp, i64 %idxprom20
  %6 = load float* %arrayidx21, align 4, !tbaa !3
  %conv221 = fpext float %6 to double
  br label %for.body

for.body:                                         ; preds = %for.end.for.body_crit_edge, %for.body.lr.ph
  %7 = phi i32 [ %0, %for.body.lr.ph ], [ %.pre, %for.end.for.body_crit_edge ]
  %indvars.iv1028 = phi i64 [ 0, %for.body.lr.ph ], [ %indvars.iv.next1029, %for.end.for.body_crit_edge ]
  %arrayidx26 = getelementptr inbounds i32* %shift, i64 %indvars.iv1028
  %8 = load i32* %arrayidx26, align 4, !tbaa !0
  %mul27 = mul nsw i32 %8, 3
  %idxprom28 = sext i32 %mul27 to i64
  %arrayidx29 = getelementptr inbounds float* %shiftvec, i64 %idxprom28
  %9 = load float* %arrayidx29, align 4, !tbaa !3
  %add30 = add nsw i32 %mul27, 1
  %idxprom31 = sext i32 %add30 to i64
  %arrayidx32 = getelementptr inbounds float* %shiftvec, i64 %idxprom31
  %10 = load float* %arrayidx32, align 4, !tbaa !3
  %add33 = add nsw i32 %mul27, 2
  %idxprom34 = sext i32 %add33 to i64
  %arrayidx35 = getelementptr inbounds float* %shiftvec, i64 %idxprom34
  %11 = load float* %arrayidx35, align 4, !tbaa !3
  %mul38 = mul nsw i32 %7, 3
  %arrayidx40 = getelementptr inbounds i32* %jindex, i64 %indvars.iv1028
  %12 = load i32* %arrayidx40, align 4, !tbaa !0
  %indvars.iv.next1029 = add i64 %indvars.iv1028, 1
  %arrayidx43 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next1029
  %13 = load i32* %arrayidx43, align 4, !tbaa !0
  %idxprom44 = sext i32 %mul38 to i64
  %arrayidx45 = getelementptr inbounds float* %pos, i64 %idxprom44
  %14 = load float* %arrayidx45, align 4, !tbaa !3
  %add46 = fadd float %9, %14
  %add47 = add nsw i32 %mul38, 1
  %idxprom48 = sext i32 %add47 to i64
  %arrayidx49 = getelementptr inbounds float* %pos, i64 %idxprom48
  %15 = load float* %arrayidx49, align 4, !tbaa !3
  %add50 = fadd float %10, %15
  %add51 = add nsw i32 %mul38, 2
  %idxprom52 = sext i32 %add51 to i64
  %arrayidx53 = getelementptr inbounds float* %pos, i64 %idxprom52
  %16 = load float* %arrayidx53, align 4, !tbaa !3
  %add54 = fadd float %11, %16
  %add55 = add nsw i32 %mul38, 3
  %idxprom56 = sext i32 %add55 to i64
  %arrayidx57 = getelementptr inbounds float* %pos, i64 %idxprom56
  %17 = load float* %arrayidx57, align 4, !tbaa !3
  %add58 = fadd float %9, %17
  %add59 = add nsw i32 %mul38, 4
  %idxprom60 = sext i32 %add59 to i64
  %arrayidx61 = getelementptr inbounds float* %pos, i64 %idxprom60
  %18 = load float* %arrayidx61, align 4, !tbaa !3
  %add62 = fadd float %10, %18
  %add63 = add nsw i32 %mul38, 5
  %idxprom64 = sext i32 %add63 to i64
  %arrayidx65 = getelementptr inbounds float* %pos, i64 %idxprom64
  %19 = load float* %arrayidx65, align 4, !tbaa !3
  %add66 = fadd float %11, %19
  %add67 = add nsw i32 %mul38, 6
  %idxprom68 = sext i32 %add67 to i64
  %arrayidx69 = getelementptr inbounds float* %pos, i64 %idxprom68
  %20 = load float* %arrayidx69, align 4, !tbaa !3
  %add70 = fadd float %9, %20
  %add71 = add nsw i32 %mul38, 7
  %idxprom72 = sext i32 %add71 to i64
  %arrayidx73 = getelementptr inbounds float* %pos, i64 %idxprom72
  %21 = load float* %arrayidx73, align 4, !tbaa !3
  %add74 = fadd float %10, %21
  %add75 = add nsw i32 %mul38, 8
  %idxprom76 = sext i32 %add75 to i64
  %arrayidx77 = getelementptr inbounds float* %pos, i64 %idxprom76
  %22 = load float* %arrayidx77, align 4, !tbaa !3
  %add78 = fadd float %11, %22
  %cmp801003 = icmp slt i32 %12, %13
  br i1 %cmp801003, label %for.body81.lr.ph, label %for.end

for.body81.lr.ph:                                 ; preds = %for.body
  %23 = sext i32 %12 to i64
  br label %for.body81

for.body81:                                       ; preds = %for.body81.lr.ph, %for.body81
  %indvars.iv = phi i64 [ %23, %for.body81.lr.ph ], [ %indvars.iv.next, %for.body81 ]
  %vctot.01014 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add436, %for.body81 ]
  %vnbtot.01013 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %sub228, %for.body81 ]
  %fix1.01012 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add300, %for.body81 ]
  %fiy1.01011 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add301, %for.body81 ]
  %fiz1.01010 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add302, %for.body81 ]
  %fix2.01009 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add366, %for.body81 ]
  %fiy2.01008 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add367, %for.body81 ]
  %fiz2.01007 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add368, %for.body81 ]
  %fix3.01006 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add440, %for.body81 ]
  %fiy3.01005 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add441, %for.body81 ]
  %fiz3.01004 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add442, %for.body81 ]
  %arrayidx83 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %24 = load i32* %arrayidx83, align 4, !tbaa !0
  %mul84 = mul nsw i32 %24, 3
  %idxprom85 = sext i32 %mul84 to i64
  %arrayidx86 = getelementptr inbounds float* %pos, i64 %idxprom85
  %25 = load float* %arrayidx86, align 4, !tbaa !3
  %add87 = add nsw i32 %mul84, 1
  %idxprom88 = sext i32 %add87 to i64
  %arrayidx89 = getelementptr inbounds float* %pos, i64 %idxprom88
  %26 = load float* %arrayidx89, align 4, !tbaa !3
  %add90 = add nsw i32 %mul84, 2
  %idxprom91 = sext i32 %add90 to i64
  %arrayidx92 = getelementptr inbounds float* %pos, i64 %idxprom91
  %27 = load float* %arrayidx92, align 4, !tbaa !3
  %add93 = add nsw i32 %mul84, 3
  %idxprom94 = sext i32 %add93 to i64
  %arrayidx95 = getelementptr inbounds float* %pos, i64 %idxprom94
  %28 = load float* %arrayidx95, align 4, !tbaa !3
  %add96 = add nsw i32 %mul84, 4
  %idxprom97 = sext i32 %add96 to i64
  %arrayidx98 = getelementptr inbounds float* %pos, i64 %idxprom97
  %29 = load float* %arrayidx98, align 4, !tbaa !3
  %add99 = add nsw i32 %mul84, 5
  %idxprom100 = sext i32 %add99 to i64
  %arrayidx101 = getelementptr inbounds float* %pos, i64 %idxprom100
  %30 = load float* %arrayidx101, align 4, !tbaa !3
  %add102 = add nsw i32 %mul84, 6
  %idxprom103 = sext i32 %add102 to i64
  %arrayidx104 = getelementptr inbounds float* %pos, i64 %idxprom103
  %31 = load float* %arrayidx104, align 4, !tbaa !3
  %add105 = add nsw i32 %mul84, 7
  %idxprom106 = sext i32 %add105 to i64
  %arrayidx107 = getelementptr inbounds float* %pos, i64 %idxprom106
  %32 = load float* %arrayidx107, align 4, !tbaa !3
  %add108 = add nsw i32 %mul84, 8
  %idxprom109 = sext i32 %add108 to i64
  %arrayidx110 = getelementptr inbounds float* %pos, i64 %idxprom109
  %33 = load float* %arrayidx110, align 4, !tbaa !3
  %sub = fsub float %add46, %25
  %sub111 = fsub float %add50, %26
  %sub112 = fsub float %add54, %27
  %mul113 = fmul float %sub, %sub
  %mul114 = fmul float %sub111, %sub111
  %add115 = fadd float %mul113, %mul114
  %mul116 = fmul float %sub112, %sub112
  %add117 = fadd float %add115, %mul116
  %sub118 = fsub float %add46, %28
  %sub119 = fsub float %add50, %29
  %sub120 = fsub float %add54, %30
  %mul121 = fmul float %sub118, %sub118
  %mul122 = fmul float %sub119, %sub119
  %add123 = fadd float %mul121, %mul122
  %mul124 = fmul float %sub120, %sub120
  %add125 = fadd float %add123, %mul124
  %sub126 = fsub float %add46, %31
  %sub127 = fsub float %add50, %32
  %sub128 = fsub float %add54, %33
  %mul129 = fmul float %sub126, %sub126
  %mul130 = fmul float %sub127, %sub127
  %add131 = fadd float %mul129, %mul130
  %mul132 = fmul float %sub128, %sub128
  %add133 = fadd float %add131, %mul132
  %sub134 = fsub float %add58, %25
  %sub135 = fsub float %add62, %26
  %sub136 = fsub float %add66, %27
  %mul137 = fmul float %sub134, %sub134
  %mul138 = fmul float %sub135, %sub135
  %add139 = fadd float %mul137, %mul138
  %mul140 = fmul float %sub136, %sub136
  %add141 = fadd float %add139, %mul140
  %sub142 = fsub float %add58, %28
  %sub143 = fsub float %add62, %29
  %sub144 = fsub float %add66, %30
  %mul145 = fmul float %sub142, %sub142
  %mul146 = fmul float %sub143, %sub143
  %add147 = fadd float %mul145, %mul146
  %mul148 = fmul float %sub144, %sub144
  %add149 = fadd float %add147, %mul148
  %sub150 = fsub float %add58, %31
  %sub151 = fsub float %add62, %32
  %sub152 = fsub float %add66, %33
  %mul153 = fmul float %sub150, %sub150
  %mul154 = fmul float %sub151, %sub151
  %add155 = fadd float %mul153, %mul154
  %mul156 = fmul float %sub152, %sub152
  %add157 = fadd float %add155, %mul156
  %sub158 = fsub float %add70, %25
  %sub159 = fsub float %add74, %26
  %sub160 = fsub float %add78, %27
  %mul161 = fmul float %sub158, %sub158
  %mul162 = fmul float %sub159, %sub159
  %add163 = fadd float %mul161, %mul162
  %mul164 = fmul float %sub160, %sub160
  %add165 = fadd float %add163, %mul164
  %sub166 = fsub float %add70, %28
  %sub167 = fsub float %add74, %29
  %sub168 = fsub float %add78, %30
  %mul169 = fmul float %sub166, %sub166
  %mul170 = fmul float %sub167, %sub167
  %add171 = fadd float %mul169, %mul170
  %mul172 = fmul float %sub168, %sub168
  %add173 = fadd float %add171, %mul172
  %sub174 = fsub float %add70, %31
  %sub175 = fsub float %add74, %32
  %sub176 = fsub float %add78, %33
  %mul177 = fmul float %sub174, %sub174
  %mul178 = fmul float %sub175, %sub175
  %add179 = fadd float %mul177, %mul178
  %mul180 = fmul float %sub176, %sub176
  %add181 = fadd float %add179, %mul180
  %conv = fpext float %add117 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv182 = fptrunc double %div to float
  %conv183 = fpext float %add141 to double
  %call184 = tail call double @sqrt(double %conv183) #2
  %div185 = fdiv double 1.000000e+00, %call184
  %conv186 = fptrunc double %div185 to float
  %conv187 = fpext float %add165 to double
  %call188 = tail call double @sqrt(double %conv187) #2
  %div189 = fdiv double 1.000000e+00, %call188
  %conv190 = fptrunc double %div189 to float
  %conv191 = fpext float %add125 to double
  %call192 = tail call double @sqrt(double %conv191) #2
  %div193 = fdiv double 1.000000e+00, %call192
  %conv194 = fptrunc double %div193 to float
  %conv195 = fpext float %add149 to double
  %call196 = tail call double @sqrt(double %conv195) #2
  %div197 = fdiv double 1.000000e+00, %call196
  %conv198 = fptrunc double %div197 to float
  %conv199 = fpext float %add173 to double
  %call200 = tail call double @sqrt(double %conv199) #2
  %div201 = fdiv double 1.000000e+00, %call200
  %conv202 = fptrunc double %div201 to float
  %conv203 = fpext float %add133 to double
  %call204 = tail call double @sqrt(double %conv203) #2
  %div205 = fdiv double 1.000000e+00, %call204
  %conv206 = fptrunc double %div205 to float
  %conv207 = fpext float %add157 to double
  %call208 = tail call double @sqrt(double %conv207) #2
  %div209 = fdiv double 1.000000e+00, %call208
  %conv210 = fptrunc double %div209 to float
  %conv211 = fpext float %add181 to double
  %call212 = tail call double @sqrt(double %conv211) #2
  %div213 = fdiv double 1.000000e+00, %call212
  %conv214 = fptrunc double %div213 to float
  %mul215 = fmul float %add117, %conv182
  %mul216 = fmul float %conv182, %conv182
  %mul217 = fmul float %mul216, %mul216
  %mul218 = fmul float %mul216, %mul217
  %mul219 = fmul float %4, %mul218
  %mul220 = fmul float %5, %mul215
  %sub222 = fsub float -0.000000e+00, %mul220
  %conv223 = fpext float %sub222 to double
  %call224 = tail call double @exp(double %conv223) #2
  %mul225 = fmul double %conv221, %call224
  %conv226 = fptrunc double %mul225 to float
  %add227 = fadd float %vnbtot.01013, %conv226
  %sub228 = fsub float %add227, %mul219
  %mul229 = fmul float %add117, %krf
  %add230 = fadd float %mul229, %conv182
  %sub231 = fsub float %add230, %crf
  %mul232 = fmul float %mul4, %sub231
  %mul233 = fmul float %mul220, %conv226
  %mul234 = fmul float %mul219, 6.000000e+00
  %sub235 = fsub float %mul233, %mul234
  %mul236 = fmul float %mul229, 2.000000e+00
  %sub237 = fsub float %conv182, %mul236
  %mul238 = fmul float %mul4, %sub237
  %add239 = fadd float %mul238, %sub235
  %mul240 = fmul float %mul216, %add239
  %add241 = fadd float %vctot.01014, %mul232
  %mul242 = fmul float %sub, %mul240
  %mul243 = fmul float %sub111, %mul240
  %mul244 = fmul float %sub112, %mul240
  %add245 = fadd float %fix1.01012, %mul242
  %add246 = fadd float %fiy1.01011, %mul243
  %add247 = fadd float %fiz1.01010, %mul244
  %arrayidx249 = getelementptr inbounds float* %faction, i64 %idxprom85
  %34 = load float* %arrayidx249, align 4, !tbaa !3
  %sub250 = fsub float %34, %mul242
  %arrayidx253 = getelementptr inbounds float* %faction, i64 %idxprom88
  %35 = load float* %arrayidx253, align 4, !tbaa !3
  %sub254 = fsub float %35, %mul243
  %arrayidx257 = getelementptr inbounds float* %faction, i64 %idxprom91
  %36 = load float* %arrayidx257, align 4, !tbaa !3
  %sub258 = fsub float %36, %mul244
  %mul259 = fmul float %conv194, %conv194
  %mul260 = fmul float %add125, %krf
  %add261 = fadd float %mul260, %conv194
  %sub262 = fsub float %add261, %crf
  %mul263 = fmul float %mul6, %sub262
  %mul264 = fmul float %mul260, 2.000000e+00
  %sub265 = fsub float %conv194, %mul264
  %mul266 = fmul float %mul6, %sub265
  %mul267 = fmul float %mul259, %mul266
  %add268 = fadd float %add241, %mul263
  %mul269 = fmul float %sub118, %mul267
  %mul270 = fmul float %sub119, %mul267
  %mul271 = fmul float %sub120, %mul267
  %add272 = fadd float %mul269, %add245
  %add273 = fadd float %mul270, %add246
  %add274 = fadd float %mul271, %add247
  %arrayidx277 = getelementptr inbounds float* %faction, i64 %idxprom94
  %37 = load float* %arrayidx277, align 4, !tbaa !3
  %sub278 = fsub float %37, %mul269
  %arrayidx281 = getelementptr inbounds float* %faction, i64 %idxprom97
  %38 = load float* %arrayidx281, align 4, !tbaa !3
  %sub282 = fsub float %38, %mul270
  %arrayidx285 = getelementptr inbounds float* %faction, i64 %idxprom100
  %39 = load float* %arrayidx285, align 4, !tbaa !3
  %sub286 = fsub float %39, %mul271
  %mul287 = fmul float %conv206, %conv206
  %mul288 = fmul float %add133, %krf
  %add289 = fadd float %mul288, %conv206
  %sub290 = fsub float %add289, %crf
  %mul291 = fmul float %mul6, %sub290
  %mul292 = fmul float %mul288, 2.000000e+00
  %sub293 = fsub float %conv206, %mul292
  %mul294 = fmul float %mul6, %sub293
  %mul295 = fmul float %mul287, %mul294
  %add296 = fadd float %add268, %mul291
  %mul297 = fmul float %sub126, %mul295
  %mul298 = fmul float %sub127, %mul295
  %mul299 = fmul float %sub128, %mul295
  %add300 = fadd float %mul297, %add272
  %add301 = fadd float %mul298, %add273
  %add302 = fadd float %mul299, %add274
  %arrayidx305 = getelementptr inbounds float* %faction, i64 %idxprom103
  %40 = load float* %arrayidx305, align 4, !tbaa !3
  %sub306 = fsub float %40, %mul297
  %arrayidx309 = getelementptr inbounds float* %faction, i64 %idxprom106
  %41 = load float* %arrayidx309, align 4, !tbaa !3
  %sub310 = fsub float %41, %mul298
  %arrayidx313 = getelementptr inbounds float* %faction, i64 %idxprom109
  %42 = load float* %arrayidx313, align 4, !tbaa !3
  %sub314 = fsub float %42, %mul299
  %mul315 = fmul float %conv186, %conv186
  %mul316 = fmul float %add141, %krf
  %add317 = fadd float %mul316, %conv186
  %sub318 = fsub float %add317, %crf
  %mul319 = fmul float %mul6, %sub318
  %mul320 = fmul float %mul316, 2.000000e+00
  %sub321 = fsub float %conv186, %mul320
  %mul322 = fmul float %mul6, %sub321
  %mul323 = fmul float %mul315, %mul322
  %add324 = fadd float %mul319, %add296
  %mul325 = fmul float %sub134, %mul323
  %mul326 = fmul float %sub135, %mul323
  %mul327 = fmul float %sub136, %mul323
  %add328 = fadd float %fix2.01009, %mul325
  %add329 = fadd float %fiy2.01008, %mul326
  %add330 = fadd float %fiz2.01007, %mul327
  %sub331 = fsub float %sub250, %mul325
  %sub332 = fsub float %sub254, %mul326
  %sub333 = fsub float %sub258, %mul327
  %mul334 = fmul float %conv198, %conv198
  %mul335 = fmul float %add149, %krf
  %add336 = fadd float %mul335, %conv198
  %sub337 = fsub float %add336, %crf
  %mul338 = fmul float %mul8, %sub337
  %mul339 = fmul float %mul335, 2.000000e+00
  %sub340 = fsub float %conv198, %mul339
  %mul341 = fmul float %mul8, %sub340
  %mul342 = fmul float %mul334, %mul341
  %add343 = fadd float %mul338, %add324
  %mul344 = fmul float %sub142, %mul342
  %mul345 = fmul float %sub143, %mul342
  %mul346 = fmul float %sub144, %mul342
  %add347 = fadd float %add328, %mul344
  %add348 = fadd float %add329, %mul345
  %add349 = fadd float %add330, %mul346
  %sub350 = fsub float %sub278, %mul344
  %sub351 = fsub float %sub282, %mul345
  %sub352 = fsub float %sub286, %mul346
  %mul353 = fmul float %conv210, %conv210
  %mul354 = fmul float %add157, %krf
  %add355 = fadd float %mul354, %conv210
  %sub356 = fsub float %add355, %crf
  %mul357 = fmul float %mul8, %sub356
  %mul358 = fmul float %mul354, 2.000000e+00
  %sub359 = fsub float %conv210, %mul358
  %mul360 = fmul float %mul8, %sub359
  %mul361 = fmul float %mul353, %mul360
  %add362 = fadd float %mul357, %add343
  %mul363 = fmul float %sub150, %mul361
  %mul364 = fmul float %sub151, %mul361
  %mul365 = fmul float %sub152, %mul361
  %add366 = fadd float %add347, %mul363
  %add367 = fadd float %add348, %mul364
  %add368 = fadd float %add349, %mul365
  %sub369 = fsub float %sub306, %mul363
  %sub370 = fsub float %sub310, %mul364
  %sub371 = fsub float %sub314, %mul365
  %mul372 = fmul float %conv190, %conv190
  %mul373 = fmul float %add165, %krf
  %add374 = fadd float %mul373, %conv190
  %sub375 = fsub float %add374, %crf
  %mul376 = fmul float %mul6, %sub375
  %mul377 = fmul float %mul373, 2.000000e+00
  %sub378 = fsub float %conv190, %mul377
  %mul379 = fmul float %mul6, %sub378
  %mul380 = fmul float %mul372, %mul379
  %add381 = fadd float %mul376, %add362
  %mul382 = fmul float %sub158, %mul380
  %mul383 = fmul float %sub159, %mul380
  %mul384 = fmul float %sub160, %mul380
  %add385 = fadd float %fix3.01006, %mul382
  %add386 = fadd float %fiy3.01005, %mul383
  %add387 = fadd float %fiz3.01004, %mul384
  %sub388 = fsub float %sub331, %mul382
  store float %sub388, float* %arrayidx249, align 4, !tbaa !3
  %sub391 = fsub float %sub332, %mul383
  store float %sub391, float* %arrayidx253, align 4, !tbaa !3
  %sub395 = fsub float %sub333, %mul384
  store float %sub395, float* %arrayidx257, align 4, !tbaa !3
  %mul399 = fmul float %conv202, %conv202
  %mul400 = fmul float %add173, %krf
  %add401 = fadd float %mul400, %conv202
  %sub402 = fsub float %add401, %crf
  %mul403 = fmul float %mul8, %sub402
  %mul404 = fmul float %mul400, 2.000000e+00
  %sub405 = fsub float %conv202, %mul404
  %mul406 = fmul float %mul8, %sub405
  %mul407 = fmul float %mul399, %mul406
  %add408 = fadd float %mul403, %add381
  %mul409 = fmul float %sub166, %mul407
  %mul410 = fmul float %sub167, %mul407
  %mul411 = fmul float %sub168, %mul407
  %add412 = fadd float %add385, %mul409
  %add413 = fadd float %add386, %mul410
  %add414 = fadd float %add387, %mul411
  %sub415 = fsub float %sub350, %mul409
  store float %sub415, float* %arrayidx277, align 4, !tbaa !3
  %sub419 = fsub float %sub351, %mul410
  store float %sub419, float* %arrayidx281, align 4, !tbaa !3
  %sub423 = fsub float %sub352, %mul411
  store float %sub423, float* %arrayidx285, align 4, !tbaa !3
  %mul427 = fmul float %conv214, %conv214
  %mul428 = fmul float %add181, %krf
  %add429 = fadd float %mul428, %conv214
  %sub430 = fsub float %add429, %crf
  %mul431 = fmul float %mul8, %sub430
  %mul432 = fmul float %mul428, 2.000000e+00
  %sub433 = fsub float %conv214, %mul432
  %mul434 = fmul float %mul8, %sub433
  %mul435 = fmul float %mul427, %mul434
  %add436 = fadd float %mul431, %add408
  %mul437 = fmul float %sub174, %mul435
  %mul438 = fmul float %sub175, %mul435
  %mul439 = fmul float %sub176, %mul435
  %add440 = fadd float %add412, %mul437
  %add441 = fadd float %add413, %mul438
  %add442 = fadd float %add414, %mul439
  %sub443 = fsub float %sub369, %mul437
  store float %sub443, float* %arrayidx305, align 4, !tbaa !3
  %sub447 = fsub float %sub370, %mul438
  store float %sub447, float* %arrayidx309, align 4, !tbaa !3
  %sub451 = fsub float %sub371, %mul439
  store float %sub451, float* %arrayidx313, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %43 = trunc i64 %indvars.iv.next to i32
  %cmp80 = icmp slt i32 %43, %13
  br i1 %cmp80, label %for.body81, label %for.end

for.end:                                          ; preds = %for.body81, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add436, %for.body81 ]
  %vnbtot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %sub228, %for.body81 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add300, %for.body81 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add301, %for.body81 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add302, %for.body81 ]
  %fix2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add366, %for.body81 ]
  %fiy2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add367, %for.body81 ]
  %fiz2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add368, %for.body81 ]
  %fix3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add440, %for.body81 ]
  %fiy3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add441, %for.body81 ]
  %fiz3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add442, %for.body81 ]
  %arrayidx456 = getelementptr inbounds float* %faction, i64 %idxprom44
  %44 = load float* %arrayidx456, align 4, !tbaa !3
  %add457 = fadd float %fix1.0.lcssa, %44
  store float %add457, float* %arrayidx456, align 4, !tbaa !3
  %arrayidx462 = getelementptr inbounds float* %faction, i64 %idxprom48
  %45 = load float* %arrayidx462, align 4, !tbaa !3
  %add463 = fadd float %fiy1.0.lcssa, %45
  store float %add463, float* %arrayidx462, align 4, !tbaa !3
  %arrayidx469 = getelementptr inbounds float* %faction, i64 %idxprom52
  %46 = load float* %arrayidx469, align 4, !tbaa !3
  %add470 = fadd float %fiz1.0.lcssa, %46
  store float %add470, float* %arrayidx469, align 4, !tbaa !3
  %arrayidx476 = getelementptr inbounds float* %faction, i64 %idxprom56
  %47 = load float* %arrayidx476, align 4, !tbaa !3
  %add477 = fadd float %fix2.0.lcssa, %47
  store float %add477, float* %arrayidx476, align 4, !tbaa !3
  %arrayidx483 = getelementptr inbounds float* %faction, i64 %idxprom60
  %48 = load float* %arrayidx483, align 4, !tbaa !3
  %add484 = fadd float %fiy2.0.lcssa, %48
  store float %add484, float* %arrayidx483, align 4, !tbaa !3
  %arrayidx490 = getelementptr inbounds float* %faction, i64 %idxprom64
  %49 = load float* %arrayidx490, align 4, !tbaa !3
  %add491 = fadd float %fiz2.0.lcssa, %49
  store float %add491, float* %arrayidx490, align 4, !tbaa !3
  %arrayidx497 = getelementptr inbounds float* %faction, i64 %idxprom68
  %50 = load float* %arrayidx497, align 4, !tbaa !3
  %add498 = fadd float %fix3.0.lcssa, %50
  store float %add498, float* %arrayidx497, align 4, !tbaa !3
  %arrayidx504 = getelementptr inbounds float* %faction, i64 %idxprom72
  %51 = load float* %arrayidx504, align 4, !tbaa !3
  %add505 = fadd float %fiy3.0.lcssa, %51
  store float %add505, float* %arrayidx504, align 4, !tbaa !3
  %arrayidx511 = getelementptr inbounds float* %faction, i64 %idxprom76
  %52 = load float* %arrayidx511, align 4, !tbaa !3
  %add512 = fadd float %fiz3.0.lcssa, %52
  store float %add512, float* %arrayidx511, align 4, !tbaa !3
  %arrayidx517 = getelementptr inbounds float* %fshift, i64 %idxprom28
  %53 = load float* %arrayidx517, align 4, !tbaa !3
  %add518 = fadd float %fix1.0.lcssa, %53
  %add519 = fadd float %fix2.0.lcssa, %add518
  %add520 = fadd float %fix3.0.lcssa, %add519
  store float %add520, float* %arrayidx517, align 4, !tbaa !3
  %arrayidx525 = getelementptr inbounds float* %fshift, i64 %idxprom31
  %54 = load float* %arrayidx525, align 4, !tbaa !3
  %add526 = fadd float %fiy1.0.lcssa, %54
  %add527 = fadd float %fiy2.0.lcssa, %add526
  %add528 = fadd float %fiy3.0.lcssa, %add527
  store float %add528, float* %arrayidx525, align 4, !tbaa !3
  %arrayidx534 = getelementptr inbounds float* %fshift, i64 %idxprom34
  %55 = load float* %arrayidx534, align 4, !tbaa !3
  %add535 = fadd float %fiz1.0.lcssa, %55
  %add536 = fadd float %fiz2.0.lcssa, %add535
  %add537 = fadd float %fiz3.0.lcssa, %add536
  store float %add537, float* %arrayidx534, align 4, !tbaa !3
  %arrayidx542 = getelementptr inbounds i32* %gid, i64 %indvars.iv1028
  %56 = load i32* %arrayidx542, align 4, !tbaa !0
  %idxprom543 = sext i32 %56 to i64
  %arrayidx544 = getelementptr inbounds float* %Vc, i64 %idxprom543
  %57 = load float* %arrayidx544, align 4, !tbaa !3
  %add545 = fadd float %vctot.0.lcssa, %57
  store float %add545, float* %arrayidx544, align 4, !tbaa !3
  %arrayidx549 = getelementptr inbounds float* %Vnb, i64 %idxprom543
  %58 = load float* %arrayidx549, align 4, !tbaa !3
  %add550 = fadd float %vnbtot.0.lcssa, %58
  store float %add550, float* %arrayidx549, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next1029 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end555, label %for.end.for.body_crit_edge

for.end.for.body_crit_edge:                       ; preds = %for.end
  %arrayidx37.phi.trans.insert = getelementptr inbounds i32* %iinr, i64 %indvars.iv.next1029
  %.pre = load i32* %arrayidx37.phi.trans.insert, align 4, !tbaa !0
  br label %for.body

for.end555:                                       ; preds = %for.end, %entry
  ret void
}
