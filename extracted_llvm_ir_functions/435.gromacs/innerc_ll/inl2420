define void @inl2420(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc, float %krf, float %crf, i32* nocapture %type, i32 %ntype, float* nocapture %nbfp, float* nocapture %Vnb, float %tabscale, float* nocapture %VFtab, float %exptabscale) #0 {
entry:
  %0 = load i32* %iinr, align 4, !tbaa !0
  %idxprom = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds float* %charge, i64 %idxprom
  %1 = load float* %arrayidx1, align 4, !tbaa !3
  %add = add nsw i32 %0, 1
  %idxprom2 = sext i32 %add to i64
  %arrayidx3 = getelementptr inbounds float* %charge, i64 %idxprom2
  %2 = load float* %arrayidx3, align 4, !tbaa !3
  %mul = fmul float %1, %facel
  %mul4 = fmul float %2, %facel
  %arrayidx7 = getelementptr inbounds i32* %type, i64 %idxprom
  %3 = load i32* %arrayidx7, align 4, !tbaa !0
  %mul8 = mul i32 %3, %ntype
  %cmp670 = icmp sgt i32 %nri, 0
  br i1 %cmp670, label %for.body, label %for.end370

for.body:                                         ; preds = %entry, %for.end.for.body_crit_edge
  %4 = phi i32 [ %.pre, %for.end.for.body_crit_edge ], [ %0, %entry ]
  %indvars.iv672 = phi i64 [ %indvars.iv.next673, %for.end.for.body_crit_edge ], [ 0, %entry ]
  %arrayidx10 = getelementptr inbounds i32* %shift, i64 %indvars.iv672
  %5 = load i32* %arrayidx10, align 4, !tbaa !0
  %mul11 = mul nsw i32 %5, 3
  %idxprom12 = sext i32 %mul11 to i64
  %arrayidx13 = getelementptr inbounds float* %shiftvec, i64 %idxprom12
  %6 = load float* %arrayidx13, align 4, !tbaa !3
  %add14 = add nsw i32 %mul11, 1
  %idxprom15 = sext i32 %add14 to i64
  %arrayidx16 = getelementptr inbounds float* %shiftvec, i64 %idxprom15
  %7 = load float* %arrayidx16, align 4, !tbaa !3
  %add17 = add nsw i32 %mul11, 2
  %idxprom18 = sext i32 %add17 to i64
  %arrayidx19 = getelementptr inbounds float* %shiftvec, i64 %idxprom18
  %8 = load float* %arrayidx19, align 4, !tbaa !3
  %mul22 = mul nsw i32 %4, 3
  %arrayidx24 = getelementptr inbounds i32* %jindex, i64 %indvars.iv672
  %9 = load i32* %arrayidx24, align 4, !tbaa !0
  %indvars.iv.next673 = add i64 %indvars.iv672, 1
  %arrayidx27 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next673
  %10 = load i32* %arrayidx27, align 4, !tbaa !0
  %idxprom28 = sext i32 %mul22 to i64
  %arrayidx29 = getelementptr inbounds float* %pos, i64 %idxprom28
  %11 = load float* %arrayidx29, align 4, !tbaa !3
  %add30 = fadd float %6, %11
  %add31 = add nsw i32 %mul22, 1
  %idxprom32 = sext i32 %add31 to i64
  %arrayidx33 = getelementptr inbounds float* %pos, i64 %idxprom32
  %12 = load float* %arrayidx33, align 4, !tbaa !3
  %add34 = fadd float %7, %12
  %add35 = add nsw i32 %mul22, 2
  %idxprom36 = sext i32 %add35 to i64
  %arrayidx37 = getelementptr inbounds float* %pos, i64 %idxprom36
  %13 = load float* %arrayidx37, align 4, !tbaa !3
  %add38 = fadd float %8, %13
  %add39 = add nsw i32 %mul22, 3
  %idxprom40 = sext i32 %add39 to i64
  %arrayidx41 = getelementptr inbounds float* %pos, i64 %idxprom40
  %14 = load float* %arrayidx41, align 4, !tbaa !3
  %add42 = fadd float %6, %14
  %add43 = add nsw i32 %mul22, 4
  %idxprom44 = sext i32 %add43 to i64
  %arrayidx45 = getelementptr inbounds float* %pos, i64 %idxprom44
  %15 = load float* %arrayidx45, align 4, !tbaa !3
  %add46 = fadd float %7, %15
  %add47 = add nsw i32 %mul22, 5
  %idxprom48 = sext i32 %add47 to i64
  %arrayidx49 = getelementptr inbounds float* %pos, i64 %idxprom48
  %16 = load float* %arrayidx49, align 4, !tbaa !3
  %add50 = fadd float %8, %16
  %add51 = add nsw i32 %mul22, 6
  %idxprom52 = sext i32 %add51 to i64
  %arrayidx53 = getelementptr inbounds float* %pos, i64 %idxprom52
  %17 = load float* %arrayidx53, align 4, !tbaa !3
  %add54 = fadd float %6, %17
  %add55 = add nsw i32 %mul22, 7
  %idxprom56 = sext i32 %add55 to i64
  %arrayidx57 = getelementptr inbounds float* %pos, i64 %idxprom56
  %18 = load float* %arrayidx57, align 4, !tbaa !3
  %add58 = fadd float %7, %18
  %add59 = add nsw i32 %mul22, 8
  %idxprom60 = sext i32 %add59 to i64
  %arrayidx61 = getelementptr inbounds float* %pos, i64 %idxprom60
  %19 = load float* %arrayidx61, align 4, !tbaa !3
  %add62 = fadd float %8, %19
  %cmp64647 = icmp slt i32 %9, %10
  br i1 %cmp64647, label %for.body65.lr.ph, label %for.end

for.body65.lr.ph:                                 ; preds = %for.body
  %20 = sext i32 %9 to i64
  br label %for.body65

for.body65:                                       ; preds = %for.body65.lr.ph, %for.body65
  %indvars.iv = phi i64 [ %20, %for.body65.lr.ph ], [ %indvars.iv.next, %for.body65 ]
  %vctot.0658 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add252, %for.body65 ]
  %vnbtot.0657 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add183, %for.body65 ]
  %fix1.0656 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add204, %for.body65 ]
  %fiy1.0655 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add205, %for.body65 ]
  %fiz1.0654 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add206, %for.body65 ]
  %fix2.0653 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add234, %for.body65 ]
  %fiy2.0652 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add235, %for.body65 ]
  %fiz2.0651 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add236, %for.body65 ]
  %fix3.0650 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add256, %for.body65 ]
  %fiy3.0649 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add257, %for.body65 ]
  %fiz3.0648 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add258, %for.body65 ]
  %arrayidx67 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %21 = load i32* %arrayidx67, align 4, !tbaa !0
  %mul68 = mul nsw i32 %21, 3
  %idxprom69 = sext i32 %mul68 to i64
  %arrayidx70 = getelementptr inbounds float* %pos, i64 %idxprom69
  %22 = load float* %arrayidx70, align 4, !tbaa !3
  %add71 = add nsw i32 %mul68, 1
  %idxprom72 = sext i32 %add71 to i64
  %arrayidx73 = getelementptr inbounds float* %pos, i64 %idxprom72
  %23 = load float* %arrayidx73, align 4, !tbaa !3
  %add74 = add nsw i32 %mul68, 2
  %idxprom75 = sext i32 %add74 to i64
  %arrayidx76 = getelementptr inbounds float* %pos, i64 %idxprom75
  %24 = load float* %arrayidx76, align 4, !tbaa !3
  %sub = fsub float %add30, %22
  %sub77 = fsub float %add34, %23
  %sub78 = fsub float %add38, %24
  %mul79 = fmul float %sub, %sub
  %mul80 = fmul float %sub77, %sub77
  %add81 = fadd float %mul79, %mul80
  %mul82 = fmul float %sub78, %sub78
  %add83 = fadd float %add81, %mul82
  %sub84 = fsub float %add42, %22
  %sub85 = fsub float %add46, %23
  %sub86 = fsub float %add50, %24
  %mul87 = fmul float %sub84, %sub84
  %mul88 = fmul float %sub85, %sub85
  %add89 = fadd float %mul87, %mul88
  %mul90 = fmul float %sub86, %sub86
  %add91 = fadd float %add89, %mul90
  %sub92 = fsub float %add54, %22
  %sub93 = fsub float %add58, %23
  %sub94 = fsub float %add62, %24
  %mul95 = fmul float %sub92, %sub92
  %mul96 = fmul float %sub93, %sub93
  %add97 = fadd float %mul95, %mul96
  %mul98 = fmul float %sub94, %sub94
  %add99 = fadd float %add97, %mul98
  %conv = fpext float %add83 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv100 = fptrunc double %div to float
  %conv101 = fpext float %add91 to double
  %call102 = tail call double @sqrt(double %conv101) #2
  %div103 = fdiv double 1.000000e+00, %call102
  %conv104 = fptrunc double %div103 to float
  %conv105 = fpext float %add99 to double
  %call106 = tail call double @sqrt(double %conv105) #2
  %div107 = fdiv double 1.000000e+00, %call106
  %conv108 = fptrunc double %div107 to float
  %mul109 = fmul float %add83, %conv100
  %mul111 = fmul float %mul109, %tabscale
  %conv112 = fptosi float %mul111 to i32
  %conv113 = sitofp i32 %conv112 to float
  %sub114 = fsub float %mul111, %conv113
  %mul115 = fmul float %sub114, %sub114
  %mul116 = shl nsw i32 %conv112, 3
  %idxprom117 = sext i32 %21 to i64
  %arrayidx118 = getelementptr inbounds i32* %type, i64 %idxprom117
  %25 = load i32* %arrayidx118, align 4, !tbaa !0
  %tmp = add i32 %25, %mul8
  %tmp646 = mul i32 %tmp, 3
  %idxprom121 = sext i32 %tmp646 to i64
  %arrayidx122 = getelementptr inbounds float* %nbfp, i64 %idxprom121
  %26 = load float* %arrayidx122, align 4, !tbaa !3
  %add123 = add nsw i32 %tmp646, 1
  %idxprom124 = sext i32 %add123 to i64
  %arrayidx125 = getelementptr inbounds float* %nbfp, i64 %idxprom124
  %27 = load float* %arrayidx125, align 4, !tbaa !3
  %add126 = add nsw i32 %tmp646, 2
  %idxprom127 = sext i32 %add126 to i64
  %arrayidx128 = getelementptr inbounds float* %nbfp, i64 %idxprom127
  %28 = load float* %arrayidx128, align 4, !tbaa !3
  %idxprom129 = sext i32 %mul116 to i64
  %arrayidx130 = getelementptr inbounds float* %VFtab, i64 %idxprom129
  %29 = load float* %arrayidx130, align 4, !tbaa !3
  %add131639 = or i32 %mul116, 1
  %idxprom132 = sext i32 %add131639 to i64
  %arrayidx133 = getelementptr inbounds float* %VFtab, i64 %idxprom132
  %30 = load float* %arrayidx133, align 4, !tbaa !3
  %add134640 = or i32 %mul116, 2
  %idxprom135 = sext i32 %add134640 to i64
  %arrayidx136 = getelementptr inbounds float* %VFtab, i64 %idxprom135
  %31 = load float* %arrayidx136, align 4, !tbaa !3
  %mul137 = fmul float %sub114, %31
  %add138641 = or i32 %mul116, 3
  %idxprom139 = sext i32 %add138641 to i64
  %arrayidx140 = getelementptr inbounds float* %VFtab, i64 %idxprom139
  %32 = load float* %arrayidx140, align 4, !tbaa !3
  %mul141 = fmul float %mul115, %32
  %add142 = fadd float %30, %mul137
  %add143 = fadd float %add142, %mul141
  %mul144 = fmul float %sub114, %add143
  %add145 = fadd float %29, %mul144
  %add146 = fadd float %mul137, %add143
  %mul147 = fmul float %mul141, 2.000000e+00
  %add148 = fadd float %mul147, %add146
  %mul149 = fmul float %26, %add145
  %mul150 = fmul float %26, %add148
  %mul151 = fmul float %mul109, %28
  %mul152 = fmul float %mul151, %exptabscale
  %conv153 = fptosi float %mul152 to i32
  %conv154 = sitofp i32 %conv153 to float
  %sub155 = fsub float %mul152, %conv154
  %mul156 = fmul float %sub155, %sub155
  %mul157 = shl nsw i32 %conv153, 3
  %add158642 = or i32 %mul157, 4
  %idxprom159 = sext i32 %add158642 to i64
  %arrayidx160 = getelementptr inbounds float* %VFtab, i64 %idxprom159
  %33 = load float* %arrayidx160, align 4, !tbaa !3
  %add161643 = or i32 %mul157, 5
  %idxprom162 = sext i32 %add161643 to i64
  %arrayidx163 = getelementptr inbounds float* %VFtab, i64 %idxprom162
  %34 = load float* %arrayidx163, align 4, !tbaa !3
  %add164644 = or i32 %mul157, 6
  %idxprom165 = sext i32 %add164644 to i64
  %arrayidx166 = getelementptr inbounds float* %VFtab, i64 %idxprom165
  %35 = load float* %arrayidx166, align 4, !tbaa !3
  %mul167 = fmul float %sub155, %35
  %add168645 = or i32 %mul157, 7
  %idxprom169 = sext i32 %add168645 to i64
  %arrayidx170 = getelementptr inbounds float* %VFtab, i64 %idxprom169
  %36 = load float* %arrayidx170, align 4, !tbaa !3
  %mul171 = fmul float %mul156, %36
  %add172 = fadd float %34, %mul167
  %add173 = fadd float %add172, %mul171
  %mul174 = fmul float %sub155, %add173
  %add175 = fadd float %33, %mul174
  %add176 = fadd float %mul167, %add173
  %mul177 = fmul float %mul171, 2.000000e+00
  %add178 = fadd float %mul177, %add176
  %mul179 = fmul float %27, %add175
  %mul180 = fmul float %27, %28
  %mul181 = fmul float %mul180, %add178
  %add182 = fadd float %vnbtot.0657, %mul149
  %add183 = fadd float %add182, %mul179
  %arrayidx185 = getelementptr inbounds float* %charge, i64 %idxprom117
  %37 = load float* %arrayidx185, align 4, !tbaa !3
  %mul186 = fmul float %mul, %37
  %mul187 = fmul float %add83, %krf
  %add188 = fadd float %conv100, %mul187
  %sub189 = fsub float %add188, %crf
  %mul190 = fmul float %sub189, %mul186
  %mul191 = fmul float %mul187, 2.000000e+00
  %sub192 = fsub float %conv100, %mul191
  %mul193 = fmul float %sub192, %mul186
  %mul194 = fmul float %conv100, %mul193
  %mul195 = fmul float %mul150, %tabscale
  %mul196 = fmul float %mul181, %exptabscale
  %add197 = fadd float %mul195, %mul196
  %sub198 = fsub float %mul194, %add197
  %mul199 = fmul float %conv100, %sub198
  %add200 = fadd float %vctot.0658, %mul190
  %mul201 = fmul float %sub, %mul199
  %mul202 = fmul float %sub77, %mul199
  %mul203 = fmul float %sub78, %mul199
  %add204 = fadd float %fix1.0656, %mul201
  %add205 = fadd float %fiy1.0655, %mul202
  %add206 = fadd float %fiz1.0654, %mul203
  %arrayidx208 = getelementptr inbounds float* %faction, i64 %idxprom69
  %38 = load float* %arrayidx208, align 4, !tbaa !3
  %sub209 = fsub float %38, %mul201
  %arrayidx212 = getelementptr inbounds float* %faction, i64 %idxprom72
  %39 = load float* %arrayidx212, align 4, !tbaa !3
  %sub213 = fsub float %39, %mul202
  %arrayidx216 = getelementptr inbounds float* %faction, i64 %idxprom75
  %40 = load float* %arrayidx216, align 4, !tbaa !3
  %sub217 = fsub float %40, %mul203
  %mul218 = fmul float %conv104, %conv104
  %mul221 = fmul float %mul4, %37
  %mul222 = fmul float %add91, %krf
  %add223 = fadd float %mul222, %conv104
  %sub224 = fsub float %add223, %crf
  %mul225 = fmul float %sub224, %mul221
  %mul226 = fmul float %mul222, 2.000000e+00
  %sub227 = fsub float %conv104, %mul226
  %mul228 = fmul float %sub227, %mul221
  %mul229 = fmul float %mul218, %mul228
  %add230 = fadd float %mul225, %add200
  %mul231 = fmul float %sub84, %mul229
  %mul232 = fmul float %sub85, %mul229
  %mul233 = fmul float %sub86, %mul229
  %add234 = fadd float %fix2.0653, %mul231
  %add235 = fadd float %fiy2.0652, %mul232
  %add236 = fadd float %fiz2.0651, %mul233
  %sub237 = fsub float %sub209, %mul231
  %sub238 = fsub float %sub213, %mul232
  %sub239 = fsub float %sub217, %mul233
  %mul240 = fmul float %conv108, %conv108
  %mul244 = fmul float %add99, %krf
  %add245 = fadd float %mul244, %conv108
  %sub246 = fsub float %add245, %crf
  %mul247 = fmul float %sub246, %mul221
  %mul248 = fmul float %mul244, 2.000000e+00
  %sub249 = fsub float %conv108, %mul248
  %mul250 = fmul float %sub249, %mul221
  %mul251 = fmul float %mul240, %mul250
  %add252 = fadd float %mul247, %add230
  %mul253 = fmul float %sub92, %mul251
  %mul254 = fmul float %sub93, %mul251
  %mul255 = fmul float %sub94, %mul251
  %add256 = fadd float %fix3.0650, %mul253
  %add257 = fadd float %fiy3.0649, %mul254
  %add258 = fadd float %fiz3.0648, %mul255
  %sub259 = fsub float %sub237, %mul253
  store float %sub259, float* %arrayidx208, align 4, !tbaa !3
  %sub262 = fsub float %sub238, %mul254
  store float %sub262, float* %arrayidx212, align 4, !tbaa !3
  %sub266 = fsub float %sub239, %mul255
  store float %sub266, float* %arrayidx216, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %41 = trunc i64 %indvars.iv.next to i32
  %cmp64 = icmp slt i32 %41, %10
  br i1 %cmp64, label %for.body65, label %for.end

for.end:                                          ; preds = %for.body65, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add252, %for.body65 ]
  %vnbtot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add183, %for.body65 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add204, %for.body65 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add205, %for.body65 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add206, %for.body65 ]
  %fix2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add234, %for.body65 ]
  %fiy2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add235, %for.body65 ]
  %fiz2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add236, %for.body65 ]
  %fix3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add256, %for.body65 ]
  %fiy3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add257, %for.body65 ]
  %fiz3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add258, %for.body65 ]
  %arrayidx271 = getelementptr inbounds float* %faction, i64 %idxprom28
  %42 = load float* %arrayidx271, align 4, !tbaa !3
  %add272 = fadd float %fix1.0.lcssa, %42
  store float %add272, float* %arrayidx271, align 4, !tbaa !3
  %arrayidx277 = getelementptr inbounds float* %faction, i64 %idxprom32
  %43 = load float* %arrayidx277, align 4, !tbaa !3
  %add278 = fadd float %fiy1.0.lcssa, %43
  store float %add278, float* %arrayidx277, align 4, !tbaa !3
  %arrayidx284 = getelementptr inbounds float* %faction, i64 %idxprom36
  %44 = load float* %arrayidx284, align 4, !tbaa !3
  %add285 = fadd float %fiz1.0.lcssa, %44
  store float %add285, float* %arrayidx284, align 4, !tbaa !3
  %arrayidx291 = getelementptr inbounds float* %faction, i64 %idxprom40
  %45 = load float* %arrayidx291, align 4, !tbaa !3
  %add292 = fadd float %fix2.0.lcssa, %45
  store float %add292, float* %arrayidx291, align 4, !tbaa !3
  %arrayidx298 = getelementptr inbounds float* %faction, i64 %idxprom44
  %46 = load float* %arrayidx298, align 4, !tbaa !3
  %add299 = fadd float %fiy2.0.lcssa, %46
  store float %add299, float* %arrayidx298, align 4, !tbaa !3
  %arrayidx305 = getelementptr inbounds float* %faction, i64 %idxprom48
  %47 = load float* %arrayidx305, align 4, !tbaa !3
  %add306 = fadd float %fiz2.0.lcssa, %47
  store float %add306, float* %arrayidx305, align 4, !tbaa !3
  %arrayidx312 = getelementptr inbounds float* %faction, i64 %idxprom52
  %48 = load float* %arrayidx312, align 4, !tbaa !3
  %add313 = fadd float %fix3.0.lcssa, %48
  store float %add313, float* %arrayidx312, align 4, !tbaa !3
  %arrayidx319 = getelementptr inbounds float* %faction, i64 %idxprom56
  %49 = load float* %arrayidx319, align 4, !tbaa !3
  %add320 = fadd float %fiy3.0.lcssa, %49
  store float %add320, float* %arrayidx319, align 4, !tbaa !3
  %arrayidx326 = getelementptr inbounds float* %faction, i64 %idxprom60
  %50 = load float* %arrayidx326, align 4, !tbaa !3
  %add327 = fadd float %fiz3.0.lcssa, %50
  store float %add327, float* %arrayidx326, align 4, !tbaa !3
  %arrayidx332 = getelementptr inbounds float* %fshift, i64 %idxprom12
  %51 = load float* %arrayidx332, align 4, !tbaa !3
  %add333 = fadd float %fix1.0.lcssa, %51
  %add334 = fadd float %fix2.0.lcssa, %add333
  %add335 = fadd float %fix3.0.lcssa, %add334
  store float %add335, float* %arrayidx332, align 4, !tbaa !3
  %arrayidx340 = getelementptr inbounds float* %fshift, i64 %idxprom15
  %52 = load float* %arrayidx340, align 4, !tbaa !3
  %add341 = fadd float %fiy1.0.lcssa, %52
  %add342 = fadd float %fiy2.0.lcssa, %add341
  %add343 = fadd float %fiy3.0.lcssa, %add342
  store float %add343, float* %arrayidx340, align 4, !tbaa !3
  %arrayidx349 = getelementptr inbounds float* %fshift, i64 %idxprom18
  %53 = load float* %arrayidx349, align 4, !tbaa !3
  %add350 = fadd float %fiz1.0.lcssa, %53
  %add351 = fadd float %fiz2.0.lcssa, %add350
  %add352 = fadd float %fiz3.0.lcssa, %add351
  store float %add352, float* %arrayidx349, align 4, !tbaa !3
  %arrayidx357 = getelementptr inbounds i32* %gid, i64 %indvars.iv672
  %54 = load i32* %arrayidx357, align 4, !tbaa !0
  %idxprom358 = sext i32 %54 to i64
  %arrayidx359 = getelementptr inbounds float* %Vc, i64 %idxprom358
  %55 = load float* %arrayidx359, align 4, !tbaa !3
  %add360 = fadd float %vctot.0.lcssa, %55
  store float %add360, float* %arrayidx359, align 4, !tbaa !3
  %arrayidx364 = getelementptr inbounds float* %Vnb, i64 %idxprom358
  %56 = load float* %arrayidx364, align 4, !tbaa !3
  %add365 = fadd float %vnbtot.0.lcssa, %56
  store float %add365, float* %arrayidx364, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next673 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end370, label %for.end.for.body_crit_edge

for.end.for.body_crit_edge:                       ; preds = %for.end
  %arrayidx21.phi.trans.insert = getelementptr inbounds i32* %iinr, i64 %indvars.iv.next673
  %.pre = load i32* %arrayidx21.phi.trans.insert, align 4, !tbaa !0
  br label %for.body

for.end370:                                       ; preds = %for.end, %entry
  ret void
}
