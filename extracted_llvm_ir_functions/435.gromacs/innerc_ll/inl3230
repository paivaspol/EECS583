define void @inl3230(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc, i32* nocapture %type, i32 %ntype, float* nocapture %nbfp, float* nocapture %Vnb, float %tabscale, float* %VFtab) #0 {
entry:
  %0 = load i32* %iinr, align 4, !tbaa !0
  %idxprom = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds float* %charge, i64 %idxprom
  %1 = load float* %arrayidx1, align 4, !tbaa !3
  %add = add nsw i32 %0, 1
  %idxprom2 = sext i32 %add to i64
  %arrayidx3 = getelementptr inbounds float* %charge, i64 %idxprom2
  %2 = load float* %arrayidx3, align 4, !tbaa !3
  %mul = fmul float %1, %facel
  %mul4 = fmul float %1, %mul
  %mul6 = fmul float %mul, %2
  %mul7 = fmul float %2, %facel
  %mul8 = fmul float %2, %mul7
  %mul9 = mul nsw i32 %ntype, 3
  %arrayidx11 = getelementptr inbounds i32* %type, i64 %idxprom
  %3 = load i32* %arrayidx11, align 4, !tbaa !0
  %mul121422 = add i32 %mul9, 3
  %add16 = mul i32 %3, %mul121422
  %idxprom17 = sext i32 %add16 to i64
  %arrayidx18 = getelementptr inbounds float* %nbfp, i64 %idxprom17
  %4 = load float* %arrayidx18, align 4, !tbaa !3
  %add22 = add nsw i32 %add16, 2
  %idxprom23 = sext i32 %add22 to i64
  %arrayidx24 = getelementptr inbounds float* %nbfp, i64 %idxprom23
  %5 = load float* %arrayidx24, align 4, !tbaa !3
  %cmp1473 = icmp sgt i32 %nri, 0
  br i1 %cmp1473, label %for.body.lr.ph, label %for.end762

for.body.lr.ph:                                   ; preds = %entry
  %add19 = add nsw i32 %add16, 1
  %idxprom20 = sext i32 %add19 to i64
  %arrayidx21 = getelementptr inbounds float* %nbfp, i64 %idxprom20
  %6 = load float* %arrayidx21, align 4, !tbaa !3
  %conv221 = fpext float %6 to double
  br label %for.body

for.body:                                         ; preds = %for.end.for.body_crit_edge, %for.body.lr.ph
  %7 = phi i32 [ %0, %for.body.lr.ph ], [ %.pre, %for.end.for.body_crit_edge ]
  %indvars.iv1475 = phi i64 [ 0, %for.body.lr.ph ], [ %indvars.iv.next1476, %for.end.for.body_crit_edge ]
  %arrayidx26 = getelementptr inbounds i32* %shift, i64 %indvars.iv1475
  %8 = load i32* %arrayidx26, align 4, !tbaa !0
  %mul27 = mul nsw i32 %8, 3
  %idxprom28 = sext i32 %mul27 to i64
  %arrayidx29 = getelementptr inbounds float* %shiftvec, i64 %idxprom28
  %9 = load float* %arrayidx29, align 4, !tbaa !3
  %add30 = add nsw i32 %mul27, 1
  %idxprom31 = sext i32 %add30 to i64
  %arrayidx32 = getelementptr inbounds float* %shiftvec, i64 %idxprom31
  %10 = load float* %arrayidx32, align 4, !tbaa !3
  %add33 = add nsw i32 %mul27, 2
  %idxprom34 = sext i32 %add33 to i64
  %arrayidx35 = getelementptr inbounds float* %shiftvec, i64 %idxprom34
  %11 = load float* %arrayidx35, align 4, !tbaa !3
  %mul38 = mul nsw i32 %7, 3
  %arrayidx40 = getelementptr inbounds i32* %jindex, i64 %indvars.iv1475
  %12 = load i32* %arrayidx40, align 4, !tbaa !0
  %indvars.iv.next1476 = add i64 %indvars.iv1475, 1
  %arrayidx43 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next1476
  %13 = load i32* %arrayidx43, align 4, !tbaa !0
  %idxprom44 = sext i32 %mul38 to i64
  %arrayidx45 = getelementptr inbounds float* %pos, i64 %idxprom44
  %14 = load float* %arrayidx45, align 4, !tbaa !3
  %add46 = fadd float %9, %14
  %add47 = add nsw i32 %mul38, 1
  %idxprom48 = sext i32 %add47 to i64
  %arrayidx49 = getelementptr inbounds float* %pos, i64 %idxprom48
  %15 = load float* %arrayidx49, align 4, !tbaa !3
  %add50 = fadd float %10, %15
  %add51 = add nsw i32 %mul38, 2
  %idxprom52 = sext i32 %add51 to i64
  %arrayidx53 = getelementptr inbounds float* %pos, i64 %idxprom52
  %16 = load float* %arrayidx53, align 4, !tbaa !3
  %add54 = fadd float %11, %16
  %add55 = add nsw i32 %mul38, 3
  %idxprom56 = sext i32 %add55 to i64
  %arrayidx57 = getelementptr inbounds float* %pos, i64 %idxprom56
  %17 = load float* %arrayidx57, align 4, !tbaa !3
  %add58 = fadd float %9, %17
  %add59 = add nsw i32 %mul38, 4
  %idxprom60 = sext i32 %add59 to i64
  %arrayidx61 = getelementptr inbounds float* %pos, i64 %idxprom60
  %18 = load float* %arrayidx61, align 4, !tbaa !3
  %add62 = fadd float %10, %18
  %add63 = add nsw i32 %mul38, 5
  %idxprom64 = sext i32 %add63 to i64
  %arrayidx65 = getelementptr inbounds float* %pos, i64 %idxprom64
  %19 = load float* %arrayidx65, align 4, !tbaa !3
  %add66 = fadd float %11, %19
  %add67 = add nsw i32 %mul38, 6
  %idxprom68 = sext i32 %add67 to i64
  %arrayidx69 = getelementptr inbounds float* %pos, i64 %idxprom68
  %20 = load float* %arrayidx69, align 4, !tbaa !3
  %add70 = fadd float %9, %20
  %add71 = add nsw i32 %mul38, 7
  %idxprom72 = sext i32 %add71 to i64
  %arrayidx73 = getelementptr inbounds float* %pos, i64 %idxprom72
  %21 = load float* %arrayidx73, align 4, !tbaa !3
  %add74 = fadd float %10, %21
  %add75 = add nsw i32 %mul38, 8
  %idxprom76 = sext i32 %add75 to i64
  %arrayidx77 = getelementptr inbounds float* %pos, i64 %idxprom76
  %22 = load float* %arrayidx77, align 4, !tbaa !3
  %add78 = fadd float %11, %22
  %cmp801450 = icmp slt i32 %12, %13
  br i1 %cmp801450, label %for.body81.lr.ph, label %for.end

for.body81.lr.ph:                                 ; preds = %for.body
  %23 = sext i32 %12 to i64
  br label %for.body81

for.body81:                                       ; preds = %for.body81.lr.ph, %for.body81
  %indvars.iv = phi i64 [ %23, %for.body81.lr.ph ], [ %indvars.iv.next, %for.body81 ]
  %vctot.01461 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add643, %for.body81 ]
  %vnbtot.01460 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %sub228, %for.body81 ]
  %fix1.01459 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add369, %for.body81 ]
  %fiy1.01458 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add370, %for.body81 ]
  %fiz1.01457 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add371, %for.body81 ]
  %fix2.01456 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add504, %for.body81 ]
  %fiy2.01455 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add505, %for.body81 ]
  %fiz2.01454 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add506, %for.body81 ]
  %fix3.01453 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add647, %for.body81 ]
  %fiy3.01452 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add648, %for.body81 ]
  %fiz3.01451 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add649, %for.body81 ]
  %arrayidx83 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %24 = load i32* %arrayidx83, align 4, !tbaa !0
  %mul84 = mul nsw i32 %24, 3
  %idxprom85 = sext i32 %mul84 to i64
  %arrayidx86 = getelementptr inbounds float* %pos, i64 %idxprom85
  %25 = load float* %arrayidx86, align 4, !tbaa !3
  %add87 = add nsw i32 %mul84, 1
  %idxprom88 = sext i32 %add87 to i64
  %arrayidx89 = getelementptr inbounds float* %pos, i64 %idxprom88
  %26 = load float* %arrayidx89, align 4, !tbaa !3
  %add90 = add nsw i32 %mul84, 2
  %idxprom91 = sext i32 %add90 to i64
  %arrayidx92 = getelementptr inbounds float* %pos, i64 %idxprom91
  %27 = load float* %arrayidx92, align 4, !tbaa !3
  %add93 = add nsw i32 %mul84, 3
  %idxprom94 = sext i32 %add93 to i64
  %arrayidx95 = getelementptr inbounds float* %pos, i64 %idxprom94
  %28 = load float* %arrayidx95, align 4, !tbaa !3
  %add96 = add nsw i32 %mul84, 4
  %idxprom97 = sext i32 %add96 to i64
  %arrayidx98 = getelementptr inbounds float* %pos, i64 %idxprom97
  %29 = load float* %arrayidx98, align 4, !tbaa !3
  %add99 = add nsw i32 %mul84, 5
  %idxprom100 = sext i32 %add99 to i64
  %arrayidx101 = getelementptr inbounds float* %pos, i64 %idxprom100
  %30 = load float* %arrayidx101, align 4, !tbaa !3
  %add102 = add nsw i32 %mul84, 6
  %idxprom103 = sext i32 %add102 to i64
  %arrayidx104 = getelementptr inbounds float* %pos, i64 %idxprom103
  %31 = load float* %arrayidx104, align 4, !tbaa !3
  %add105 = add nsw i32 %mul84, 7
  %idxprom106 = sext i32 %add105 to i64
  %arrayidx107 = getelementptr inbounds float* %pos, i64 %idxprom106
  %32 = load float* %arrayidx107, align 4, !tbaa !3
  %add108 = add nsw i32 %mul84, 8
  %idxprom109 = sext i32 %add108 to i64
  %arrayidx110 = getelementptr inbounds float* %pos, i64 %idxprom109
  %33 = load float* %arrayidx110, align 4, !tbaa !3
  %sub = fsub float %add46, %25
  %sub111 = fsub float %add50, %26
  %sub112 = fsub float %add54, %27
  %mul113 = fmul float %sub, %sub
  %mul114 = fmul float %sub111, %sub111
  %add115 = fadd float %mul113, %mul114
  %mul116 = fmul float %sub112, %sub112
  %add117 = fadd float %add115, %mul116
  %sub118 = fsub float %add46, %28
  %sub119 = fsub float %add50, %29
  %sub120 = fsub float %add54, %30
  %mul121 = fmul float %sub118, %sub118
  %mul122 = fmul float %sub119, %sub119
  %add123 = fadd float %mul121, %mul122
  %mul124 = fmul float %sub120, %sub120
  %add125 = fadd float %add123, %mul124
  %sub126 = fsub float %add46, %31
  %sub127 = fsub float %add50, %32
  %sub128 = fsub float %add54, %33
  %mul129 = fmul float %sub126, %sub126
  %mul130 = fmul float %sub127, %sub127
  %add131 = fadd float %mul129, %mul130
  %mul132 = fmul float %sub128, %sub128
  %add133 = fadd float %add131, %mul132
  %sub134 = fsub float %add58, %25
  %sub135 = fsub float %add62, %26
  %sub136 = fsub float %add66, %27
  %mul137 = fmul float %sub134, %sub134
  %mul138 = fmul float %sub135, %sub135
  %add139 = fadd float %mul137, %mul138
  %mul140 = fmul float %sub136, %sub136
  %add141 = fadd float %add139, %mul140
  %sub142 = fsub float %add58, %28
  %sub143 = fsub float %add62, %29
  %sub144 = fsub float %add66, %30
  %mul145 = fmul float %sub142, %sub142
  %mul146 = fmul float %sub143, %sub143
  %add147 = fadd float %mul145, %mul146
  %mul148 = fmul float %sub144, %sub144
  %add149 = fadd float %add147, %mul148
  %sub150 = fsub float %add58, %31
  %sub151 = fsub float %add62, %32
  %sub152 = fsub float %add66, %33
  %mul153 = fmul float %sub150, %sub150
  %mul154 = fmul float %sub151, %sub151
  %add155 = fadd float %mul153, %mul154
  %mul156 = fmul float %sub152, %sub152
  %add157 = fadd float %add155, %mul156
  %sub158 = fsub float %add70, %25
  %sub159 = fsub float %add74, %26
  %sub160 = fsub float %add78, %27
  %mul161 = fmul float %sub158, %sub158
  %mul162 = fmul float %sub159, %sub159
  %add163 = fadd float %mul161, %mul162
  %mul164 = fmul float %sub160, %sub160
  %add165 = fadd float %add163, %mul164
  %sub166 = fsub float %add70, %28
  %sub167 = fsub float %add74, %29
  %sub168 = fsub float %add78, %30
  %mul169 = fmul float %sub166, %sub166
  %mul170 = fmul float %sub167, %sub167
  %add171 = fadd float %mul169, %mul170
  %mul172 = fmul float %sub168, %sub168
  %add173 = fadd float %add171, %mul172
  %sub174 = fsub float %add70, %31
  %sub175 = fsub float %add74, %32
  %sub176 = fsub float %add78, %33
  %mul177 = fmul float %sub174, %sub174
  %mul178 = fmul float %sub175, %sub175
  %add179 = fadd float %mul177, %mul178
  %mul180 = fmul float %sub176, %sub176
  %add181 = fadd float %add179, %mul180
  %conv = fpext float %add117 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv182 = fptrunc double %div to float
  %conv183 = fpext float %add141 to double
  %call184 = tail call double @sqrt(double %conv183) #2
  %div185 = fdiv double 1.000000e+00, %call184
  %conv186 = fptrunc double %div185 to float
  %conv187 = fpext float %add165 to double
  %call188 = tail call double @sqrt(double %conv187) #2
  %div189 = fdiv double 1.000000e+00, %call188
  %conv190 = fptrunc double %div189 to float
  %conv191 = fpext float %add125 to double
  %call192 = tail call double @sqrt(double %conv191) #2
  %div193 = fdiv double 1.000000e+00, %call192
  %conv194 = fptrunc double %div193 to float
  %conv195 = fpext float %add149 to double
  %call196 = tail call double @sqrt(double %conv195) #2
  %div197 = fdiv double 1.000000e+00, %call196
  %conv198 = fptrunc double %div197 to float
  %conv199 = fpext float %add173 to double
  %call200 = tail call double @sqrt(double %conv199) #2
  %div201 = fdiv double 1.000000e+00, %call200
  %conv202 = fptrunc double %div201 to float
  %conv203 = fpext float %add133 to double
  %call204 = tail call double @sqrt(double %conv203) #2
  %div205 = fdiv double 1.000000e+00, %call204
  %conv206 = fptrunc double %div205 to float
  %conv207 = fpext float %add157 to double
  %call208 = tail call double @sqrt(double %conv207) #2
  %div209 = fdiv double 1.000000e+00, %call208
  %conv210 = fptrunc double %div209 to float
  %conv211 = fpext float %add181 to double
  %call212 = tail call double @sqrt(double %conv211) #2
  %div213 = fdiv double 1.000000e+00, %call212
  %conv214 = fptrunc double %div213 to float
  %mul215 = fmul float %add117, %conv182
  %mul216 = fmul float %conv182, %conv182
  %mul217 = fmul float %mul216, %mul216
  %mul218 = fmul float %mul216, %mul217
  %mul219 = fmul float %4, %mul218
  %mul220 = fmul float %5, %mul215
  %sub222 = fsub float -0.000000e+00, %mul220
  %conv223 = fpext float %sub222 to double
  %call224 = tail call double @exp(double %conv223) #2
  %mul225 = fmul double %conv221, %call224
  %conv226 = fptrunc double %mul225 to float
  %add227 = fadd float %vnbtot.01460, %conv226
  %sub228 = fsub float %add227, %mul219
  %mul229 = fmul float %mul215, %tabscale
  %conv230 = fptosi float %mul229 to i32
  %conv231 = sitofp i32 %conv230 to float
  %sub232 = fsub float %mul229, %conv231
  %mul233 = fmul float %sub232, %sub232
  %mul234 = shl nsw i32 %conv230, 2
  %idxprom235 = sext i32 %mul234 to i64
  %arrayidx236 = getelementptr inbounds float* %VFtab, i64 %idxprom235
  %34 = load float* %arrayidx236, align 4, !tbaa !3
  %add2371423 = or i32 %mul234, 1
  %idxprom238 = sext i32 %add2371423 to i64
  %arrayidx239 = getelementptr inbounds float* %VFtab, i64 %idxprom238
  %35 = load float* %arrayidx239, align 4, !tbaa !3
  %add2401424 = or i32 %mul234, 2
  %idxprom241 = sext i32 %add2401424 to i64
  %arrayidx242 = getelementptr inbounds float* %VFtab, i64 %idxprom241
  %36 = load float* %arrayidx242, align 4, !tbaa !3
  %mul243 = fmul float %sub232, %36
  %add2441425 = or i32 %mul234, 3
  %idxprom245 = sext i32 %add2441425 to i64
  %arrayidx246 = getelementptr inbounds float* %VFtab, i64 %idxprom245
  %37 = load float* %arrayidx246, align 4, !tbaa !3
  %mul247 = fmul float %mul233, %37
  %add248 = fadd float %35, %mul243
  %add249 = fadd float %add248, %mul247
  %mul250 = fmul float %sub232, %add249
  %add251 = fadd float %34, %mul250
  %add252 = fadd float %mul243, %add249
  %mul253 = fmul float %mul247, 2.000000e+00
  %add254 = fadd float %mul253, %add252
  %mul255 = fmul float %mul4, %add251
  %mul256 = fmul float %mul4, %add254
  %mul257 = fmul float %mul220, %conv226
  %mul258 = fmul float %mul219, 6.000000e+00
  %sub259 = fsub float %mul257, %mul258
  %mul260 = fmul float %conv182, %sub259
  %mul261 = fmul float %mul256, %tabscale
  %sub262 = fsub float %mul260, %mul261
  %mul263 = fmul float %conv182, %sub262
  %add264 = fadd float %vctot.01461, %mul255
  %mul265 = fmul float %sub, %mul263
  %mul266 = fmul float %sub111, %mul263
  %mul267 = fmul float %sub112, %mul263
  %add268 = fadd float %fix1.01459, %mul265
  %add269 = fadd float %fiy1.01458, %mul266
  %add270 = fadd float %fiz1.01457, %mul267
  %arrayidx272 = getelementptr inbounds float* %faction, i64 %idxprom85
  %38 = load float* %arrayidx272, align 4, !tbaa !3
  %sub273 = fsub float %38, %mul265
  %arrayidx276 = getelementptr inbounds float* %faction, i64 %idxprom88
  %39 = load float* %arrayidx276, align 4, !tbaa !3
  %sub277 = fsub float %39, %mul266
  %arrayidx280 = getelementptr inbounds float* %faction, i64 %idxprom91
  %40 = load float* %arrayidx280, align 4, !tbaa !3
  %sub281 = fsub float %40, %mul267
  %mul282 = fmul float %add125, %conv194
  %mul283 = fmul float %mul282, %tabscale
  %conv284 = fptosi float %mul283 to i32
  %conv285 = sitofp i32 %conv284 to float
  %sub286 = fsub float %mul283, %conv285
  %mul287 = fmul float %sub286, %sub286
  %mul288 = shl nsw i32 %conv284, 2
  %idxprom289 = sext i32 %mul288 to i64
  %arrayidx290 = getelementptr inbounds float* %VFtab, i64 %idxprom289
  %41 = load float* %arrayidx290, align 4, !tbaa !3
  %add2911426 = or i32 %mul288, 1
  %idxprom292 = sext i32 %add2911426 to i64
  %arrayidx293 = getelementptr inbounds float* %VFtab, i64 %idxprom292
  %42 = load float* %arrayidx293, align 4, !tbaa !3
  %add2941427 = or i32 %mul288, 2
  %idxprom295 = sext i32 %add2941427 to i64
  %arrayidx296 = getelementptr inbounds float* %VFtab, i64 %idxprom295
  %43 = load float* %arrayidx296, align 4, !tbaa !3
  %mul297 = fmul float %sub286, %43
  %add2981428 = or i32 %mul288, 3
  %idxprom299 = sext i32 %add2981428 to i64
  %arrayidx300 = getelementptr inbounds float* %VFtab, i64 %idxprom299
  %44 = load float* %arrayidx300, align 4, !tbaa !3
  %mul301 = fmul float %mul287, %44
  %add302 = fadd float %42, %mul297
  %add303 = fadd float %add302, %mul301
  %mul304 = fmul float %sub286, %add303
  %add305 = fadd float %41, %mul304
  %add306 = fadd float %mul297, %add303
  %mul307 = fmul float %mul301, 2.000000e+00
  %add308 = fadd float %mul307, %add306
  %mul309 = fmul float %mul6, %add305
  %mul310 = fmul float %mul6, %add308
  %mul311 = fmul float %mul310, %tabscale
  %45 = fmul float %conv194, %mul311
  %mul313 = fsub float -0.000000e+00, %45
  %add314 = fadd float %add264, %mul309
  %mul315 = fmul float %sub118, %mul313
  %mul316 = fmul float %sub119, %mul313
  %mul317 = fmul float %sub120, %mul313
  %add318 = fadd float %add268, %mul315
  %add319 = fadd float %add269, %mul316
  %add320 = fadd float %add270, %mul317
  %arrayidx323 = getelementptr inbounds float* %faction, i64 %idxprom94
  %46 = load float* %arrayidx323, align 4, !tbaa !3
  %sub324 = fsub float %46, %mul315
  %arrayidx327 = getelementptr inbounds float* %faction, i64 %idxprom97
  %47 = load float* %arrayidx327, align 4, !tbaa !3
  %sub328 = fsub float %47, %mul316
  %arrayidx331 = getelementptr inbounds float* %faction, i64 %idxprom100
  %48 = load float* %arrayidx331, align 4, !tbaa !3
  %sub332 = fsub float %48, %mul317
  %mul333 = fmul float %add133, %conv206
  %mul334 = fmul float %mul333, %tabscale
  %conv335 = fptosi float %mul334 to i32
  %conv336 = sitofp i32 %conv335 to float
  %sub337 = fsub float %mul334, %conv336
  %mul338 = fmul float %sub337, %sub337
  %mul339 = shl nsw i32 %conv335, 2
  %idxprom340 = sext i32 %mul339 to i64
  %arrayidx341 = getelementptr inbounds float* %VFtab, i64 %idxprom340
  %49 = load float* %arrayidx341, align 4, !tbaa !3
  %add3421429 = or i32 %mul339, 1
  %idxprom343 = sext i32 %add3421429 to i64
  %arrayidx344 = getelementptr inbounds float* %VFtab, i64 %idxprom343
  %50 = load float* %arrayidx344, align 4, !tbaa !3
  %add3451430 = or i32 %mul339, 2
  %idxprom346 = sext i32 %add3451430 to i64
  %arrayidx347 = getelementptr inbounds float* %VFtab, i64 %idxprom346
  %51 = load float* %arrayidx347, align 4, !tbaa !3
  %mul348 = fmul float %sub337, %51
  %add3491431 = or i32 %mul339, 3
  %idxprom350 = sext i32 %add3491431 to i64
  %arrayidx351 = getelementptr inbounds float* %VFtab, i64 %idxprom350
  %52 = load float* %arrayidx351, align 4, !tbaa !3
  %mul352 = fmul float %mul338, %52
  %add353 = fadd float %50, %mul348
  %add354 = fadd float %add353, %mul352
  %mul355 = fmul float %sub337, %add354
  %add356 = fadd float %49, %mul355
  %add357 = fadd float %mul348, %add354
  %mul358 = fmul float %mul352, 2.000000e+00
  %add359 = fadd float %mul358, %add357
  %mul360 = fmul float %mul6, %add356
  %mul361 = fmul float %mul6, %add359
  %mul362 = fmul float %mul361, %tabscale
  %53 = fmul float %conv206, %mul362
  %mul364 = fsub float -0.000000e+00, %53
  %add365 = fadd float %add314, %mul360
  %mul366 = fmul float %sub126, %mul364
  %mul367 = fmul float %sub127, %mul364
  %mul368 = fmul float %sub128, %mul364
  %add369 = fadd float %add318, %mul366
  %add370 = fadd float %add319, %mul367
  %add371 = fadd float %add320, %mul368
  %arrayidx374 = getelementptr inbounds float* %faction, i64 %idxprom103
  %54 = load float* %arrayidx374, align 4, !tbaa !3
  %sub375 = fsub float %54, %mul366
  %arrayidx378 = getelementptr inbounds float* %faction, i64 %idxprom106
  %55 = load float* %arrayidx378, align 4, !tbaa !3
  %sub379 = fsub float %55, %mul367
  %arrayidx382 = getelementptr inbounds float* %faction, i64 %idxprom109
  %56 = load float* %arrayidx382, align 4, !tbaa !3
  %sub383 = fsub float %56, %mul368
  %mul384 = fmul float %add141, %conv186
  %mul385 = fmul float %mul384, %tabscale
  %conv386 = fptosi float %mul385 to i32
  %conv387 = sitofp i32 %conv386 to float
  %sub388 = fsub float %mul385, %conv387
  %mul389 = fmul float %sub388, %sub388
  %mul390 = shl nsw i32 %conv386, 2
  %idxprom391 = sext i32 %mul390 to i64
  %arrayidx392 = getelementptr inbounds float* %VFtab, i64 %idxprom391
  %57 = load float* %arrayidx392, align 4, !tbaa !3
  %add3931432 = or i32 %mul390, 1
  %idxprom394 = sext i32 %add3931432 to i64
  %arrayidx395 = getelementptr inbounds float* %VFtab, i64 %idxprom394
  %58 = load float* %arrayidx395, align 4, !tbaa !3
  %add3961433 = or i32 %mul390, 2
  %idxprom397 = sext i32 %add3961433 to i64
  %arrayidx398 = getelementptr inbounds float* %VFtab, i64 %idxprom397
  %59 = load float* %arrayidx398, align 4, !tbaa !3
  %mul399 = fmul float %sub388, %59
  %add4001434 = or i32 %mul390, 3
  %idxprom401 = sext i32 %add4001434 to i64
  %arrayidx402 = getelementptr inbounds float* %VFtab, i64 %idxprom401
  %60 = load float* %arrayidx402, align 4, !tbaa !3
  %mul403 = fmul float %mul389, %60
  %add404 = fadd float %58, %mul399
  %add405 = fadd float %add404, %mul403
  %mul406 = fmul float %sub388, %add405
  %add407 = fadd float %57, %mul406
  %add408 = fadd float %mul399, %add405
  %mul409 = fmul float %mul403, 2.000000e+00
  %add410 = fadd float %mul409, %add408
  %mul411 = fmul float %mul6, %add407
  %mul412 = fmul float %mul6, %add410
  %mul413 = fmul float %mul412, %tabscale
  %61 = fmul float %conv186, %mul413
  %mul415 = fsub float -0.000000e+00, %61
  %add416 = fadd float %add365, %mul411
  %mul417 = fmul float %sub134, %mul415
  %mul418 = fmul float %sub135, %mul415
  %mul419 = fmul float %sub136, %mul415
  %add420 = fadd float %fix2.01456, %mul417
  %add421 = fadd float %fiy2.01455, %mul418
  %add422 = fadd float %fiz2.01454, %mul419
  %sub423 = fsub float %sub273, %mul417
  %sub424 = fsub float %sub277, %mul418
  %sub425 = fsub float %sub281, %mul419
  %mul426 = fmul float %add149, %conv198
  %mul427 = fmul float %mul426, %tabscale
  %conv428 = fptosi float %mul427 to i32
  %conv429 = sitofp i32 %conv428 to float
  %sub430 = fsub float %mul427, %conv429
  %mul431 = fmul float %sub430, %sub430
  %mul432 = shl nsw i32 %conv428, 2
  %idxprom433 = sext i32 %mul432 to i64
  %arrayidx434 = getelementptr inbounds float* %VFtab, i64 %idxprom433
  %62 = load float* %arrayidx434, align 4, !tbaa !3
  %add4351435 = or i32 %mul432, 1
  %idxprom436 = sext i32 %add4351435 to i64
  %arrayidx437 = getelementptr inbounds float* %VFtab, i64 %idxprom436
  %63 = load float* %arrayidx437, align 4, !tbaa !3
  %add4381436 = or i32 %mul432, 2
  %idxprom439 = sext i32 %add4381436 to i64
  %arrayidx440 = getelementptr inbounds float* %VFtab, i64 %idxprom439
  %64 = load float* %arrayidx440, align 4, !tbaa !3
  %mul441 = fmul float %sub430, %64
  %add4421437 = or i32 %mul432, 3
  %idxprom443 = sext i32 %add4421437 to i64
  %arrayidx444 = getelementptr inbounds float* %VFtab, i64 %idxprom443
  %65 = load float* %arrayidx444, align 4, !tbaa !3
  %mul445 = fmul float %mul431, %65
  %add446 = fadd float %63, %mul441
  %add447 = fadd float %add446, %mul445
  %mul448 = fmul float %sub430, %add447
  %add449 = fadd float %62, %mul448
  %add450 = fadd float %mul441, %add447
  %mul451 = fmul float %mul445, 2.000000e+00
  %add452 = fadd float %mul451, %add450
  %mul453 = fmul float %mul8, %add449
  %mul454 = fmul float %mul8, %add452
  %mul455 = fmul float %mul454, %tabscale
  %66 = fmul float %conv198, %mul455
  %mul457 = fsub float -0.000000e+00, %66
  %add458 = fadd float %add416, %mul453
  %mul459 = fmul float %sub142, %mul457
  %mul460 = fmul float %sub143, %mul457
  %mul461 = fmul float %sub144, %mul457
  %add462 = fadd float %add420, %mul459
  %add463 = fadd float %add421, %mul460
  %add464 = fadd float %add422, %mul461
  %sub465 = fsub float %sub324, %mul459
  %sub466 = fsub float %sub328, %mul460
  %sub467 = fsub float %sub332, %mul461
  %mul468 = fmul float %add157, %conv210
  %mul469 = fmul float %mul468, %tabscale
  %conv470 = fptosi float %mul469 to i32
  %conv471 = sitofp i32 %conv470 to float
  %sub472 = fsub float %mul469, %conv471
  %mul473 = fmul float %sub472, %sub472
  %mul474 = shl nsw i32 %conv470, 2
  %idxprom475 = sext i32 %mul474 to i64
  %arrayidx476 = getelementptr inbounds float* %VFtab, i64 %idxprom475
  %67 = load float* %arrayidx476, align 4, !tbaa !3
  %add4771438 = or i32 %mul474, 1
  %idxprom478 = sext i32 %add4771438 to i64
  %arrayidx479 = getelementptr inbounds float* %VFtab, i64 %idxprom478
  %68 = load float* %arrayidx479, align 4, !tbaa !3
  %add4801439 = or i32 %mul474, 2
  %idxprom481 = sext i32 %add4801439 to i64
  %arrayidx482 = getelementptr inbounds float* %VFtab, i64 %idxprom481
  %69 = load float* %arrayidx482, align 4, !tbaa !3
  %mul483 = fmul float %sub472, %69
  %add4841440 = or i32 %mul474, 3
  %idxprom485 = sext i32 %add4841440 to i64
  %arrayidx486 = getelementptr inbounds float* %VFtab, i64 %idxprom485
  %70 = load float* %arrayidx486, align 4, !tbaa !3
  %mul487 = fmul float %mul473, %70
  %add488 = fadd float %68, %mul483
  %add489 = fadd float %add488, %mul487
  %mul490 = fmul float %sub472, %add489
  %add491 = fadd float %67, %mul490
  %add492 = fadd float %mul483, %add489
  %mul493 = fmul float %mul487, 2.000000e+00
  %add494 = fadd float %mul493, %add492
  %mul495 = fmul float %mul8, %add491
  %mul496 = fmul float %mul8, %add494
  %mul497 = fmul float %mul496, %tabscale
  %71 = fmul float %conv210, %mul497
  %mul499 = fsub float -0.000000e+00, %71
  %add500 = fadd float %add458, %mul495
  %mul501 = fmul float %sub150, %mul499
  %mul502 = fmul float %sub151, %mul499
  %mul503 = fmul float %sub152, %mul499
  %add504 = fadd float %add462, %mul501
  %add505 = fadd float %add463, %mul502
  %add506 = fadd float %add464, %mul503
  %sub507 = fsub float %sub375, %mul501
  %sub508 = fsub float %sub379, %mul502
  %sub509 = fsub float %sub383, %mul503
  %mul510 = fmul float %add165, %conv190
  %mul511 = fmul float %mul510, %tabscale
  %conv512 = fptosi float %mul511 to i32
  %conv513 = sitofp i32 %conv512 to float
  %sub514 = fsub float %mul511, %conv513
  %mul515 = fmul float %sub514, %sub514
  %mul516 = shl nsw i32 %conv512, 2
  %idxprom517 = sext i32 %mul516 to i64
  %arrayidx518 = getelementptr inbounds float* %VFtab, i64 %idxprom517
  %72 = load float* %arrayidx518, align 4, !tbaa !3
  %add5191441 = or i32 %mul516, 1
  %idxprom520 = sext i32 %add5191441 to i64
  %arrayidx521 = getelementptr inbounds float* %VFtab, i64 %idxprom520
  %73 = load float* %arrayidx521, align 4, !tbaa !3
  %add5221442 = or i32 %mul516, 2
  %idxprom523 = sext i32 %add5221442 to i64
  %arrayidx524 = getelementptr inbounds float* %VFtab, i64 %idxprom523
  %74 = load float* %arrayidx524, align 4, !tbaa !3
  %mul525 = fmul float %sub514, %74
  %add5261443 = or i32 %mul516, 3
  %idxprom527 = sext i32 %add5261443 to i64
  %arrayidx528 = getelementptr inbounds float* %VFtab, i64 %idxprom527
  %75 = load float* %arrayidx528, align 4, !tbaa !3
  %mul529 = fmul float %mul515, %75
  %add530 = fadd float %73, %mul525
  %add531 = fadd float %add530, %mul529
  %mul532 = fmul float %sub514, %add531
  %add533 = fadd float %72, %mul532
  %add534 = fadd float %mul525, %add531
  %mul535 = fmul float %mul529, 2.000000e+00
  %add536 = fadd float %mul535, %add534
  %mul537 = fmul float %mul6, %add533
  %mul538 = fmul float %mul6, %add536
  %mul539 = fmul float %mul538, %tabscale
  %76 = fmul float %conv190, %mul539
  %mul541 = fsub float -0.000000e+00, %76
  %add542 = fadd float %add500, %mul537
  %mul543 = fmul float %sub158, %mul541
  %mul544 = fmul float %sub159, %mul541
  %mul545 = fmul float %sub160, %mul541
  %add546 = fadd float %fix3.01453, %mul543
  %add547 = fadd float %fiy3.01452, %mul544
  %add548 = fadd float %fiz3.01451, %mul545
  %sub549 = fsub float %sub423, %mul543
  store float %sub549, float* %arrayidx272, align 4, !tbaa !3
  %sub552 = fsub float %sub424, %mul544
  store float %sub552, float* %arrayidx276, align 4, !tbaa !3
  %sub556 = fsub float %sub425, %mul545
  store float %sub556, float* %arrayidx280, align 4, !tbaa !3
  %mul560 = fmul float %add173, %conv202
  %mul561 = fmul float %mul560, %tabscale
  %conv562 = fptosi float %mul561 to i32
  %conv563 = sitofp i32 %conv562 to float
  %sub564 = fsub float %mul561, %conv563
  %mul565 = fmul float %sub564, %sub564
  %mul566 = shl nsw i32 %conv562, 2
  %idxprom567 = sext i32 %mul566 to i64
  %arrayidx568 = getelementptr inbounds float* %VFtab, i64 %idxprom567
  %77 = load float* %arrayidx568, align 4, !tbaa !3
  %add5691444 = or i32 %mul566, 1
  %idxprom570 = sext i32 %add5691444 to i64
  %arrayidx571 = getelementptr inbounds float* %VFtab, i64 %idxprom570
  %78 = load float* %arrayidx571, align 4, !tbaa !3
  %add5721445 = or i32 %mul566, 2
  %idxprom573 = sext i32 %add5721445 to i64
  %arrayidx574 = getelementptr inbounds float* %VFtab, i64 %idxprom573
  %79 = load float* %arrayidx574, align 4, !tbaa !3
  %mul575 = fmul float %sub564, %79
  %add5761446 = or i32 %mul566, 3
  %idxprom577 = sext i32 %add5761446 to i64
  %arrayidx578 = getelementptr inbounds float* %VFtab, i64 %idxprom577
  %80 = load float* %arrayidx578, align 4, !tbaa !3
  %mul579 = fmul float %mul565, %80
  %add580 = fadd float %78, %mul575
  %add581 = fadd float %add580, %mul579
  %mul582 = fmul float %sub564, %add581
  %add583 = fadd float %77, %mul582
  %add584 = fadd float %mul575, %add581
  %mul585 = fmul float %mul579, 2.000000e+00
  %add586 = fadd float %mul585, %add584
  %mul587 = fmul float %mul8, %add583
  %mul588 = fmul float %mul8, %add586
  %mul589 = fmul float %mul588, %tabscale
  %81 = fmul float %conv202, %mul589
  %mul591 = fsub float -0.000000e+00, %81
  %add592 = fadd float %add542, %mul587
  %mul593 = fmul float %sub166, %mul591
  %mul594 = fmul float %sub167, %mul591
  %mul595 = fmul float %sub168, %mul591
  %add596 = fadd float %add546, %mul593
  %add597 = fadd float %add547, %mul594
  %add598 = fadd float %add548, %mul595
  %sub599 = fsub float %sub465, %mul593
  store float %sub599, float* %arrayidx323, align 4, !tbaa !3
  %sub603 = fsub float %sub466, %mul594
  store float %sub603, float* %arrayidx327, align 4, !tbaa !3
  %sub607 = fsub float %sub467, %mul595
  store float %sub607, float* %arrayidx331, align 4, !tbaa !3
  %mul611 = fmul float %add181, %conv214
  %mul612 = fmul float %mul611, %tabscale
  %conv613 = fptosi float %mul612 to i32
  %conv614 = sitofp i32 %conv613 to float
  %sub615 = fsub float %mul612, %conv614
  %mul616 = fmul float %sub615, %sub615
  %mul617 = shl nsw i32 %conv613, 2
  %idxprom618 = sext i32 %mul617 to i64
  %arrayidx619 = getelementptr inbounds float* %VFtab, i64 %idxprom618
  %82 = load float* %arrayidx619, align 4, !tbaa !3
  %add6201447 = or i32 %mul617, 1
  %idxprom621 = sext i32 %add6201447 to i64
  %arrayidx622 = getelementptr inbounds float* %VFtab, i64 %idxprom621
  %83 = load float* %arrayidx622, align 4, !tbaa !3
  %add6231448 = or i32 %mul617, 2
  %idxprom624 = sext i32 %add6231448 to i64
  %arrayidx625 = getelementptr inbounds float* %VFtab, i64 %idxprom624
  %84 = load float* %arrayidx625, align 4, !tbaa !3
  %mul626 = fmul float %sub615, %84
  %add6271449 = or i32 %mul617, 3
  %idxprom628 = sext i32 %add6271449 to i64
  %arrayidx629 = getelementptr inbounds float* %VFtab, i64 %idxprom628
  %85 = load float* %arrayidx629, align 4, !tbaa !3
  %mul630 = fmul float %mul616, %85
  %add631 = fadd float %83, %mul626
  %add632 = fadd float %add631, %mul630
  %mul633 = fmul float %sub615, %add632
  %add634 = fadd float %82, %mul633
  %add635 = fadd float %mul626, %add632
  %mul636 = fmul float %mul630, 2.000000e+00
  %add637 = fadd float %mul636, %add635
  %mul638 = fmul float %mul8, %add634
  %mul639 = fmul float %mul8, %add637
  %mul640 = fmul float %mul639, %tabscale
  %86 = fmul float %conv214, %mul640
  %mul642 = fsub float -0.000000e+00, %86
  %add643 = fadd float %add592, %mul638
  %mul644 = fmul float %sub174, %mul642
  %mul645 = fmul float %sub175, %mul642
  %mul646 = fmul float %sub176, %mul642
  %add647 = fadd float %add596, %mul644
  %add648 = fadd float %add597, %mul645
  %add649 = fadd float %add598, %mul646
  %sub650 = fsub float %sub507, %mul644
  store float %sub650, float* %arrayidx374, align 4, !tbaa !3
  %sub654 = fsub float %sub508, %mul645
  store float %sub654, float* %arrayidx378, align 4, !tbaa !3
  %sub658 = fsub float %sub509, %mul646
  store float %sub658, float* %arrayidx382, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %87 = trunc i64 %indvars.iv.next to i32
  %cmp80 = icmp slt i32 %87, %13
  br i1 %cmp80, label %for.body81, label %for.end

for.end:                                          ; preds = %for.body81, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add643, %for.body81 ]
  %vnbtot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %sub228, %for.body81 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add369, %for.body81 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add370, %for.body81 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add371, %for.body81 ]
  %fix2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add504, %for.body81 ]
  %fiy2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add505, %for.body81 ]
  %fiz2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add506, %for.body81 ]
  %fix3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add647, %for.body81 ]
  %fiy3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add648, %for.body81 ]
  %fiz3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add649, %for.body81 ]
  %arrayidx663 = getelementptr inbounds float* %faction, i64 %idxprom44
  %88 = load float* %arrayidx663, align 4, !tbaa !3
  %add664 = fadd float %fix1.0.lcssa, %88
  store float %add664, float* %arrayidx663, align 4, !tbaa !3
  %arrayidx669 = getelementptr inbounds float* %faction, i64 %idxprom48
  %89 = load float* %arrayidx669, align 4, !tbaa !3
  %add670 = fadd float %fiy1.0.lcssa, %89
  store float %add670, float* %arrayidx669, align 4, !tbaa !3
  %arrayidx676 = getelementptr inbounds float* %faction, i64 %idxprom52
  %90 = load float* %arrayidx676, align 4, !tbaa !3
  %add677 = fadd float %fiz1.0.lcssa, %90
  store float %add677, float* %arrayidx676, align 4, !tbaa !3
  %arrayidx683 = getelementptr inbounds float* %faction, i64 %idxprom56
  %91 = load float* %arrayidx683, align 4, !tbaa !3
  %add684 = fadd float %fix2.0.lcssa, %91
  store float %add684, float* %arrayidx683, align 4, !tbaa !3
  %arrayidx690 = getelementptr inbounds float* %faction, i64 %idxprom60
  %92 = load float* %arrayidx690, align 4, !tbaa !3
  %add691 = fadd float %fiy2.0.lcssa, %92
  store float %add691, float* %arrayidx690, align 4, !tbaa !3
  %arrayidx697 = getelementptr inbounds float* %faction, i64 %idxprom64
  %93 = load float* %arrayidx697, align 4, !tbaa !3
  %add698 = fadd float %fiz2.0.lcssa, %93
  store float %add698, float* %arrayidx697, align 4, !tbaa !3
  %arrayidx704 = getelementptr inbounds float* %faction, i64 %idxprom68
  %94 = load float* %arrayidx704, align 4, !tbaa !3
  %add705 = fadd float %fix3.0.lcssa, %94
  store float %add705, float* %arrayidx704, align 4, !tbaa !3
  %arrayidx711 = getelementptr inbounds float* %faction, i64 %idxprom72
  %95 = load float* %arrayidx711, align 4, !tbaa !3
  %add712 = fadd float %fiy3.0.lcssa, %95
  store float %add712, float* %arrayidx711, align 4, !tbaa !3
  %arrayidx718 = getelementptr inbounds float* %faction, i64 %idxprom76
  %96 = load float* %arrayidx718, align 4, !tbaa !3
  %add719 = fadd float %fiz3.0.lcssa, %96
  store float %add719, float* %arrayidx718, align 4, !tbaa !3
  %arrayidx724 = getelementptr inbounds float* %fshift, i64 %idxprom28
  %97 = load float* %arrayidx724, align 4, !tbaa !3
  %add725 = fadd float %fix1.0.lcssa, %97
  %add726 = fadd float %fix2.0.lcssa, %add725
  %add727 = fadd float %fix3.0.lcssa, %add726
  store float %add727, float* %arrayidx724, align 4, !tbaa !3
  %arrayidx732 = getelementptr inbounds float* %fshift, i64 %idxprom31
  %98 = load float* %arrayidx732, align 4, !tbaa !3
  %add733 = fadd float %fiy1.0.lcssa, %98
  %add734 = fadd float %fiy2.0.lcssa, %add733
  %add735 = fadd float %fiy3.0.lcssa, %add734
  store float %add735, float* %arrayidx732, align 4, !tbaa !3
  %arrayidx741 = getelementptr inbounds float* %fshift, i64 %idxprom34
  %99 = load float* %arrayidx741, align 4, !tbaa !3
  %add742 = fadd float %fiz1.0.lcssa, %99
  %add743 = fadd float %fiz2.0.lcssa, %add742
  %add744 = fadd float %fiz3.0.lcssa, %add743
  store float %add744, float* %arrayidx741, align 4, !tbaa !3
  %arrayidx749 = getelementptr inbounds i32* %gid, i64 %indvars.iv1475
  %100 = load i32* %arrayidx749, align 4, !tbaa !0
  %idxprom750 = sext i32 %100 to i64
  %arrayidx751 = getelementptr inbounds float* %Vc, i64 %idxprom750
  %101 = load float* %arrayidx751, align 4, !tbaa !3
  %add752 = fadd float %vctot.0.lcssa, %101
  store float %add752, float* %arrayidx751, align 4, !tbaa !3
  %arrayidx756 = getelementptr inbounds float* %Vnb, i64 %idxprom750
  %102 = load float* %arrayidx756, align 4, !tbaa !3
  %add757 = fadd float %vnbtot.0.lcssa, %102
  store float %add757, float* %arrayidx756, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next1476 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end762, label %for.end.for.body_crit_edge

for.end.for.body_crit_edge:                       ; preds = %for.end
  %arrayidx37.phi.trans.insert = getelementptr inbounds i32* %iinr, i64 %indvars.iv.next1476
  %.pre = load i32* %arrayidx37.phi.trans.insert, align 4, !tbaa !0
  br label %for.body

for.end762:                                       ; preds = %for.end, %entry
  ret void
}
