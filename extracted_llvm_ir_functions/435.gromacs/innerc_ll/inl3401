define void @inl3401(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc, i32* nocapture %type, i32 %ntype, float* nocapture %nbfp, float* nocapture %Vnb, float %tabscale, float* nocapture %VFtab, float %exptabscale, float %lambda, float* nocapture %dvdlambda, float* nocapture %chargeB, i32* nocapture %typeB) #0 {
entry:
  %sub = fsub float 1.000000e+00, %lambda
  %cmp559 = icmp sgt i32 %nri, 0
  br i1 %cmp559, label %for.body.lr.ph, label %for.end314

for.body.lr.ph:                                   ; preds = %entry
  %mul33 = mul nsw i32 %ntype, 3
  br label %for.body

for.body:                                         ; preds = %for.end, %for.body.lr.ph
  %indvars.iv563 = phi i64 [ 0, %for.body.lr.ph ], [ %indvars.iv.next564, %for.end ]
  %dvdl.0560 = phi float [ 0.000000e+00, %for.body.lr.ph ], [ %dvdl.1.lcssa, %for.end ]
  %arrayidx = getelementptr inbounds i32* %shift, i64 %indvars.iv563
  %0 = load i32* %arrayidx, align 4, !tbaa !0
  %mul = mul nsw i32 %0, 3
  %idxprom1 = sext i32 %mul to i64
  %arrayidx2 = getelementptr inbounds float* %shiftvec, i64 %idxprom1
  %1 = load float* %arrayidx2, align 4, !tbaa !3
  %add = add nsw i32 %mul, 1
  %idxprom3 = sext i32 %add to i64
  %arrayidx4 = getelementptr inbounds float* %shiftvec, i64 %idxprom3
  %2 = load float* %arrayidx4, align 4, !tbaa !3
  %add5 = add nsw i32 %mul, 2
  %idxprom6 = sext i32 %add5 to i64
  %arrayidx7 = getelementptr inbounds float* %shiftvec, i64 %idxprom6
  %3 = load float* %arrayidx7, align 4, !tbaa !3
  %arrayidx9 = getelementptr inbounds i32* %iinr, i64 %indvars.iv563
  %4 = load i32* %arrayidx9, align 4, !tbaa !0
  %mul10 = mul nsw i32 %4, 3
  %arrayidx12 = getelementptr inbounds i32* %jindex, i64 %indvars.iv563
  %5 = load i32* %arrayidx12, align 4, !tbaa !0
  %indvars.iv.next564 = add i64 %indvars.iv563, 1
  %arrayidx15 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next564
  %6 = load i32* %arrayidx15, align 4, !tbaa !0
  %idxprom16 = sext i32 %mul10 to i64
  %arrayidx17 = getelementptr inbounds float* %pos, i64 %idxprom16
  %7 = load float* %arrayidx17, align 4, !tbaa !3
  %add18 = fadd float %1, %7
  %add19 = add nsw i32 %mul10, 1
  %idxprom20 = sext i32 %add19 to i64
  %arrayidx21 = getelementptr inbounds float* %pos, i64 %idxprom20
  %8 = load float* %arrayidx21, align 4, !tbaa !3
  %add22 = fadd float %2, %8
  %add23 = add nsw i32 %mul10, 2
  %idxprom24 = sext i32 %add23 to i64
  %arrayidx25 = getelementptr inbounds float* %pos, i64 %idxprom24
  %9 = load float* %arrayidx25, align 4, !tbaa !3
  %add26 = fadd float %3, %9
  %idxprom27 = sext i32 %4 to i64
  %arrayidx28 = getelementptr inbounds float* %charge, i64 %idxprom27
  %10 = load float* %arrayidx28, align 4, !tbaa !3
  %mul29 = fmul float %10, %facel
  %arrayidx31 = getelementptr inbounds float* %chargeB, i64 %idxprom27
  %11 = load float* %arrayidx31, align 4, !tbaa !3
  %mul32 = fmul float %11, %facel
  %arrayidx35 = getelementptr inbounds i32* %type, i64 %idxprom27
  %12 = load i32* %arrayidx35, align 4, !tbaa !0
  %mul36 = mul nsw i32 %12, %mul33
  %arrayidx39 = getelementptr inbounds i32* %typeB, i64 %idxprom27
  %13 = load i32* %arrayidx39, align 4, !tbaa !0
  %mul40 = mul nsw i32 %13, %mul33
  %cmp42546 = icmp slt i32 %5, %6
  br i1 %cmp42546, label %for.body43.lr.ph, label %for.end

for.body43.lr.ph:                                 ; preds = %for.body
  %14 = sext i32 %5 to i64
  br label %for.body43

for.body43:                                       ; preds = %for.body43.lr.ph, %for.body43
  %indvars.iv = phi i64 [ %14, %for.body43.lr.ph ], [ %indvars.iv.next, %for.body43 ]
  %vctot.0552 = phi float [ 0.000000e+00, %for.body43.lr.ph ], [ %add236, %for.body43 ]
  %dvdl.1551 = phi float [ %dvdl.0560, %for.body43.lr.ph ], [ %sub229, %for.body43 ]
  %vnbtot.0550 = phi float [ 0.000000e+00, %for.body43.lr.ph ], [ %add227, %for.body43 ]
  %fix1.0549 = phi float [ 0.000000e+00, %for.body43.lr.ph ], [ %add240, %for.body43 ]
  %fiy1.0548 = phi float [ 0.000000e+00, %for.body43.lr.ph ], [ %add241, %for.body43 ]
  %fiz1.0547 = phi float [ 0.000000e+00, %for.body43.lr.ph ], [ %add242, %for.body43 ]
  %arrayidx45 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %15 = load i32* %arrayidx45, align 4, !tbaa !0
  %mul46 = mul nsw i32 %15, 3
  %idxprom47 = sext i32 %mul46 to i64
  %arrayidx48 = getelementptr inbounds float* %pos, i64 %idxprom47
  %16 = load float* %arrayidx48, align 4, !tbaa !3
  %add49 = add nsw i32 %mul46, 1
  %idxprom50 = sext i32 %add49 to i64
  %arrayidx51 = getelementptr inbounds float* %pos, i64 %idxprom50
  %17 = load float* %arrayidx51, align 4, !tbaa !3
  %add52 = add nsw i32 %mul46, 2
  %idxprom53 = sext i32 %add52 to i64
  %arrayidx54 = getelementptr inbounds float* %pos, i64 %idxprom53
  %18 = load float* %arrayidx54, align 4, !tbaa !3
  %sub55 = fsub float %add18, %16
  %sub56 = fsub float %add22, %17
  %sub57 = fsub float %add26, %18
  %mul58 = fmul float %sub55, %sub55
  %mul59 = fmul float %sub56, %sub56
  %add60 = fadd float %mul58, %mul59
  %mul61 = fmul float %sub57, %sub57
  %add62 = fadd float %add60, %mul61
  %conv = fpext float %add62 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv63 = fptrunc double %div to float
  %mul64 = fmul float %add62, %conv63
  %mul65 = fmul float %mul64, %tabscale
  %conv66 = fptosi float %mul65 to i32
  %conv67 = sitofp i32 %conv66 to float
  %sub68 = fsub float %mul65, %conv67
  %mul69 = fmul float %sub68, %sub68
  %mul70 = mul nsw i32 %conv66, 12
  %idxprom71 = sext i32 %15 to i64
  %arrayidx72 = getelementptr inbounds float* %charge, i64 %idxprom71
  %19 = load float* %arrayidx72, align 4, !tbaa !3
  %mul73 = fmul float %mul29, %19
  %arrayidx75 = getelementptr inbounds float* %chargeB, i64 %idxprom71
  %20 = load float* %arrayidx75, align 4, !tbaa !3
  %mul76 = fmul float %mul32, %20
  %mul77 = fmul float %sub, %mul73
  %mul78 = fmul float %mul76, %lambda
  %add79 = fadd float %mul77, %mul78
  %idxprom80 = sext i32 %mul70 to i64
  %arrayidx81 = getelementptr inbounds float* %VFtab, i64 %idxprom80
  %21 = load float* %arrayidx81, align 4, !tbaa !3
  %add82543 = or i32 %mul70, 1
  %idxprom83 = sext i32 %add82543 to i64
  %arrayidx84 = getelementptr inbounds float* %VFtab, i64 %idxprom83
  %22 = load float* %arrayidx84, align 4, !tbaa !3
  %add85544 = or i32 %mul70, 2
  %idxprom86 = sext i32 %add85544 to i64
  %arrayidx87 = getelementptr inbounds float* %VFtab, i64 %idxprom86
  %23 = load float* %arrayidx87, align 4, !tbaa !3
  %mul88 = fmul float %23, %sub68
  %add89545 = or i32 %mul70, 3
  %idxprom90 = sext i32 %add89545 to i64
  %arrayidx91 = getelementptr inbounds float* %VFtab, i64 %idxprom90
  %24 = load float* %arrayidx91, align 4, !tbaa !3
  %mul92 = fmul float %24, %mul69
  %add93 = fadd float %22, %mul88
  %add94 = fadd float %add93, %mul92
  %mul95 = fmul float %sub68, %add94
  %add96 = fadd float %21, %mul95
  %add97 = fadd float %mul88, %add94
  %mul98 = fmul float %mul92, 2.000000e+00
  %add99 = fadd float %mul98, %add97
  %mul100 = fmul float %add79, %add96
  %mul101 = fmul float %add79, %add99
  %sub102 = fsub float %mul76, %mul73
  %mul103 = fmul float %sub102, %add96
  %add104 = fadd float %dvdl.1551, %mul103
  %arrayidx106 = getelementptr inbounds i32* %type, i64 %idxprom71
  %25 = load i32* %arrayidx106, align 4, !tbaa !0
  %mul107 = mul nsw i32 %25, 3
  %add108 = add nsw i32 %mul107, %mul36
  %arrayidx110 = getelementptr inbounds i32* %typeB, i64 %idxprom71
  %26 = load i32* %arrayidx110, align 4, !tbaa !0
  %mul111 = mul nsw i32 %26, 3
  %add112 = add nsw i32 %mul111, %mul40
  %idxprom113 = sext i32 %add108 to i64
  %arrayidx114 = getelementptr inbounds float* %nbfp, i64 %idxprom113
  %27 = load float* %arrayidx114, align 4, !tbaa !3
  %idxprom115 = sext i32 %add112 to i64
  %arrayidx116 = getelementptr inbounds float* %nbfp, i64 %idxprom115
  %28 = load float* %arrayidx116, align 4, !tbaa !3
  %mul117 = fmul float %sub, %27
  %mul118 = fmul float %28, %lambda
  %add119 = fadd float %mul117, %mul118
  %add120 = add nsw i32 %add108, 1
  %idxprom121 = sext i32 %add120 to i64
  %arrayidx122 = getelementptr inbounds float* %nbfp, i64 %idxprom121
  %29 = load float* %arrayidx122, align 4, !tbaa !3
  %add123 = add nsw i32 %add112, 1
  %idxprom124 = sext i32 %add123 to i64
  %arrayidx125 = getelementptr inbounds float* %nbfp, i64 %idxprom124
  %30 = load float* %arrayidx125, align 4, !tbaa !3
  %add126 = add nsw i32 %add108, 2
  %idxprom127 = sext i32 %add126 to i64
  %arrayidx128 = getelementptr inbounds float* %nbfp, i64 %idxprom127
  %31 = load float* %arrayidx128, align 4, !tbaa !3
  %add129 = add nsw i32 %add112, 2
  %idxprom130 = sext i32 %add129 to i64
  %arrayidx131 = getelementptr inbounds float* %nbfp, i64 %idxprom130
  %32 = load float* %arrayidx131, align 4, !tbaa !3
  %add132 = add nsw i32 %mul70, 4
  %idxprom133 = sext i32 %add132 to i64
  %arrayidx134 = getelementptr inbounds float* %VFtab, i64 %idxprom133
  %33 = load float* %arrayidx134, align 4, !tbaa !3
  %add135 = add nsw i32 %mul70, 5
  %idxprom136 = sext i32 %add135 to i64
  %arrayidx137 = getelementptr inbounds float* %VFtab, i64 %idxprom136
  %34 = load float* %arrayidx137, align 4, !tbaa !3
  %add138 = add nsw i32 %mul70, 6
  %idxprom139 = sext i32 %add138 to i64
  %arrayidx140 = getelementptr inbounds float* %VFtab, i64 %idxprom139
  %35 = load float* %arrayidx140, align 4, !tbaa !3
  %mul141 = fmul float %sub68, %35
  %add142 = add nsw i32 %mul70, 7
  %idxprom143 = sext i32 %add142 to i64
  %arrayidx144 = getelementptr inbounds float* %VFtab, i64 %idxprom143
  %36 = load float* %arrayidx144, align 4, !tbaa !3
  %mul145 = fmul float %mul69, %36
  %add146 = fadd float %34, %mul141
  %add147 = fadd float %add146, %mul145
  %mul148 = fmul float %sub68, %add147
  %add149 = fadd float %33, %mul148
  %add150 = fadd float %mul141, %add147
  %mul151 = fmul float %mul145, 2.000000e+00
  %add152 = fadd float %mul151, %add150
  %mul153 = fmul float %add119, %add149
  %mul154 = fmul float %add119, %add152
  %sub155 = fsub float %28, %27
  %mul156 = fmul float %sub155, %add149
  %add157 = fadd float %add104, %mul156
  %mul158 = fmul float %mul64, %31
  %mul159 = fmul float %mul158, %exptabscale
  %conv160 = fptosi float %mul159 to i32
  %conv161 = sitofp i32 %conv160 to float
  %sub162 = fsub float %mul159, %conv161
  %mul163 = fmul float %sub162, %sub162
  %mul164 = mul nsw i32 %conv160, 12
  %add165 = add nsw i32 %mul164, 8
  %idxprom166 = sext i32 %add165 to i64
  %arrayidx167 = getelementptr inbounds float* %VFtab, i64 %idxprom166
  %37 = load float* %arrayidx167, align 4, !tbaa !3
  %add168 = add nsw i32 %mul164, 9
  %idxprom169 = sext i32 %add168 to i64
  %arrayidx170 = getelementptr inbounds float* %VFtab, i64 %idxprom169
  %38 = load float* %arrayidx170, align 4, !tbaa !3
  %add171 = add nsw i32 %mul164, 10
  %idxprom172 = sext i32 %add171 to i64
  %arrayidx173 = getelementptr inbounds float* %VFtab, i64 %idxprom172
  %39 = load float* %arrayidx173, align 4, !tbaa !3
  %mul174 = fmul float %sub162, %39
  %add175 = add nsw i32 %mul164, 11
  %idxprom176 = sext i32 %add175 to i64
  %arrayidx177 = getelementptr inbounds float* %VFtab, i64 %idxprom176
  %40 = load float* %arrayidx177, align 4, !tbaa !3
  %mul178 = fmul float %mul163, %40
  %add179 = fadd float %38, %mul174
  %add180 = fadd float %add179, %mul178
  %mul181 = fmul float %sub162, %add180
  %add182 = fadd float %37, %mul181
  %add183 = fadd float %mul174, %add180
  %mul184 = fmul float %mul178, 2.000000e+00
  %add185 = fadd float %mul184, %add183
  %mul186 = fmul float %29, %add182
  %mul187 = fmul float %29, %31
  %mul188 = fmul float %mul187, %add185
  %mul189 = fmul float %mul64, %32
  %mul190 = fmul float %mul189, %exptabscale
  %conv191 = fptosi float %mul190 to i32
  %conv192 = sitofp i32 %conv191 to float
  %sub193 = fsub float %mul190, %conv192
  %mul194 = fmul float %sub193, %sub193
  %mul195 = mul nsw i32 %conv191, 12
  %add196 = add nsw i32 %mul195, 8
  %idxprom197 = sext i32 %add196 to i64
  %arrayidx198 = getelementptr inbounds float* %VFtab, i64 %idxprom197
  %41 = load float* %arrayidx198, align 4, !tbaa !3
  %add199 = add nsw i32 %mul195, 9
  %idxprom200 = sext i32 %add199 to i64
  %arrayidx201 = getelementptr inbounds float* %VFtab, i64 %idxprom200
  %42 = load float* %arrayidx201, align 4, !tbaa !3
  %add202 = add nsw i32 %mul195, 10
  %idxprom203 = sext i32 %add202 to i64
  %arrayidx204 = getelementptr inbounds float* %VFtab, i64 %idxprom203
  %43 = load float* %arrayidx204, align 4, !tbaa !3
  %mul205 = fmul float %sub193, %43
  %add206 = add nsw i32 %mul195, 11
  %idxprom207 = sext i32 %add206 to i64
  %arrayidx208 = getelementptr inbounds float* %VFtab, i64 %idxprom207
  %44 = load float* %arrayidx208, align 4, !tbaa !3
  %mul209 = fmul float %mul194, %44
  %add210 = fadd float %42, %mul205
  %add211 = fadd float %add210, %mul209
  %mul212 = fmul float %sub193, %add211
  %add213 = fadd float %41, %mul212
  %add214 = fadd float %mul205, %add211
  %mul215 = fmul float %mul209, 2.000000e+00
  %add216 = fadd float %mul215, %add214
  %mul217 = fmul float %30, %add213
  %mul218 = fmul float %30, %32
  %mul219 = fmul float %mul218, %add216
  %mul220 = fmul float %sub, %mul188
  %mul221 = fmul float %mul219, %lambda
  %add222 = fadd float %mul220, %mul221
  %add223 = fadd float %vnbtot.0550, %mul153
  %mul224 = fmul float %sub, %mul186
  %add225 = fadd float %add223, %mul224
  %mul226 = fmul float %mul217, %lambda
  %add227 = fadd float %add225, %mul226
  %add228 = fadd float %add157, %mul217
  %sub229 = fsub float %add228, %mul186
  %add230 = fadd float %mul101, %mul154
  %mul231 = fmul float %add230, %tabscale
  %mul232 = fmul float %add222, %exptabscale
  %add233 = fadd float %mul231, %mul232
  %45 = fmul float %conv63, %add233
  %mul235 = fsub float -0.000000e+00, %45
  %add236 = fadd float %vctot.0552, %mul100
  %mul237 = fmul float %sub55, %mul235
  %mul238 = fmul float %sub56, %mul235
  %mul239 = fmul float %sub57, %mul235
  %add240 = fadd float %fix1.0549, %mul237
  %add241 = fadd float %fiy1.0548, %mul238
  %add242 = fadd float %fiz1.0547, %mul239
  %arrayidx244 = getelementptr inbounds float* %faction, i64 %idxprom47
  %46 = load float* %arrayidx244, align 4, !tbaa !3
  %sub245 = fsub float %46, %mul237
  store float %sub245, float* %arrayidx244, align 4, !tbaa !3
  %arrayidx250 = getelementptr inbounds float* %faction, i64 %idxprom50
  %47 = load float* %arrayidx250, align 4, !tbaa !3
  %sub251 = fsub float %47, %mul238
  store float %sub251, float* %arrayidx250, align 4, !tbaa !3
  %arrayidx257 = getelementptr inbounds float* %faction, i64 %idxprom53
  %48 = load float* %arrayidx257, align 4, !tbaa !3
  %sub258 = fsub float %48, %mul239
  store float %sub258, float* %arrayidx257, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %49 = trunc i64 %indvars.iv.next to i32
  %cmp42 = icmp slt i32 %49, %6
  br i1 %cmp42, label %for.body43, label %for.end

for.end:                                          ; preds = %for.body43, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add236, %for.body43 ]
  %dvdl.1.lcssa = phi float [ %dvdl.0560, %for.body ], [ %sub229, %for.body43 ]
  %vnbtot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add227, %for.body43 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add240, %for.body43 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add241, %for.body43 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add242, %for.body43 ]
  %arrayidx263 = getelementptr inbounds float* %faction, i64 %idxprom16
  %50 = load float* %arrayidx263, align 4, !tbaa !3
  %add264 = fadd float %fix1.0.lcssa, %50
  store float %add264, float* %arrayidx263, align 4, !tbaa !3
  %arrayidx269 = getelementptr inbounds float* %faction, i64 %idxprom20
  %51 = load float* %arrayidx269, align 4, !tbaa !3
  %add270 = fadd float %fiy1.0.lcssa, %51
  store float %add270, float* %arrayidx269, align 4, !tbaa !3
  %arrayidx276 = getelementptr inbounds float* %faction, i64 %idxprom24
  %52 = load float* %arrayidx276, align 4, !tbaa !3
  %add277 = fadd float %fiz1.0.lcssa, %52
  store float %add277, float* %arrayidx276, align 4, !tbaa !3
  %arrayidx282 = getelementptr inbounds float* %fshift, i64 %idxprom1
  %53 = load float* %arrayidx282, align 4, !tbaa !3
  %add283 = fadd float %fix1.0.lcssa, %53
  store float %add283, float* %arrayidx282, align 4, !tbaa !3
  %arrayidx288 = getelementptr inbounds float* %fshift, i64 %idxprom3
  %54 = load float* %arrayidx288, align 4, !tbaa !3
  %add289 = fadd float %fiy1.0.lcssa, %54
  store float %add289, float* %arrayidx288, align 4, !tbaa !3
  %arrayidx295 = getelementptr inbounds float* %fshift, i64 %idxprom6
  %55 = load float* %arrayidx295, align 4, !tbaa !3
  %add296 = fadd float %fiz1.0.lcssa, %55
  store float %add296, float* %arrayidx295, align 4, !tbaa !3
  %arrayidx301 = getelementptr inbounds i32* %gid, i64 %indvars.iv563
  %56 = load i32* %arrayidx301, align 4, !tbaa !0
  %idxprom302 = sext i32 %56 to i64
  %arrayidx303 = getelementptr inbounds float* %Vc, i64 %idxprom302
  %57 = load float* %arrayidx303, align 4, !tbaa !3
  %add304 = fadd float %vctot.0.lcssa, %57
  store float %add304, float* %arrayidx303, align 4, !tbaa !3
  %arrayidx308 = getelementptr inbounds float* %Vnb, i64 %idxprom302
  %58 = load float* %arrayidx308, align 4, !tbaa !3
  %add309 = fadd float %vnbtot.0.lcssa, %58
  store float %add309, float* %arrayidx308, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next564 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end314, label %for.body

for.end314:                                       ; preds = %for.end, %entry
  %dvdl.0.lcssa = phi float [ 0.000000e+00, %entry ], [ %dvdl.1.lcssa, %for.end ]
  %59 = load float* %dvdlambda, align 4, !tbaa !3
  %add315 = fadd float %dvdl.0.lcssa, %59
  store float %add315, float* %dvdlambda, align 4, !tbaa !3
  ret void
}
