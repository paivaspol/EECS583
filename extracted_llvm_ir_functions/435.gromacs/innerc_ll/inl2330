define void @inl2330(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc, float %krf, float %crf, i32* nocapture %type, i32 %ntype, float* nocapture %nbfp, float* nocapture %Vnb, float %tabscale, float* nocapture %VFtab) #0 {
entry:
  %0 = load i32* %iinr, align 4, !tbaa !0
  %idxprom = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds float* %charge, i64 %idxprom
  %1 = load float* %arrayidx1, align 4, !tbaa !3
  %add = add nsw i32 %0, 1
  %idxprom2 = sext i32 %add to i64
  %arrayidx3 = getelementptr inbounds float* %charge, i64 %idxprom2
  %2 = load float* %arrayidx3, align 4, !tbaa !3
  %mul = fmul float %1, %facel
  %mul4 = fmul float %1, %mul
  %mul6 = fmul float %mul, %2
  %mul7 = fmul float %2, %facel
  %mul8 = fmul float %2, %mul7
  %mul9 = shl i32 %ntype, 1
  %arrayidx11 = getelementptr inbounds i32* %type, i64 %idxprom
  %3 = load i32* %arrayidx11, align 4, !tbaa !0
  %mul12 = mul nsw i32 %mul9, %3
  %mul15 = shl nsw i32 %3, 1
  %add16 = add nsw i32 %mul12, %mul15
  %idxprom17 = sext i32 %add16 to i64
  %arrayidx18 = getelementptr inbounds float* %nbfp, i64 %idxprom17
  %4 = load float* %arrayidx18, align 4, !tbaa !3
  %add191073 = or i32 %add16, 1
  %idxprom20 = sext i32 %add191073 to i64
  %arrayidx21 = getelementptr inbounds float* %nbfp, i64 %idxprom20
  %5 = load float* %arrayidx21, align 4, !tbaa !3
  %cmp1104 = icmp sgt i32 %nri, 0
  br i1 %cmp1104, label %for.body, label %for.end593

for.body:                                         ; preds = %entry, %for.end.for.body_crit_edge
  %6 = phi i32 [ %.pre, %for.end.for.body_crit_edge ], [ %0, %entry ]
  %indvars.iv1106 = phi i64 [ %indvars.iv.next1107, %for.end.for.body_crit_edge ], [ 0, %entry ]
  %arrayidx23 = getelementptr inbounds i32* %shift, i64 %indvars.iv1106
  %7 = load i32* %arrayidx23, align 4, !tbaa !0
  %mul24 = mul nsw i32 %7, 3
  %idxprom25 = sext i32 %mul24 to i64
  %arrayidx26 = getelementptr inbounds float* %shiftvec, i64 %idxprom25
  %8 = load float* %arrayidx26, align 4, !tbaa !3
  %add27 = add nsw i32 %mul24, 1
  %idxprom28 = sext i32 %add27 to i64
  %arrayidx29 = getelementptr inbounds float* %shiftvec, i64 %idxprom28
  %9 = load float* %arrayidx29, align 4, !tbaa !3
  %add30 = add nsw i32 %mul24, 2
  %idxprom31 = sext i32 %add30 to i64
  %arrayidx32 = getelementptr inbounds float* %shiftvec, i64 %idxprom31
  %10 = load float* %arrayidx32, align 4, !tbaa !3
  %mul35 = mul nsw i32 %6, 3
  %arrayidx37 = getelementptr inbounds i32* %jindex, i64 %indvars.iv1106
  %11 = load i32* %arrayidx37, align 4, !tbaa !0
  %indvars.iv.next1107 = add i64 %indvars.iv1106, 1
  %arrayidx40 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next1107
  %12 = load i32* %arrayidx40, align 4, !tbaa !0
  %idxprom41 = sext i32 %mul35 to i64
  %arrayidx42 = getelementptr inbounds float* %pos, i64 %idxprom41
  %13 = load float* %arrayidx42, align 4, !tbaa !3
  %add43 = fadd float %8, %13
  %add44 = add nsw i32 %mul35, 1
  %idxprom45 = sext i32 %add44 to i64
  %arrayidx46 = getelementptr inbounds float* %pos, i64 %idxprom45
  %14 = load float* %arrayidx46, align 4, !tbaa !3
  %add47 = fadd float %9, %14
  %add48 = add nsw i32 %mul35, 2
  %idxprom49 = sext i32 %add48 to i64
  %arrayidx50 = getelementptr inbounds float* %pos, i64 %idxprom49
  %15 = load float* %arrayidx50, align 4, !tbaa !3
  %add51 = fadd float %10, %15
  %add52 = add nsw i32 %mul35, 3
  %idxprom53 = sext i32 %add52 to i64
  %arrayidx54 = getelementptr inbounds float* %pos, i64 %idxprom53
  %16 = load float* %arrayidx54, align 4, !tbaa !3
  %add55 = fadd float %8, %16
  %add56 = add nsw i32 %mul35, 4
  %idxprom57 = sext i32 %add56 to i64
  %arrayidx58 = getelementptr inbounds float* %pos, i64 %idxprom57
  %17 = load float* %arrayidx58, align 4, !tbaa !3
  %add59 = fadd float %9, %17
  %add60 = add nsw i32 %mul35, 5
  %idxprom61 = sext i32 %add60 to i64
  %arrayidx62 = getelementptr inbounds float* %pos, i64 %idxprom61
  %18 = load float* %arrayidx62, align 4, !tbaa !3
  %add63 = fadd float %10, %18
  %add64 = add nsw i32 %mul35, 6
  %idxprom65 = sext i32 %add64 to i64
  %arrayidx66 = getelementptr inbounds float* %pos, i64 %idxprom65
  %19 = load float* %arrayidx66, align 4, !tbaa !3
  %add67 = fadd float %8, %19
  %add68 = add nsw i32 %mul35, 7
  %idxprom69 = sext i32 %add68 to i64
  %arrayidx70 = getelementptr inbounds float* %pos, i64 %idxprom69
  %20 = load float* %arrayidx70, align 4, !tbaa !3
  %add71 = fadd float %9, %20
  %add72 = add nsw i32 %mul35, 8
  %idxprom73 = sext i32 %add72 to i64
  %arrayidx74 = getelementptr inbounds float* %pos, i64 %idxprom73
  %21 = load float* %arrayidx74, align 4, !tbaa !3
  %add75 = fadd float %10, %21
  %cmp771081 = icmp slt i32 %11, %12
  br i1 %cmp771081, label %for.body78.lr.ph, label %for.end

for.body78.lr.ph:                                 ; preds = %for.body
  %22 = sext i32 %11 to i64
  br label %for.body78

for.body78:                                       ; preds = %for.body78.lr.ph, %for.body78
  %indvars.iv = phi i64 [ %22, %for.body78.lr.ph ], [ %indvars.iv.next, %for.body78 ]
  %vctot.01092 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add474, %for.body78 ]
  %vnbtot.01091 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add266, %for.body78 ]
  %fix1.01090 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add338, %for.body78 ]
  %fiy1.01089 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add339, %for.body78 ]
  %fiz1.01088 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add340, %for.body78 ]
  %fix2.01087 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add404, %for.body78 ]
  %fiy2.01086 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add405, %for.body78 ]
  %fiz2.01085 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add406, %for.body78 ]
  %fix3.01084 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add478, %for.body78 ]
  %fiy3.01083 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add479, %for.body78 ]
  %fiz3.01082 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add480, %for.body78 ]
  %arrayidx80 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %23 = load i32* %arrayidx80, align 4, !tbaa !0
  %mul81 = mul nsw i32 %23, 3
  %idxprom82 = sext i32 %mul81 to i64
  %arrayidx83 = getelementptr inbounds float* %pos, i64 %idxprom82
  %24 = load float* %arrayidx83, align 4, !tbaa !3
  %add84 = add nsw i32 %mul81, 1
  %idxprom85 = sext i32 %add84 to i64
  %arrayidx86 = getelementptr inbounds float* %pos, i64 %idxprom85
  %25 = load float* %arrayidx86, align 4, !tbaa !3
  %add87 = add nsw i32 %mul81, 2
  %idxprom88 = sext i32 %add87 to i64
  %arrayidx89 = getelementptr inbounds float* %pos, i64 %idxprom88
  %26 = load float* %arrayidx89, align 4, !tbaa !3
  %add90 = add nsw i32 %mul81, 3
  %idxprom91 = sext i32 %add90 to i64
  %arrayidx92 = getelementptr inbounds float* %pos, i64 %idxprom91
  %27 = load float* %arrayidx92, align 4, !tbaa !3
  %add93 = add nsw i32 %mul81, 4
  %idxprom94 = sext i32 %add93 to i64
  %arrayidx95 = getelementptr inbounds float* %pos, i64 %idxprom94
  %28 = load float* %arrayidx95, align 4, !tbaa !3
  %add96 = add nsw i32 %mul81, 5
  %idxprom97 = sext i32 %add96 to i64
  %arrayidx98 = getelementptr inbounds float* %pos, i64 %idxprom97
  %29 = load float* %arrayidx98, align 4, !tbaa !3
  %add99 = add nsw i32 %mul81, 6
  %idxprom100 = sext i32 %add99 to i64
  %arrayidx101 = getelementptr inbounds float* %pos, i64 %idxprom100
  %30 = load float* %arrayidx101, align 4, !tbaa !3
  %add102 = add nsw i32 %mul81, 7
  %idxprom103 = sext i32 %add102 to i64
  %arrayidx104 = getelementptr inbounds float* %pos, i64 %idxprom103
  %31 = load float* %arrayidx104, align 4, !tbaa !3
  %add105 = add nsw i32 %mul81, 8
  %idxprom106 = sext i32 %add105 to i64
  %arrayidx107 = getelementptr inbounds float* %pos, i64 %idxprom106
  %32 = load float* %arrayidx107, align 4, !tbaa !3
  %sub = fsub float %add43, %24
  %sub108 = fsub float %add47, %25
  %sub109 = fsub float %add51, %26
  %mul110 = fmul float %sub, %sub
  %mul111 = fmul float %sub108, %sub108
  %add112 = fadd float %mul110, %mul111
  %mul113 = fmul float %sub109, %sub109
  %add114 = fadd float %add112, %mul113
  %sub115 = fsub float %add43, %27
  %sub116 = fsub float %add47, %28
  %sub117 = fsub float %add51, %29
  %mul118 = fmul float %sub115, %sub115
  %mul119 = fmul float %sub116, %sub116
  %add120 = fadd float %mul118, %mul119
  %mul121 = fmul float %sub117, %sub117
  %add122 = fadd float %add120, %mul121
  %sub123 = fsub float %add43, %30
  %sub124 = fsub float %add47, %31
  %sub125 = fsub float %add51, %32
  %mul126 = fmul float %sub123, %sub123
  %mul127 = fmul float %sub124, %sub124
  %add128 = fadd float %mul126, %mul127
  %mul129 = fmul float %sub125, %sub125
  %add130 = fadd float %add128, %mul129
  %sub131 = fsub float %add55, %24
  %sub132 = fsub float %add59, %25
  %sub133 = fsub float %add63, %26
  %mul134 = fmul float %sub131, %sub131
  %mul135 = fmul float %sub132, %sub132
  %add136 = fadd float %mul134, %mul135
  %mul137 = fmul float %sub133, %sub133
  %add138 = fadd float %add136, %mul137
  %sub139 = fsub float %add55, %27
  %sub140 = fsub float %add59, %28
  %sub141 = fsub float %add63, %29
  %mul142 = fmul float %sub139, %sub139
  %mul143 = fmul float %sub140, %sub140
  %add144 = fadd float %mul142, %mul143
  %mul145 = fmul float %sub141, %sub141
  %add146 = fadd float %add144, %mul145
  %sub147 = fsub float %add55, %30
  %sub148 = fsub float %add59, %31
  %sub149 = fsub float %add63, %32
  %mul150 = fmul float %sub147, %sub147
  %mul151 = fmul float %sub148, %sub148
  %add152 = fadd float %mul150, %mul151
  %mul153 = fmul float %sub149, %sub149
  %add154 = fadd float %add152, %mul153
  %sub155 = fsub float %add67, %24
  %sub156 = fsub float %add71, %25
  %sub157 = fsub float %add75, %26
  %mul158 = fmul float %sub155, %sub155
  %mul159 = fmul float %sub156, %sub156
  %add160 = fadd float %mul158, %mul159
  %mul161 = fmul float %sub157, %sub157
  %add162 = fadd float %add160, %mul161
  %sub163 = fsub float %add67, %27
  %sub164 = fsub float %add71, %28
  %sub165 = fsub float %add75, %29
  %mul166 = fmul float %sub163, %sub163
  %mul167 = fmul float %sub164, %sub164
  %add168 = fadd float %mul166, %mul167
  %mul169 = fmul float %sub165, %sub165
  %add170 = fadd float %add168, %mul169
  %sub171 = fsub float %add67, %30
  %sub172 = fsub float %add71, %31
  %sub173 = fsub float %add75, %32
  %mul174 = fmul float %sub171, %sub171
  %mul175 = fmul float %sub172, %sub172
  %add176 = fadd float %mul174, %mul175
  %mul177 = fmul float %sub173, %sub173
  %add178 = fadd float %add176, %mul177
  %conv = fpext float %add114 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv179 = fptrunc double %div to float
  %conv180 = fpext float %add138 to double
  %call181 = tail call double @sqrt(double %conv180) #2
  %div182 = fdiv double 1.000000e+00, %call181
  %conv183 = fptrunc double %div182 to float
  %conv184 = fpext float %add162 to double
  %call185 = tail call double @sqrt(double %conv184) #2
  %div186 = fdiv double 1.000000e+00, %call185
  %conv187 = fptrunc double %div186 to float
  %conv188 = fpext float %add122 to double
  %call189 = tail call double @sqrt(double %conv188) #2
  %div190 = fdiv double 1.000000e+00, %call189
  %conv191 = fptrunc double %div190 to float
  %conv192 = fpext float %add146 to double
  %call193 = tail call double @sqrt(double %conv192) #2
  %div194 = fdiv double 1.000000e+00, %call193
  %conv195 = fptrunc double %div194 to float
  %conv196 = fpext float %add170 to double
  %call197 = tail call double @sqrt(double %conv196) #2
  %div198 = fdiv double 1.000000e+00, %call197
  %conv199 = fptrunc double %div198 to float
  %conv200 = fpext float %add130 to double
  %call201 = tail call double @sqrt(double %conv200) #2
  %div202 = fdiv double 1.000000e+00, %call201
  %conv203 = fptrunc double %div202 to float
  %conv204 = fpext float %add154 to double
  %call205 = tail call double @sqrt(double %conv204) #2
  %div206 = fdiv double 1.000000e+00, %call205
  %conv207 = fptrunc double %div206 to float
  %conv208 = fpext float %add178 to double
  %call209 = tail call double @sqrt(double %conv208) #2
  %div210 = fdiv double 1.000000e+00, %call209
  %conv211 = fptrunc double %div210 to float
  %mul212 = fmul float %add114, %conv179
  %mul214 = fmul float %mul212, %tabscale
  %conv215 = fptosi float %mul214 to i32
  %conv216 = sitofp i32 %conv215 to float
  %sub217 = fsub float %mul214, %conv216
  %mul218 = fmul float %sub217, %sub217
  %mul219 = shl nsw i32 %conv215, 3
  %idxprom220 = sext i32 %mul219 to i64
  %arrayidx221 = getelementptr inbounds float* %VFtab, i64 %idxprom220
  %33 = load float* %arrayidx221, align 4, !tbaa !3
  %add2221074 = or i32 %mul219, 1
  %idxprom223 = sext i32 %add2221074 to i64
  %arrayidx224 = getelementptr inbounds float* %VFtab, i64 %idxprom223
  %34 = load float* %arrayidx224, align 4, !tbaa !3
  %add2251075 = or i32 %mul219, 2
  %idxprom226 = sext i32 %add2251075 to i64
  %arrayidx227 = getelementptr inbounds float* %VFtab, i64 %idxprom226
  %35 = load float* %arrayidx227, align 4, !tbaa !3
  %mul228 = fmul float %sub217, %35
  %add2291076 = or i32 %mul219, 3
  %idxprom230 = sext i32 %add2291076 to i64
  %arrayidx231 = getelementptr inbounds float* %VFtab, i64 %idxprom230
  %36 = load float* %arrayidx231, align 4, !tbaa !3
  %mul232 = fmul float %mul218, %36
  %add233 = fadd float %34, %mul228
  %add234 = fadd float %add233, %mul232
  %mul235 = fmul float %sub217, %add234
  %add236 = fadd float %33, %mul235
  %add237 = fadd float %mul228, %add234
  %mul238 = fmul float %mul232, 2.000000e+00
  %add239 = fadd float %mul238, %add237
  %mul240 = fmul float %4, %add236
  %mul241 = fmul float %4, %add239
  %add2421077 = or i32 %mul219, 4
  %idxprom243 = sext i32 %add2421077 to i64
  %arrayidx244 = getelementptr inbounds float* %VFtab, i64 %idxprom243
  %37 = load float* %arrayidx244, align 4, !tbaa !3
  %add2451078 = or i32 %mul219, 5
  %idxprom246 = sext i32 %add2451078 to i64
  %arrayidx247 = getelementptr inbounds float* %VFtab, i64 %idxprom246
  %38 = load float* %arrayidx247, align 4, !tbaa !3
  %add2481079 = or i32 %mul219, 6
  %idxprom249 = sext i32 %add2481079 to i64
  %arrayidx250 = getelementptr inbounds float* %VFtab, i64 %idxprom249
  %39 = load float* %arrayidx250, align 4, !tbaa !3
  %mul251 = fmul float %sub217, %39
  %add2521080 = or i32 %mul219, 7
  %idxprom253 = sext i32 %add2521080 to i64
  %arrayidx254 = getelementptr inbounds float* %VFtab, i64 %idxprom253
  %40 = load float* %arrayidx254, align 4, !tbaa !3
  %mul255 = fmul float %mul218, %40
  %add256 = fadd float %38, %mul251
  %add257 = fadd float %add256, %mul255
  %mul258 = fmul float %sub217, %add257
  %add259 = fadd float %37, %mul258
  %add260 = fadd float %mul251, %add257
  %mul261 = fmul float %mul255, 2.000000e+00
  %add262 = fadd float %mul261, %add260
  %mul263 = fmul float %5, %add259
  %mul264 = fmul float %5, %add262
  %add265 = fadd float %vnbtot.01091, %mul240
  %add266 = fadd float %add265, %mul263
  %mul267 = fmul float %add114, %krf
  %add268 = fadd float %mul267, %conv179
  %sub269 = fsub float %add268, %crf
  %mul270 = fmul float %mul4, %sub269
  %mul271 = fmul float %mul267, 2.000000e+00
  %sub272 = fsub float %conv179, %mul271
  %mul273 = fmul float %mul4, %sub272
  %mul274 = fmul float %conv179, %mul273
  %add275 = fadd float %mul241, %mul264
  %mul276 = fmul float %add275, %tabscale
  %sub277 = fsub float %mul274, %mul276
  %mul278 = fmul float %conv179, %sub277
  %add279 = fadd float %vctot.01092, %mul270
  %mul280 = fmul float %sub, %mul278
  %mul281 = fmul float %sub108, %mul278
  %mul282 = fmul float %sub109, %mul278
  %add283 = fadd float %fix1.01090, %mul280
  %add284 = fadd float %fiy1.01089, %mul281
  %add285 = fadd float %fiz1.01088, %mul282
  %arrayidx287 = getelementptr inbounds float* %faction, i64 %idxprom82
  %41 = load float* %arrayidx287, align 4, !tbaa !3
  %sub288 = fsub float %41, %mul280
  %arrayidx291 = getelementptr inbounds float* %faction, i64 %idxprom85
  %42 = load float* %arrayidx291, align 4, !tbaa !3
  %sub292 = fsub float %42, %mul281
  %arrayidx295 = getelementptr inbounds float* %faction, i64 %idxprom88
  %43 = load float* %arrayidx295, align 4, !tbaa !3
  %sub296 = fsub float %43, %mul282
  %mul297 = fmul float %conv191, %conv191
  %mul298 = fmul float %add122, %krf
  %add299 = fadd float %mul298, %conv191
  %sub300 = fsub float %add299, %crf
  %mul301 = fmul float %mul6, %sub300
  %mul302 = fmul float %mul298, 2.000000e+00
  %sub303 = fsub float %conv191, %mul302
  %mul304 = fmul float %mul6, %sub303
  %mul305 = fmul float %mul297, %mul304
  %add306 = fadd float %add279, %mul301
  %mul307 = fmul float %sub115, %mul305
  %mul308 = fmul float %sub116, %mul305
  %mul309 = fmul float %sub117, %mul305
  %add310 = fadd float %mul307, %add283
  %add311 = fadd float %mul308, %add284
  %add312 = fadd float %mul309, %add285
  %arrayidx315 = getelementptr inbounds float* %faction, i64 %idxprom91
  %44 = load float* %arrayidx315, align 4, !tbaa !3
  %sub316 = fsub float %44, %mul307
  %arrayidx319 = getelementptr inbounds float* %faction, i64 %idxprom94
  %45 = load float* %arrayidx319, align 4, !tbaa !3
  %sub320 = fsub float %45, %mul308
  %arrayidx323 = getelementptr inbounds float* %faction, i64 %idxprom97
  %46 = load float* %arrayidx323, align 4, !tbaa !3
  %sub324 = fsub float %46, %mul309
  %mul325 = fmul float %conv203, %conv203
  %mul326 = fmul float %add130, %krf
  %add327 = fadd float %mul326, %conv203
  %sub328 = fsub float %add327, %crf
  %mul329 = fmul float %mul6, %sub328
  %mul330 = fmul float %mul326, 2.000000e+00
  %sub331 = fsub float %conv203, %mul330
  %mul332 = fmul float %mul6, %sub331
  %mul333 = fmul float %mul325, %mul332
  %add334 = fadd float %add306, %mul329
  %mul335 = fmul float %sub123, %mul333
  %mul336 = fmul float %sub124, %mul333
  %mul337 = fmul float %sub125, %mul333
  %add338 = fadd float %mul335, %add310
  %add339 = fadd float %mul336, %add311
  %add340 = fadd float %mul337, %add312
  %arrayidx343 = getelementptr inbounds float* %faction, i64 %idxprom100
  %47 = load float* %arrayidx343, align 4, !tbaa !3
  %sub344 = fsub float %47, %mul335
  %arrayidx347 = getelementptr inbounds float* %faction, i64 %idxprom103
  %48 = load float* %arrayidx347, align 4, !tbaa !3
  %sub348 = fsub float %48, %mul336
  %arrayidx351 = getelementptr inbounds float* %faction, i64 %idxprom106
  %49 = load float* %arrayidx351, align 4, !tbaa !3
  %sub352 = fsub float %49, %mul337
  %mul353 = fmul float %conv183, %conv183
  %mul354 = fmul float %add138, %krf
  %add355 = fadd float %mul354, %conv183
  %sub356 = fsub float %add355, %crf
  %mul357 = fmul float %mul6, %sub356
  %mul358 = fmul float %mul354, 2.000000e+00
  %sub359 = fsub float %conv183, %mul358
  %mul360 = fmul float %mul6, %sub359
  %mul361 = fmul float %mul353, %mul360
  %add362 = fadd float %mul357, %add334
  %mul363 = fmul float %sub131, %mul361
  %mul364 = fmul float %sub132, %mul361
  %mul365 = fmul float %sub133, %mul361
  %add366 = fadd float %fix2.01087, %mul363
  %add367 = fadd float %fiy2.01086, %mul364
  %add368 = fadd float %fiz2.01085, %mul365
  %sub369 = fsub float %sub288, %mul363
  %sub370 = fsub float %sub292, %mul364
  %sub371 = fsub float %sub296, %mul365
  %mul372 = fmul float %conv195, %conv195
  %mul373 = fmul float %add146, %krf
  %add374 = fadd float %mul373, %conv195
  %sub375 = fsub float %add374, %crf
  %mul376 = fmul float %mul8, %sub375
  %mul377 = fmul float %mul373, 2.000000e+00
  %sub378 = fsub float %conv195, %mul377
  %mul379 = fmul float %mul8, %sub378
  %mul380 = fmul float %mul372, %mul379
  %add381 = fadd float %mul376, %add362
  %mul382 = fmul float %sub139, %mul380
  %mul383 = fmul float %sub140, %mul380
  %mul384 = fmul float %sub141, %mul380
  %add385 = fadd float %add366, %mul382
  %add386 = fadd float %add367, %mul383
  %add387 = fadd float %add368, %mul384
  %sub388 = fsub float %sub316, %mul382
  %sub389 = fsub float %sub320, %mul383
  %sub390 = fsub float %sub324, %mul384
  %mul391 = fmul float %conv207, %conv207
  %mul392 = fmul float %add154, %krf
  %add393 = fadd float %mul392, %conv207
  %sub394 = fsub float %add393, %crf
  %mul395 = fmul float %mul8, %sub394
  %mul396 = fmul float %mul392, 2.000000e+00
  %sub397 = fsub float %conv207, %mul396
  %mul398 = fmul float %mul8, %sub397
  %mul399 = fmul float %mul391, %mul398
  %add400 = fadd float %mul395, %add381
  %mul401 = fmul float %sub147, %mul399
  %mul402 = fmul float %sub148, %mul399
  %mul403 = fmul float %sub149, %mul399
  %add404 = fadd float %add385, %mul401
  %add405 = fadd float %add386, %mul402
  %add406 = fadd float %add387, %mul403
  %sub407 = fsub float %sub344, %mul401
  %sub408 = fsub float %sub348, %mul402
  %sub409 = fsub float %sub352, %mul403
  %mul410 = fmul float %conv187, %conv187
  %mul411 = fmul float %add162, %krf
  %add412 = fadd float %mul411, %conv187
  %sub413 = fsub float %add412, %crf
  %mul414 = fmul float %mul6, %sub413
  %mul415 = fmul float %mul411, 2.000000e+00
  %sub416 = fsub float %conv187, %mul415
  %mul417 = fmul float %mul6, %sub416
  %mul418 = fmul float %mul410, %mul417
  %add419 = fadd float %mul414, %add400
  %mul420 = fmul float %sub155, %mul418
  %mul421 = fmul float %sub156, %mul418
  %mul422 = fmul float %sub157, %mul418
  %add423 = fadd float %fix3.01084, %mul420
  %add424 = fadd float %fiy3.01083, %mul421
  %add425 = fadd float %fiz3.01082, %mul422
  %sub426 = fsub float %sub369, %mul420
  store float %sub426, float* %arrayidx287, align 4, !tbaa !3
  %sub429 = fsub float %sub370, %mul421
  store float %sub429, float* %arrayidx291, align 4, !tbaa !3
  %sub433 = fsub float %sub371, %mul422
  store float %sub433, float* %arrayidx295, align 4, !tbaa !3
  %mul437 = fmul float %conv199, %conv199
  %mul438 = fmul float %add170, %krf
  %add439 = fadd float %mul438, %conv199
  %sub440 = fsub float %add439, %crf
  %mul441 = fmul float %mul8, %sub440
  %mul442 = fmul float %mul438, 2.000000e+00
  %sub443 = fsub float %conv199, %mul442
  %mul444 = fmul float %mul8, %sub443
  %mul445 = fmul float %mul437, %mul444
  %add446 = fadd float %mul441, %add419
  %mul447 = fmul float %sub163, %mul445
  %mul448 = fmul float %sub164, %mul445
  %mul449 = fmul float %sub165, %mul445
  %add450 = fadd float %add423, %mul447
  %add451 = fadd float %add424, %mul448
  %add452 = fadd float %add425, %mul449
  %sub453 = fsub float %sub388, %mul447
  store float %sub453, float* %arrayidx315, align 4, !tbaa !3
  %sub457 = fsub float %sub389, %mul448
  store float %sub457, float* %arrayidx319, align 4, !tbaa !3
  %sub461 = fsub float %sub390, %mul449
  store float %sub461, float* %arrayidx323, align 4, !tbaa !3
  %mul465 = fmul float %conv211, %conv211
  %mul466 = fmul float %add178, %krf
  %add467 = fadd float %mul466, %conv211
  %sub468 = fsub float %add467, %crf
  %mul469 = fmul float %mul8, %sub468
  %mul470 = fmul float %mul466, 2.000000e+00
  %sub471 = fsub float %conv211, %mul470
  %mul472 = fmul float %mul8, %sub471
  %mul473 = fmul float %mul465, %mul472
  %add474 = fadd float %mul469, %add446
  %mul475 = fmul float %sub171, %mul473
  %mul476 = fmul float %sub172, %mul473
  %mul477 = fmul float %sub173, %mul473
  %add478 = fadd float %add450, %mul475
  %add479 = fadd float %add451, %mul476
  %add480 = fadd float %add452, %mul477
  %sub481 = fsub float %sub407, %mul475
  store float %sub481, float* %arrayidx343, align 4, !tbaa !3
  %sub485 = fsub float %sub408, %mul476
  store float %sub485, float* %arrayidx347, align 4, !tbaa !3
  %sub489 = fsub float %sub409, %mul477
  store float %sub489, float* %arrayidx351, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %50 = trunc i64 %indvars.iv.next to i32
  %cmp77 = icmp slt i32 %50, %12
  br i1 %cmp77, label %for.body78, label %for.end

for.end:                                          ; preds = %for.body78, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add474, %for.body78 ]
  %vnbtot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add266, %for.body78 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add338, %for.body78 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add339, %for.body78 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add340, %for.body78 ]
  %fix2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add404, %for.body78 ]
  %fiy2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add405, %for.body78 ]
  %fiz2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add406, %for.body78 ]
  %fix3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add478, %for.body78 ]
  %fiy3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add479, %for.body78 ]
  %fiz3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add480, %for.body78 ]
  %arrayidx494 = getelementptr inbounds float* %faction, i64 %idxprom41
  %51 = load float* %arrayidx494, align 4, !tbaa !3
  %add495 = fadd float %fix1.0.lcssa, %51
  store float %add495, float* %arrayidx494, align 4, !tbaa !3
  %arrayidx500 = getelementptr inbounds float* %faction, i64 %idxprom45
  %52 = load float* %arrayidx500, align 4, !tbaa !3
  %add501 = fadd float %fiy1.0.lcssa, %52
  store float %add501, float* %arrayidx500, align 4, !tbaa !3
  %arrayidx507 = getelementptr inbounds float* %faction, i64 %idxprom49
  %53 = load float* %arrayidx507, align 4, !tbaa !3
  %add508 = fadd float %fiz1.0.lcssa, %53
  store float %add508, float* %arrayidx507, align 4, !tbaa !3
  %arrayidx514 = getelementptr inbounds float* %faction, i64 %idxprom53
  %54 = load float* %arrayidx514, align 4, !tbaa !3
  %add515 = fadd float %fix2.0.lcssa, %54
  store float %add515, float* %arrayidx514, align 4, !tbaa !3
  %arrayidx521 = getelementptr inbounds float* %faction, i64 %idxprom57
  %55 = load float* %arrayidx521, align 4, !tbaa !3
  %add522 = fadd float %fiy2.0.lcssa, %55
  store float %add522, float* %arrayidx521, align 4, !tbaa !3
  %arrayidx528 = getelementptr inbounds float* %faction, i64 %idxprom61
  %56 = load float* %arrayidx528, align 4, !tbaa !3
  %add529 = fadd float %fiz2.0.lcssa, %56
  store float %add529, float* %arrayidx528, align 4, !tbaa !3
  %arrayidx535 = getelementptr inbounds float* %faction, i64 %idxprom65
  %57 = load float* %arrayidx535, align 4, !tbaa !3
  %add536 = fadd float %fix3.0.lcssa, %57
  store float %add536, float* %arrayidx535, align 4, !tbaa !3
  %arrayidx542 = getelementptr inbounds float* %faction, i64 %idxprom69
  %58 = load float* %arrayidx542, align 4, !tbaa !3
  %add543 = fadd float %fiy3.0.lcssa, %58
  store float %add543, float* %arrayidx542, align 4, !tbaa !3
  %arrayidx549 = getelementptr inbounds float* %faction, i64 %idxprom73
  %59 = load float* %arrayidx549, align 4, !tbaa !3
  %add550 = fadd float %fiz3.0.lcssa, %59
  store float %add550, float* %arrayidx549, align 4, !tbaa !3
  %arrayidx555 = getelementptr inbounds float* %fshift, i64 %idxprom25
  %60 = load float* %arrayidx555, align 4, !tbaa !3
  %add556 = fadd float %fix1.0.lcssa, %60
  %add557 = fadd float %fix2.0.lcssa, %add556
  %add558 = fadd float %fix3.0.lcssa, %add557
  store float %add558, float* %arrayidx555, align 4, !tbaa !3
  %arrayidx563 = getelementptr inbounds float* %fshift, i64 %idxprom28
  %61 = load float* %arrayidx563, align 4, !tbaa !3
  %add564 = fadd float %fiy1.0.lcssa, %61
  %add565 = fadd float %fiy2.0.lcssa, %add564
  %add566 = fadd float %fiy3.0.lcssa, %add565
  store float %add566, float* %arrayidx563, align 4, !tbaa !3
  %arrayidx572 = getelementptr inbounds float* %fshift, i64 %idxprom31
  %62 = load float* %arrayidx572, align 4, !tbaa !3
  %add573 = fadd float %fiz1.0.lcssa, %62
  %add574 = fadd float %fiz2.0.lcssa, %add573
  %add575 = fadd float %fiz3.0.lcssa, %add574
  store float %add575, float* %arrayidx572, align 4, !tbaa !3
  %arrayidx580 = getelementptr inbounds i32* %gid, i64 %indvars.iv1106
  %63 = load i32* %arrayidx580, align 4, !tbaa !0
  %idxprom581 = sext i32 %63 to i64
  %arrayidx582 = getelementptr inbounds float* %Vc, i64 %idxprom581
  %64 = load float* %arrayidx582, align 4, !tbaa !3
  %add583 = fadd float %vctot.0.lcssa, %64
  store float %add583, float* %arrayidx582, align 4, !tbaa !3
  %arrayidx587 = getelementptr inbounds float* %Vnb, i64 %idxprom581
  %65 = load float* %arrayidx587, align 4, !tbaa !3
  %add588 = fadd float %vnbtot.0.lcssa, %65
  store float %add588, float* %arrayidx587, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next1107 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end593, label %for.end.for.body_crit_edge

for.end.for.body_crit_edge:                       ; preds = %for.end
  %arrayidx34.phi.trans.insert = getelementptr inbounds i32* %iinr, i64 %indvars.iv.next1107
  %.pre = load i32* %arrayidx34.phi.trans.insert, align 4, !tbaa !0
  br label %for.body

for.end593:                                       ; preds = %for.end, %entry
  ret void
}
