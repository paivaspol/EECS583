define void @inl2430(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc, float %krf, float %crf, i32* nocapture %type, i32 %ntype, float* nocapture %nbfp, float* nocapture %Vnb, float %tabscale, float* nocapture %VFtab, float %exptabscale) #0 {
entry:
  %0 = load i32* %iinr, align 4, !tbaa !0
  %idxprom = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds float* %charge, i64 %idxprom
  %1 = load float* %arrayidx1, align 4, !tbaa !3
  %add = add nsw i32 %0, 1
  %idxprom2 = sext i32 %add to i64
  %arrayidx3 = getelementptr inbounds float* %charge, i64 %idxprom2
  %2 = load float* %arrayidx3, align 4, !tbaa !3
  %mul = fmul float %1, %facel
  %mul4 = fmul float %1, %mul
  %mul6 = fmul float %mul, %2
  %mul7 = fmul float %2, %facel
  %mul8 = fmul float %2, %mul7
  %mul9 = mul nsw i32 %ntype, 3
  %arrayidx11 = getelementptr inbounds i32* %type, i64 %idxprom
  %3 = load i32* %arrayidx11, align 4, !tbaa !0
  %mul121096 = add i32 %mul9, 3
  %add16 = mul i32 %3, %mul121096
  %idxprom17 = sext i32 %add16 to i64
  %arrayidx18 = getelementptr inbounds float* %nbfp, i64 %idxprom17
  %4 = load float* %arrayidx18, align 4, !tbaa !3
  %add19 = add nsw i32 %add16, 1
  %idxprom20 = sext i32 %add19 to i64
  %arrayidx21 = getelementptr inbounds float* %nbfp, i64 %idxprom20
  %5 = load float* %arrayidx21, align 4, !tbaa !3
  %add22 = add nsw i32 %add16, 2
  %idxprom23 = sext i32 %add22 to i64
  %arrayidx24 = getelementptr inbounds float* %nbfp, i64 %idxprom23
  %6 = load float* %arrayidx24, align 4, !tbaa !3
  %cmp1127 = icmp sgt i32 %nri, 0
  br i1 %cmp1127, label %for.body.lr.ph, label %for.end605

for.body.lr.ph:                                   ; preds = %entry
  %mul274 = fmul float %5, %6
  br label %for.body

for.body:                                         ; preds = %for.end.for.body_crit_edge, %for.body.lr.ph
  %7 = phi i32 [ %0, %for.body.lr.ph ], [ %.pre, %for.end.for.body_crit_edge ]
  %indvars.iv1129 = phi i64 [ 0, %for.body.lr.ph ], [ %indvars.iv.next1130, %for.end.for.body_crit_edge ]
  %arrayidx26 = getelementptr inbounds i32* %shift, i64 %indvars.iv1129
  %8 = load i32* %arrayidx26, align 4, !tbaa !0
  %mul27 = mul nsw i32 %8, 3
  %idxprom28 = sext i32 %mul27 to i64
  %arrayidx29 = getelementptr inbounds float* %shiftvec, i64 %idxprom28
  %9 = load float* %arrayidx29, align 4, !tbaa !3
  %add30 = add nsw i32 %mul27, 1
  %idxprom31 = sext i32 %add30 to i64
  %arrayidx32 = getelementptr inbounds float* %shiftvec, i64 %idxprom31
  %10 = load float* %arrayidx32, align 4, !tbaa !3
  %add33 = add nsw i32 %mul27, 2
  %idxprom34 = sext i32 %add33 to i64
  %arrayidx35 = getelementptr inbounds float* %shiftvec, i64 %idxprom34
  %11 = load float* %arrayidx35, align 4, !tbaa !3
  %mul38 = mul nsw i32 %7, 3
  %arrayidx40 = getelementptr inbounds i32* %jindex, i64 %indvars.iv1129
  %12 = load i32* %arrayidx40, align 4, !tbaa !0
  %indvars.iv.next1130 = add i64 %indvars.iv1129, 1
  %arrayidx43 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next1130
  %13 = load i32* %arrayidx43, align 4, !tbaa !0
  %idxprom44 = sext i32 %mul38 to i64
  %arrayidx45 = getelementptr inbounds float* %pos, i64 %idxprom44
  %14 = load float* %arrayidx45, align 4, !tbaa !3
  %add46 = fadd float %9, %14
  %add47 = add nsw i32 %mul38, 1
  %idxprom48 = sext i32 %add47 to i64
  %arrayidx49 = getelementptr inbounds float* %pos, i64 %idxprom48
  %15 = load float* %arrayidx49, align 4, !tbaa !3
  %add50 = fadd float %10, %15
  %add51 = add nsw i32 %mul38, 2
  %idxprom52 = sext i32 %add51 to i64
  %arrayidx53 = getelementptr inbounds float* %pos, i64 %idxprom52
  %16 = load float* %arrayidx53, align 4, !tbaa !3
  %add54 = fadd float %11, %16
  %add55 = add nsw i32 %mul38, 3
  %idxprom56 = sext i32 %add55 to i64
  %arrayidx57 = getelementptr inbounds float* %pos, i64 %idxprom56
  %17 = load float* %arrayidx57, align 4, !tbaa !3
  %add58 = fadd float %9, %17
  %add59 = add nsw i32 %mul38, 4
  %idxprom60 = sext i32 %add59 to i64
  %arrayidx61 = getelementptr inbounds float* %pos, i64 %idxprom60
  %18 = load float* %arrayidx61, align 4, !tbaa !3
  %add62 = fadd float %10, %18
  %add63 = add nsw i32 %mul38, 5
  %idxprom64 = sext i32 %add63 to i64
  %arrayidx65 = getelementptr inbounds float* %pos, i64 %idxprom64
  %19 = load float* %arrayidx65, align 4, !tbaa !3
  %add66 = fadd float %11, %19
  %add67 = add nsw i32 %mul38, 6
  %idxprom68 = sext i32 %add67 to i64
  %arrayidx69 = getelementptr inbounds float* %pos, i64 %idxprom68
  %20 = load float* %arrayidx69, align 4, !tbaa !3
  %add70 = fadd float %9, %20
  %add71 = add nsw i32 %mul38, 7
  %idxprom72 = sext i32 %add71 to i64
  %arrayidx73 = getelementptr inbounds float* %pos, i64 %idxprom72
  %21 = load float* %arrayidx73, align 4, !tbaa !3
  %add74 = fadd float %10, %21
  %add75 = add nsw i32 %mul38, 8
  %idxprom76 = sext i32 %add75 to i64
  %arrayidx77 = getelementptr inbounds float* %pos, i64 %idxprom76
  %22 = load float* %arrayidx77, align 4, !tbaa !3
  %add78 = fadd float %11, %22
  %cmp801104 = icmp slt i32 %12, %13
  br i1 %cmp801104, label %for.body81.lr.ph, label %for.end

for.body81.lr.ph:                                 ; preds = %for.body
  %23 = sext i32 %12 to i64
  br label %for.body81

for.body81:                                       ; preds = %for.body81.lr.ph, %for.body81
  %indvars.iv = phi i64 [ %23, %for.body81.lr.ph ], [ %indvars.iv.next, %for.body81 ]
  %vctot.01115 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add486, %for.body81 ]
  %vnbtot.01114 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add277, %for.body81 ]
  %fix1.01113 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add350, %for.body81 ]
  %fiy1.01112 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add351, %for.body81 ]
  %fiz1.01111 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add352, %for.body81 ]
  %fix2.01110 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add416, %for.body81 ]
  %fiy2.01109 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add417, %for.body81 ]
  %fiz2.01108 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add418, %for.body81 ]
  %fix3.01107 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add490, %for.body81 ]
  %fiy3.01106 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add491, %for.body81 ]
  %fiz3.01105 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add492, %for.body81 ]
  %arrayidx83 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %24 = load i32* %arrayidx83, align 4, !tbaa !0
  %mul84 = mul nsw i32 %24, 3
  %idxprom85 = sext i32 %mul84 to i64
  %arrayidx86 = getelementptr inbounds float* %pos, i64 %idxprom85
  %25 = load float* %arrayidx86, align 4, !tbaa !3
  %add87 = add nsw i32 %mul84, 1
  %idxprom88 = sext i32 %add87 to i64
  %arrayidx89 = getelementptr inbounds float* %pos, i64 %idxprom88
  %26 = load float* %arrayidx89, align 4, !tbaa !3
  %add90 = add nsw i32 %mul84, 2
  %idxprom91 = sext i32 %add90 to i64
  %arrayidx92 = getelementptr inbounds float* %pos, i64 %idxprom91
  %27 = load float* %arrayidx92, align 4, !tbaa !3
  %add93 = add nsw i32 %mul84, 3
  %idxprom94 = sext i32 %add93 to i64
  %arrayidx95 = getelementptr inbounds float* %pos, i64 %idxprom94
  %28 = load float* %arrayidx95, align 4, !tbaa !3
  %add96 = add nsw i32 %mul84, 4
  %idxprom97 = sext i32 %add96 to i64
  %arrayidx98 = getelementptr inbounds float* %pos, i64 %idxprom97
  %29 = load float* %arrayidx98, align 4, !tbaa !3
  %add99 = add nsw i32 %mul84, 5
  %idxprom100 = sext i32 %add99 to i64
  %arrayidx101 = getelementptr inbounds float* %pos, i64 %idxprom100
  %30 = load float* %arrayidx101, align 4, !tbaa !3
  %add102 = add nsw i32 %mul84, 6
  %idxprom103 = sext i32 %add102 to i64
  %arrayidx104 = getelementptr inbounds float* %pos, i64 %idxprom103
  %31 = load float* %arrayidx104, align 4, !tbaa !3
  %add105 = add nsw i32 %mul84, 7
  %idxprom106 = sext i32 %add105 to i64
  %arrayidx107 = getelementptr inbounds float* %pos, i64 %idxprom106
  %32 = load float* %arrayidx107, align 4, !tbaa !3
  %add108 = add nsw i32 %mul84, 8
  %idxprom109 = sext i32 %add108 to i64
  %arrayidx110 = getelementptr inbounds float* %pos, i64 %idxprom109
  %33 = load float* %arrayidx110, align 4, !tbaa !3
  %sub = fsub float %add46, %25
  %sub111 = fsub float %add50, %26
  %sub112 = fsub float %add54, %27
  %mul113 = fmul float %sub, %sub
  %mul114 = fmul float %sub111, %sub111
  %add115 = fadd float %mul113, %mul114
  %mul116 = fmul float %sub112, %sub112
  %add117 = fadd float %add115, %mul116
  %sub118 = fsub float %add46, %28
  %sub119 = fsub float %add50, %29
  %sub120 = fsub float %add54, %30
  %mul121 = fmul float %sub118, %sub118
  %mul122 = fmul float %sub119, %sub119
  %add123 = fadd float %mul121, %mul122
  %mul124 = fmul float %sub120, %sub120
  %add125 = fadd float %add123, %mul124
  %sub126 = fsub float %add46, %31
  %sub127 = fsub float %add50, %32
  %sub128 = fsub float %add54, %33
  %mul129 = fmul float %sub126, %sub126
  %mul130 = fmul float %sub127, %sub127
  %add131 = fadd float %mul129, %mul130
  %mul132 = fmul float %sub128, %sub128
  %add133 = fadd float %add131, %mul132
  %sub134 = fsub float %add58, %25
  %sub135 = fsub float %add62, %26
  %sub136 = fsub float %add66, %27
  %mul137 = fmul float %sub134, %sub134
  %mul138 = fmul float %sub135, %sub135
  %add139 = fadd float %mul137, %mul138
  %mul140 = fmul float %sub136, %sub136
  %add141 = fadd float %add139, %mul140
  %sub142 = fsub float %add58, %28
  %sub143 = fsub float %add62, %29
  %sub144 = fsub float %add66, %30
  %mul145 = fmul float %sub142, %sub142
  %mul146 = fmul float %sub143, %sub143
  %add147 = fadd float %mul145, %mul146
  %mul148 = fmul float %sub144, %sub144
  %add149 = fadd float %add147, %mul148
  %sub150 = fsub float %add58, %31
  %sub151 = fsub float %add62, %32
  %sub152 = fsub float %add66, %33
  %mul153 = fmul float %sub150, %sub150
  %mul154 = fmul float %sub151, %sub151
  %add155 = fadd float %mul153, %mul154
  %mul156 = fmul float %sub152, %sub152
  %add157 = fadd float %add155, %mul156
  %sub158 = fsub float %add70, %25
  %sub159 = fsub float %add74, %26
  %sub160 = fsub float %add78, %27
  %mul161 = fmul float %sub158, %sub158
  %mul162 = fmul float %sub159, %sub159
  %add163 = fadd float %mul161, %mul162
  %mul164 = fmul float %sub160, %sub160
  %add165 = fadd float %add163, %mul164
  %sub166 = fsub float %add70, %28
  %sub167 = fsub float %add74, %29
  %sub168 = fsub float %add78, %30
  %mul169 = fmul float %sub166, %sub166
  %mul170 = fmul float %sub167, %sub167
  %add171 = fadd float %mul169, %mul170
  %mul172 = fmul float %sub168, %sub168
  %add173 = fadd float %add171, %mul172
  %sub174 = fsub float %add70, %31
  %sub175 = fsub float %add74, %32
  %sub176 = fsub float %add78, %33
  %mul177 = fmul float %sub174, %sub174
  %mul178 = fmul float %sub175, %sub175
  %add179 = fadd float %mul177, %mul178
  %mul180 = fmul float %sub176, %sub176
  %add181 = fadd float %add179, %mul180
  %conv = fpext float %add117 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv182 = fptrunc double %div to float
  %conv183 = fpext float %add141 to double
  %call184 = tail call double @sqrt(double %conv183) #2
  %div185 = fdiv double 1.000000e+00, %call184
  %conv186 = fptrunc double %div185 to float
  %conv187 = fpext float %add165 to double
  %call188 = tail call double @sqrt(double %conv187) #2
  %div189 = fdiv double 1.000000e+00, %call188
  %conv190 = fptrunc double %div189 to float
  %conv191 = fpext float %add125 to double
  %call192 = tail call double @sqrt(double %conv191) #2
  %div193 = fdiv double 1.000000e+00, %call192
  %conv194 = fptrunc double %div193 to float
  %conv195 = fpext float %add149 to double
  %call196 = tail call double @sqrt(double %conv195) #2
  %div197 = fdiv double 1.000000e+00, %call196
  %conv198 = fptrunc double %div197 to float
  %conv199 = fpext float %add173 to double
  %call200 = tail call double @sqrt(double %conv199) #2
  %div201 = fdiv double 1.000000e+00, %call200
  %conv202 = fptrunc double %div201 to float
  %conv203 = fpext float %add133 to double
  %call204 = tail call double @sqrt(double %conv203) #2
  %div205 = fdiv double 1.000000e+00, %call204
  %conv206 = fptrunc double %div205 to float
  %conv207 = fpext float %add157 to double
  %call208 = tail call double @sqrt(double %conv207) #2
  %div209 = fdiv double 1.000000e+00, %call208
  %conv210 = fptrunc double %div209 to float
  %conv211 = fpext float %add181 to double
  %call212 = tail call double @sqrt(double %conv211) #2
  %div213 = fdiv double 1.000000e+00, %call212
  %conv214 = fptrunc double %div213 to float
  %mul215 = fmul float %add117, %conv182
  %mul217 = fmul float %mul215, %tabscale
  %conv218 = fptosi float %mul217 to i32
  %conv219 = sitofp i32 %conv218 to float
  %sub220 = fsub float %mul217, %conv219
  %mul221 = fmul float %sub220, %sub220
  %mul222 = shl nsw i32 %conv218, 3
  %idxprom223 = sext i32 %mul222 to i64
  %arrayidx224 = getelementptr inbounds float* %VFtab, i64 %idxprom223
  %34 = load float* %arrayidx224, align 4, !tbaa !3
  %add2251097 = or i32 %mul222, 1
  %idxprom226 = sext i32 %add2251097 to i64
  %arrayidx227 = getelementptr inbounds float* %VFtab, i64 %idxprom226
  %35 = load float* %arrayidx227, align 4, !tbaa !3
  %add2281098 = or i32 %mul222, 2
  %idxprom229 = sext i32 %add2281098 to i64
  %arrayidx230 = getelementptr inbounds float* %VFtab, i64 %idxprom229
  %36 = load float* %arrayidx230, align 4, !tbaa !3
  %mul231 = fmul float %sub220, %36
  %add2321099 = or i32 %mul222, 3
  %idxprom233 = sext i32 %add2321099 to i64
  %arrayidx234 = getelementptr inbounds float* %VFtab, i64 %idxprom233
  %37 = load float* %arrayidx234, align 4, !tbaa !3
  %mul235 = fmul float %mul221, %37
  %add236 = fadd float %35, %mul231
  %add237 = fadd float %add236, %mul235
  %mul238 = fmul float %sub220, %add237
  %add239 = fadd float %34, %mul238
  %add240 = fadd float %mul231, %add237
  %mul241 = fmul float %mul235, 2.000000e+00
  %add242 = fadd float %mul241, %add240
  %mul243 = fmul float %4, %add239
  %mul244 = fmul float %4, %add242
  %mul245 = fmul float %6, %mul215
  %mul246 = fmul float %mul245, %exptabscale
  %conv247 = fptosi float %mul246 to i32
  %conv248 = sitofp i32 %conv247 to float
  %sub249 = fsub float %mul246, %conv248
  %mul250 = fmul float %sub249, %sub249
  %mul251 = shl nsw i32 %conv247, 3
  %add2521100 = or i32 %mul251, 4
  %idxprom253 = sext i32 %add2521100 to i64
  %arrayidx254 = getelementptr inbounds float* %VFtab, i64 %idxprom253
  %38 = load float* %arrayidx254, align 4, !tbaa !3
  %add2551101 = or i32 %mul251, 5
  %idxprom256 = sext i32 %add2551101 to i64
  %arrayidx257 = getelementptr inbounds float* %VFtab, i64 %idxprom256
  %39 = load float* %arrayidx257, align 4, !tbaa !3
  %add2581102 = or i32 %mul251, 6
  %idxprom259 = sext i32 %add2581102 to i64
  %arrayidx260 = getelementptr inbounds float* %VFtab, i64 %idxprom259
  %40 = load float* %arrayidx260, align 4, !tbaa !3
  %mul261 = fmul float %sub249, %40
  %add2621103 = or i32 %mul251, 7
  %idxprom263 = sext i32 %add2621103 to i64
  %arrayidx264 = getelementptr inbounds float* %VFtab, i64 %idxprom263
  %41 = load float* %arrayidx264, align 4, !tbaa !3
  %mul265 = fmul float %mul250, %41
  %add266 = fadd float %39, %mul261
  %add267 = fadd float %add266, %mul265
  %mul268 = fmul float %sub249, %add267
  %add269 = fadd float %38, %mul268
  %add270 = fadd float %mul261, %add267
  %mul271 = fmul float %mul265, 2.000000e+00
  %add272 = fadd float %mul271, %add270
  %mul273 = fmul float %5, %add269
  %mul275 = fmul float %mul274, %add272
  %add276 = fadd float %vnbtot.01114, %mul243
  %add277 = fadd float %add276, %mul273
  %mul278 = fmul float %add117, %krf
  %add279 = fadd float %mul278, %conv182
  %sub280 = fsub float %add279, %crf
  %mul281 = fmul float %mul4, %sub280
  %mul282 = fmul float %mul278, 2.000000e+00
  %sub283 = fsub float %conv182, %mul282
  %mul284 = fmul float %mul4, %sub283
  %mul285 = fmul float %conv182, %mul284
  %mul286 = fmul float %mul244, %tabscale
  %mul287 = fmul float %mul275, %exptabscale
  %add288 = fadd float %mul286, %mul287
  %sub289 = fsub float %mul285, %add288
  %mul290 = fmul float %conv182, %sub289
  %add291 = fadd float %vctot.01115, %mul281
  %mul292 = fmul float %sub, %mul290
  %mul293 = fmul float %sub111, %mul290
  %mul294 = fmul float %sub112, %mul290
  %add295 = fadd float %fix1.01113, %mul292
  %add296 = fadd float %fiy1.01112, %mul293
  %add297 = fadd float %fiz1.01111, %mul294
  %arrayidx299 = getelementptr inbounds float* %faction, i64 %idxprom85
  %42 = load float* %arrayidx299, align 4, !tbaa !3
  %sub300 = fsub float %42, %mul292
  %arrayidx303 = getelementptr inbounds float* %faction, i64 %idxprom88
  %43 = load float* %arrayidx303, align 4, !tbaa !3
  %sub304 = fsub float %43, %mul293
  %arrayidx307 = getelementptr inbounds float* %faction, i64 %idxprom91
  %44 = load float* %arrayidx307, align 4, !tbaa !3
  %sub308 = fsub float %44, %mul294
  %mul309 = fmul float %conv194, %conv194
  %mul310 = fmul float %add125, %krf
  %add311 = fadd float %mul310, %conv194
  %sub312 = fsub float %add311, %crf
  %mul313 = fmul float %mul6, %sub312
  %mul314 = fmul float %mul310, 2.000000e+00
  %sub315 = fsub float %conv194, %mul314
  %mul316 = fmul float %mul6, %sub315
  %mul317 = fmul float %mul309, %mul316
  %add318 = fadd float %add291, %mul313
  %mul319 = fmul float %sub118, %mul317
  %mul320 = fmul float %sub119, %mul317
  %mul321 = fmul float %sub120, %mul317
  %add322 = fadd float %mul319, %add295
  %add323 = fadd float %mul320, %add296
  %add324 = fadd float %mul321, %add297
  %arrayidx327 = getelementptr inbounds float* %faction, i64 %idxprom94
  %45 = load float* %arrayidx327, align 4, !tbaa !3
  %sub328 = fsub float %45, %mul319
  %arrayidx331 = getelementptr inbounds float* %faction, i64 %idxprom97
  %46 = load float* %arrayidx331, align 4, !tbaa !3
  %sub332 = fsub float %46, %mul320
  %arrayidx335 = getelementptr inbounds float* %faction, i64 %idxprom100
  %47 = load float* %arrayidx335, align 4, !tbaa !3
  %sub336 = fsub float %47, %mul321
  %mul337 = fmul float %conv206, %conv206
  %mul338 = fmul float %add133, %krf
  %add339 = fadd float %mul338, %conv206
  %sub340 = fsub float %add339, %crf
  %mul341 = fmul float %mul6, %sub340
  %mul342 = fmul float %mul338, 2.000000e+00
  %sub343 = fsub float %conv206, %mul342
  %mul344 = fmul float %mul6, %sub343
  %mul345 = fmul float %mul337, %mul344
  %add346 = fadd float %add318, %mul341
  %mul347 = fmul float %sub126, %mul345
  %mul348 = fmul float %sub127, %mul345
  %mul349 = fmul float %sub128, %mul345
  %add350 = fadd float %mul347, %add322
  %add351 = fadd float %mul348, %add323
  %add352 = fadd float %mul349, %add324
  %arrayidx355 = getelementptr inbounds float* %faction, i64 %idxprom103
  %48 = load float* %arrayidx355, align 4, !tbaa !3
  %sub356 = fsub float %48, %mul347
  %arrayidx359 = getelementptr inbounds float* %faction, i64 %idxprom106
  %49 = load float* %arrayidx359, align 4, !tbaa !3
  %sub360 = fsub float %49, %mul348
  %arrayidx363 = getelementptr inbounds float* %faction, i64 %idxprom109
  %50 = load float* %arrayidx363, align 4, !tbaa !3
  %sub364 = fsub float %50, %mul349
  %mul365 = fmul float %conv186, %conv186
  %mul366 = fmul float %add141, %krf
  %add367 = fadd float %mul366, %conv186
  %sub368 = fsub float %add367, %crf
  %mul369 = fmul float %mul6, %sub368
  %mul370 = fmul float %mul366, 2.000000e+00
  %sub371 = fsub float %conv186, %mul370
  %mul372 = fmul float %mul6, %sub371
  %mul373 = fmul float %mul365, %mul372
  %add374 = fadd float %mul369, %add346
  %mul375 = fmul float %sub134, %mul373
  %mul376 = fmul float %sub135, %mul373
  %mul377 = fmul float %sub136, %mul373
  %add378 = fadd float %fix2.01110, %mul375
  %add379 = fadd float %fiy2.01109, %mul376
  %add380 = fadd float %fiz2.01108, %mul377
  %sub381 = fsub float %sub300, %mul375
  %sub382 = fsub float %sub304, %mul376
  %sub383 = fsub float %sub308, %mul377
  %mul384 = fmul float %conv198, %conv198
  %mul385 = fmul float %add149, %krf
  %add386 = fadd float %mul385, %conv198
  %sub387 = fsub float %add386, %crf
  %mul388 = fmul float %mul8, %sub387
  %mul389 = fmul float %mul385, 2.000000e+00
  %sub390 = fsub float %conv198, %mul389
  %mul391 = fmul float %mul8, %sub390
  %mul392 = fmul float %mul384, %mul391
  %add393 = fadd float %mul388, %add374
  %mul394 = fmul float %sub142, %mul392
  %mul395 = fmul float %sub143, %mul392
  %mul396 = fmul float %sub144, %mul392
  %add397 = fadd float %add378, %mul394
  %add398 = fadd float %add379, %mul395
  %add399 = fadd float %add380, %mul396
  %sub400 = fsub float %sub328, %mul394
  %sub401 = fsub float %sub332, %mul395
  %sub402 = fsub float %sub336, %mul396
  %mul403 = fmul float %conv210, %conv210
  %mul404 = fmul float %add157, %krf
  %add405 = fadd float %mul404, %conv210
  %sub406 = fsub float %add405, %crf
  %mul407 = fmul float %mul8, %sub406
  %mul408 = fmul float %mul404, 2.000000e+00
  %sub409 = fsub float %conv210, %mul408
  %mul410 = fmul float %mul8, %sub409
  %mul411 = fmul float %mul403, %mul410
  %add412 = fadd float %mul407, %add393
  %mul413 = fmul float %sub150, %mul411
  %mul414 = fmul float %sub151, %mul411
  %mul415 = fmul float %sub152, %mul411
  %add416 = fadd float %add397, %mul413
  %add417 = fadd float %add398, %mul414
  %add418 = fadd float %add399, %mul415
  %sub419 = fsub float %sub356, %mul413
  %sub420 = fsub float %sub360, %mul414
  %sub421 = fsub float %sub364, %mul415
  %mul422 = fmul float %conv190, %conv190
  %mul423 = fmul float %add165, %krf
  %add424 = fadd float %mul423, %conv190
  %sub425 = fsub float %add424, %crf
  %mul426 = fmul float %mul6, %sub425
  %mul427 = fmul float %mul423, 2.000000e+00
  %sub428 = fsub float %conv190, %mul427
  %mul429 = fmul float %mul6, %sub428
  %mul430 = fmul float %mul422, %mul429
  %add431 = fadd float %mul426, %add412
  %mul432 = fmul float %sub158, %mul430
  %mul433 = fmul float %sub159, %mul430
  %mul434 = fmul float %sub160, %mul430
  %add435 = fadd float %fix3.01107, %mul432
  %add436 = fadd float %fiy3.01106, %mul433
  %add437 = fadd float %fiz3.01105, %mul434
  %sub438 = fsub float %sub381, %mul432
  store float %sub438, float* %arrayidx299, align 4, !tbaa !3
  %sub441 = fsub float %sub382, %mul433
  store float %sub441, float* %arrayidx303, align 4, !tbaa !3
  %sub445 = fsub float %sub383, %mul434
  store float %sub445, float* %arrayidx307, align 4, !tbaa !3
  %mul449 = fmul float %conv202, %conv202
  %mul450 = fmul float %add173, %krf
  %add451 = fadd float %mul450, %conv202
  %sub452 = fsub float %add451, %crf
  %mul453 = fmul float %mul8, %sub452
  %mul454 = fmul float %mul450, 2.000000e+00
  %sub455 = fsub float %conv202, %mul454
  %mul456 = fmul float %mul8, %sub455
  %mul457 = fmul float %mul449, %mul456
  %add458 = fadd float %mul453, %add431
  %mul459 = fmul float %sub166, %mul457
  %mul460 = fmul float %sub167, %mul457
  %mul461 = fmul float %sub168, %mul457
  %add462 = fadd float %add435, %mul459
  %add463 = fadd float %add436, %mul460
  %add464 = fadd float %add437, %mul461
  %sub465 = fsub float %sub400, %mul459
  store float %sub465, float* %arrayidx327, align 4, !tbaa !3
  %sub469 = fsub float %sub401, %mul460
  store float %sub469, float* %arrayidx331, align 4, !tbaa !3
  %sub473 = fsub float %sub402, %mul461
  store float %sub473, float* %arrayidx335, align 4, !tbaa !3
  %mul477 = fmul float %conv214, %conv214
  %mul478 = fmul float %add181, %krf
  %add479 = fadd float %mul478, %conv214
  %sub480 = fsub float %add479, %crf
  %mul481 = fmul float %mul8, %sub480
  %mul482 = fmul float %mul478, 2.000000e+00
  %sub483 = fsub float %conv214, %mul482
  %mul484 = fmul float %mul8, %sub483
  %mul485 = fmul float %mul477, %mul484
  %add486 = fadd float %mul481, %add458
  %mul487 = fmul float %sub174, %mul485
  %mul488 = fmul float %sub175, %mul485
  %mul489 = fmul float %sub176, %mul485
  %add490 = fadd float %add462, %mul487
  %add491 = fadd float %add463, %mul488
  %add492 = fadd float %add464, %mul489
  %sub493 = fsub float %sub419, %mul487
  store float %sub493, float* %arrayidx355, align 4, !tbaa !3
  %sub497 = fsub float %sub420, %mul488
  store float %sub497, float* %arrayidx359, align 4, !tbaa !3
  %sub501 = fsub float %sub421, %mul489
  store float %sub501, float* %arrayidx363, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %51 = trunc i64 %indvars.iv.next to i32
  %cmp80 = icmp slt i32 %51, %13
  br i1 %cmp80, label %for.body81, label %for.end

for.end:                                          ; preds = %for.body81, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add486, %for.body81 ]
  %vnbtot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add277, %for.body81 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add350, %for.body81 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add351, %for.body81 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add352, %for.body81 ]
  %fix2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add416, %for.body81 ]
  %fiy2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add417, %for.body81 ]
  %fiz2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add418, %for.body81 ]
  %fix3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add490, %for.body81 ]
  %fiy3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add491, %for.body81 ]
  %fiz3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add492, %for.body81 ]
  %arrayidx506 = getelementptr inbounds float* %faction, i64 %idxprom44
  %52 = load float* %arrayidx506, align 4, !tbaa !3
  %add507 = fadd float %fix1.0.lcssa, %52
  store float %add507, float* %arrayidx506, align 4, !tbaa !3
  %arrayidx512 = getelementptr inbounds float* %faction, i64 %idxprom48
  %53 = load float* %arrayidx512, align 4, !tbaa !3
  %add513 = fadd float %fiy1.0.lcssa, %53
  store float %add513, float* %arrayidx512, align 4, !tbaa !3
  %arrayidx519 = getelementptr inbounds float* %faction, i64 %idxprom52
  %54 = load float* %arrayidx519, align 4, !tbaa !3
  %add520 = fadd float %fiz1.0.lcssa, %54
  store float %add520, float* %arrayidx519, align 4, !tbaa !3
  %arrayidx526 = getelementptr inbounds float* %faction, i64 %idxprom56
  %55 = load float* %arrayidx526, align 4, !tbaa !3
  %add527 = fadd float %fix2.0.lcssa, %55
  store float %add527, float* %arrayidx526, align 4, !tbaa !3
  %arrayidx533 = getelementptr inbounds float* %faction, i64 %idxprom60
  %56 = load float* %arrayidx533, align 4, !tbaa !3
  %add534 = fadd float %fiy2.0.lcssa, %56
  store float %add534, float* %arrayidx533, align 4, !tbaa !3
  %arrayidx540 = getelementptr inbounds float* %faction, i64 %idxprom64
  %57 = load float* %arrayidx540, align 4, !tbaa !3
  %add541 = fadd float %fiz2.0.lcssa, %57
  store float %add541, float* %arrayidx540, align 4, !tbaa !3
  %arrayidx547 = getelementptr inbounds float* %faction, i64 %idxprom68
  %58 = load float* %arrayidx547, align 4, !tbaa !3
  %add548 = fadd float %fix3.0.lcssa, %58
  store float %add548, float* %arrayidx547, align 4, !tbaa !3
  %arrayidx554 = getelementptr inbounds float* %faction, i64 %idxprom72
  %59 = load float* %arrayidx554, align 4, !tbaa !3
  %add555 = fadd float %fiy3.0.lcssa, %59
  store float %add555, float* %arrayidx554, align 4, !tbaa !3
  %arrayidx561 = getelementptr inbounds float* %faction, i64 %idxprom76
  %60 = load float* %arrayidx561, align 4, !tbaa !3
  %add562 = fadd float %fiz3.0.lcssa, %60
  store float %add562, float* %arrayidx561, align 4, !tbaa !3
  %arrayidx567 = getelementptr inbounds float* %fshift, i64 %idxprom28
  %61 = load float* %arrayidx567, align 4, !tbaa !3
  %add568 = fadd float %fix1.0.lcssa, %61
  %add569 = fadd float %fix2.0.lcssa, %add568
  %add570 = fadd float %fix3.0.lcssa, %add569
  store float %add570, float* %arrayidx567, align 4, !tbaa !3
  %arrayidx575 = getelementptr inbounds float* %fshift, i64 %idxprom31
  %62 = load float* %arrayidx575, align 4, !tbaa !3
  %add576 = fadd float %fiy1.0.lcssa, %62
  %add577 = fadd float %fiy2.0.lcssa, %add576
  %add578 = fadd float %fiy3.0.lcssa, %add577
  store float %add578, float* %arrayidx575, align 4, !tbaa !3
  %arrayidx584 = getelementptr inbounds float* %fshift, i64 %idxprom34
  %63 = load float* %arrayidx584, align 4, !tbaa !3
  %add585 = fadd float %fiz1.0.lcssa, %63
  %add586 = fadd float %fiz2.0.lcssa, %add585
  %add587 = fadd float %fiz3.0.lcssa, %add586
  store float %add587, float* %arrayidx584, align 4, !tbaa !3
  %arrayidx592 = getelementptr inbounds i32* %gid, i64 %indvars.iv1129
  %64 = load i32* %arrayidx592, align 4, !tbaa !0
  %idxprom593 = sext i32 %64 to i64
  %arrayidx594 = getelementptr inbounds float* %Vc, i64 %idxprom593
  %65 = load float* %arrayidx594, align 4, !tbaa !3
  %add595 = fadd float %vctot.0.lcssa, %65
  store float %add595, float* %arrayidx594, align 4, !tbaa !3
  %arrayidx599 = getelementptr inbounds float* %Vnb, i64 %idxprom593
  %66 = load float* %arrayidx599, align 4, !tbaa !3
  %add600 = fadd float %vnbtot.0.lcssa, %66
  store float %add600, float* %arrayidx599, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next1130 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end605, label %for.end.for.body_crit_edge

for.end.for.body_crit_edge:                       ; preds = %for.end
  %arrayidx37.phi.trans.insert = getelementptr inbounds i32* %iinr, i64 %indvars.iv.next1130
  %.pre = load i32* %arrayidx37.phi.trans.insert, align 4, !tbaa !0
  br label %for.body

for.end605:                                       ; preds = %for.end, %entry
  ret void
}
