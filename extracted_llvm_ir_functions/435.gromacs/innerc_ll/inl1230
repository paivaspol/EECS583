define void @inl1230(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc, i32* nocapture %type, i32 %ntype, float* nocapture %nbfp, float* nocapture %Vnb) #0 {
entry:
  %0 = load i32* %iinr, align 4, !tbaa !0
  %idxprom = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds float* %charge, i64 %idxprom
  %1 = load float* %arrayidx1, align 4, !tbaa !3
  %add = add nsw i32 %0, 1
  %idxprom2 = sext i32 %add to i64
  %arrayidx3 = getelementptr inbounds float* %charge, i64 %idxprom2
  %2 = load float* %arrayidx3, align 4, !tbaa !3
  %mul = fmul float %1, %facel
  %mul4 = fmul float %1, %mul
  %mul6 = fmul float %mul, %2
  %mul7 = fmul float %2, %facel
  %mul8 = fmul float %2, %mul7
  %mul9 = mul nsw i32 %ntype, 3
  %arrayidx11 = getelementptr inbounds i32* %type, i64 %idxprom
  %3 = load i32* %arrayidx11, align 4, !tbaa !0
  %mul12897 = add i32 %mul9, 3
  %add16 = mul i32 %3, %mul12897
  %idxprom17 = sext i32 %add16 to i64
  %arrayidx18 = getelementptr inbounds float* %nbfp, i64 %idxprom17
  %4 = load float* %arrayidx18, align 4, !tbaa !3
  %add22 = add nsw i32 %add16, 2
  %idxprom23 = sext i32 %add22 to i64
  %arrayidx24 = getelementptr inbounds float* %nbfp, i64 %idxprom23
  %5 = load float* %arrayidx24, align 4, !tbaa !3
  %cmp921 = icmp sgt i32 %nri, 0
  br i1 %cmp921, label %for.body.lr.ph, label %for.end501

for.body.lr.ph:                                   ; preds = %entry
  %add19 = add nsw i32 %add16, 1
  %idxprom20 = sext i32 %add19 to i64
  %arrayidx21 = getelementptr inbounds float* %nbfp, i64 %idxprom20
  %6 = load float* %arrayidx21, align 4, !tbaa !3
  %conv221 = fpext float %6 to double
  br label %for.body

for.body:                                         ; preds = %for.end.for.body_crit_edge, %for.body.lr.ph
  %7 = phi i32 [ %0, %for.body.lr.ph ], [ %.pre, %for.end.for.body_crit_edge ]
  %indvars.iv923 = phi i64 [ 0, %for.body.lr.ph ], [ %indvars.iv.next924, %for.end.for.body_crit_edge ]
  %arrayidx26 = getelementptr inbounds i32* %shift, i64 %indvars.iv923
  %8 = load i32* %arrayidx26, align 4, !tbaa !0
  %mul27 = mul nsw i32 %8, 3
  %idxprom28 = sext i32 %mul27 to i64
  %arrayidx29 = getelementptr inbounds float* %shiftvec, i64 %idxprom28
  %9 = load float* %arrayidx29, align 4, !tbaa !3
  %add30 = add nsw i32 %mul27, 1
  %idxprom31 = sext i32 %add30 to i64
  %arrayidx32 = getelementptr inbounds float* %shiftvec, i64 %idxprom31
  %10 = load float* %arrayidx32, align 4, !tbaa !3
  %add33 = add nsw i32 %mul27, 2
  %idxprom34 = sext i32 %add33 to i64
  %arrayidx35 = getelementptr inbounds float* %shiftvec, i64 %idxprom34
  %11 = load float* %arrayidx35, align 4, !tbaa !3
  %mul38 = mul nsw i32 %7, 3
  %arrayidx40 = getelementptr inbounds i32* %jindex, i64 %indvars.iv923
  %12 = load i32* %arrayidx40, align 4, !tbaa !0
  %indvars.iv.next924 = add i64 %indvars.iv923, 1
  %arrayidx43 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next924
  %13 = load i32* %arrayidx43, align 4, !tbaa !0
  %idxprom44 = sext i32 %mul38 to i64
  %arrayidx45 = getelementptr inbounds float* %pos, i64 %idxprom44
  %14 = load float* %arrayidx45, align 4, !tbaa !3
  %add46 = fadd float %9, %14
  %add47 = add nsw i32 %mul38, 1
  %idxprom48 = sext i32 %add47 to i64
  %arrayidx49 = getelementptr inbounds float* %pos, i64 %idxprom48
  %15 = load float* %arrayidx49, align 4, !tbaa !3
  %add50 = fadd float %10, %15
  %add51 = add nsw i32 %mul38, 2
  %idxprom52 = sext i32 %add51 to i64
  %arrayidx53 = getelementptr inbounds float* %pos, i64 %idxprom52
  %16 = load float* %arrayidx53, align 4, !tbaa !3
  %add54 = fadd float %11, %16
  %add55 = add nsw i32 %mul38, 3
  %idxprom56 = sext i32 %add55 to i64
  %arrayidx57 = getelementptr inbounds float* %pos, i64 %idxprom56
  %17 = load float* %arrayidx57, align 4, !tbaa !3
  %add58 = fadd float %9, %17
  %add59 = add nsw i32 %mul38, 4
  %idxprom60 = sext i32 %add59 to i64
  %arrayidx61 = getelementptr inbounds float* %pos, i64 %idxprom60
  %18 = load float* %arrayidx61, align 4, !tbaa !3
  %add62 = fadd float %10, %18
  %add63 = add nsw i32 %mul38, 5
  %idxprom64 = sext i32 %add63 to i64
  %arrayidx65 = getelementptr inbounds float* %pos, i64 %idxprom64
  %19 = load float* %arrayidx65, align 4, !tbaa !3
  %add66 = fadd float %11, %19
  %add67 = add nsw i32 %mul38, 6
  %idxprom68 = sext i32 %add67 to i64
  %arrayidx69 = getelementptr inbounds float* %pos, i64 %idxprom68
  %20 = load float* %arrayidx69, align 4, !tbaa !3
  %add70 = fadd float %9, %20
  %add71 = add nsw i32 %mul38, 7
  %idxprom72 = sext i32 %add71 to i64
  %arrayidx73 = getelementptr inbounds float* %pos, i64 %idxprom72
  %21 = load float* %arrayidx73, align 4, !tbaa !3
  %add74 = fadd float %10, %21
  %add75 = add nsw i32 %mul38, 8
  %idxprom76 = sext i32 %add75 to i64
  %arrayidx77 = getelementptr inbounds float* %pos, i64 %idxprom76
  %22 = load float* %arrayidx77, align 4, !tbaa !3
  %add78 = fadd float %11, %22
  %cmp80898 = icmp slt i32 %12, %13
  br i1 %cmp80898, label %for.body81.lr.ph, label %for.end

for.body81.lr.ph:                                 ; preds = %for.body
  %23 = sext i32 %12 to i64
  br label %for.body81

for.body81:                                       ; preds = %for.body81.lr.ph, %for.body81
  %indvars.iv = phi i64 [ %23, %for.body81.lr.ph ], [ %indvars.iv.next, %for.body81 ]
  %vctot.0909 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add382, %for.body81 ]
  %vnbtot.0908 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %sub228, %for.body81 ]
  %fix1.0907 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add282, %for.body81 ]
  %fiy1.0906 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add283, %for.body81 ]
  %fiz1.0905 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add284, %for.body81 ]
  %fix2.0904 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add330, %for.body81 ]
  %fiy2.0903 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add331, %for.body81 ]
  %fiz2.0902 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add332, %for.body81 ]
  %fix3.0901 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add386, %for.body81 ]
  %fiy3.0900 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add387, %for.body81 ]
  %fiz3.0899 = phi float [ 0.000000e+00, %for.body81.lr.ph ], [ %add388, %for.body81 ]
  %arrayidx83 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %24 = load i32* %arrayidx83, align 4, !tbaa !0
  %mul84 = mul nsw i32 %24, 3
  %idxprom85 = sext i32 %mul84 to i64
  %arrayidx86 = getelementptr inbounds float* %pos, i64 %idxprom85
  %25 = load float* %arrayidx86, align 4, !tbaa !3
  %add87 = add nsw i32 %mul84, 1
  %idxprom88 = sext i32 %add87 to i64
  %arrayidx89 = getelementptr inbounds float* %pos, i64 %idxprom88
  %26 = load float* %arrayidx89, align 4, !tbaa !3
  %add90 = add nsw i32 %mul84, 2
  %idxprom91 = sext i32 %add90 to i64
  %arrayidx92 = getelementptr inbounds float* %pos, i64 %idxprom91
  %27 = load float* %arrayidx92, align 4, !tbaa !3
  %add93 = add nsw i32 %mul84, 3
  %idxprom94 = sext i32 %add93 to i64
  %arrayidx95 = getelementptr inbounds float* %pos, i64 %idxprom94
  %28 = load float* %arrayidx95, align 4, !tbaa !3
  %add96 = add nsw i32 %mul84, 4
  %idxprom97 = sext i32 %add96 to i64
  %arrayidx98 = getelementptr inbounds float* %pos, i64 %idxprom97
  %29 = load float* %arrayidx98, align 4, !tbaa !3
  %add99 = add nsw i32 %mul84, 5
  %idxprom100 = sext i32 %add99 to i64
  %arrayidx101 = getelementptr inbounds float* %pos, i64 %idxprom100
  %30 = load float* %arrayidx101, align 4, !tbaa !3
  %add102 = add nsw i32 %mul84, 6
  %idxprom103 = sext i32 %add102 to i64
  %arrayidx104 = getelementptr inbounds float* %pos, i64 %idxprom103
  %31 = load float* %arrayidx104, align 4, !tbaa !3
  %add105 = add nsw i32 %mul84, 7
  %idxprom106 = sext i32 %add105 to i64
  %arrayidx107 = getelementptr inbounds float* %pos, i64 %idxprom106
  %32 = load float* %arrayidx107, align 4, !tbaa !3
  %add108 = add nsw i32 %mul84, 8
  %idxprom109 = sext i32 %add108 to i64
  %arrayidx110 = getelementptr inbounds float* %pos, i64 %idxprom109
  %33 = load float* %arrayidx110, align 4, !tbaa !3
  %sub = fsub float %add46, %25
  %sub111 = fsub float %add50, %26
  %sub112 = fsub float %add54, %27
  %mul113 = fmul float %sub, %sub
  %mul114 = fmul float %sub111, %sub111
  %add115 = fadd float %mul113, %mul114
  %mul116 = fmul float %sub112, %sub112
  %add117 = fadd float %add115, %mul116
  %sub118 = fsub float %add46, %28
  %sub119 = fsub float %add50, %29
  %sub120 = fsub float %add54, %30
  %mul121 = fmul float %sub118, %sub118
  %mul122 = fmul float %sub119, %sub119
  %add123 = fadd float %mul121, %mul122
  %mul124 = fmul float %sub120, %sub120
  %add125 = fadd float %add123, %mul124
  %sub126 = fsub float %add46, %31
  %sub127 = fsub float %add50, %32
  %sub128 = fsub float %add54, %33
  %mul129 = fmul float %sub126, %sub126
  %mul130 = fmul float %sub127, %sub127
  %add131 = fadd float %mul129, %mul130
  %mul132 = fmul float %sub128, %sub128
  %add133 = fadd float %add131, %mul132
  %sub134 = fsub float %add58, %25
  %sub135 = fsub float %add62, %26
  %sub136 = fsub float %add66, %27
  %mul137 = fmul float %sub134, %sub134
  %mul138 = fmul float %sub135, %sub135
  %add139 = fadd float %mul137, %mul138
  %mul140 = fmul float %sub136, %sub136
  %add141 = fadd float %add139, %mul140
  %sub142 = fsub float %add58, %28
  %sub143 = fsub float %add62, %29
  %sub144 = fsub float %add66, %30
  %mul145 = fmul float %sub142, %sub142
  %mul146 = fmul float %sub143, %sub143
  %add147 = fadd float %mul145, %mul146
  %mul148 = fmul float %sub144, %sub144
  %add149 = fadd float %add147, %mul148
  %sub150 = fsub float %add58, %31
  %sub151 = fsub float %add62, %32
  %sub152 = fsub float %add66, %33
  %mul153 = fmul float %sub150, %sub150
  %mul154 = fmul float %sub151, %sub151
  %add155 = fadd float %mul153, %mul154
  %mul156 = fmul float %sub152, %sub152
  %add157 = fadd float %add155, %mul156
  %sub158 = fsub float %add70, %25
  %sub159 = fsub float %add74, %26
  %sub160 = fsub float %add78, %27
  %mul161 = fmul float %sub158, %sub158
  %mul162 = fmul float %sub159, %sub159
  %add163 = fadd float %mul161, %mul162
  %mul164 = fmul float %sub160, %sub160
  %add165 = fadd float %add163, %mul164
  %sub166 = fsub float %add70, %28
  %sub167 = fsub float %add74, %29
  %sub168 = fsub float %add78, %30
  %mul169 = fmul float %sub166, %sub166
  %mul170 = fmul float %sub167, %sub167
  %add171 = fadd float %mul169, %mul170
  %mul172 = fmul float %sub168, %sub168
  %add173 = fadd float %add171, %mul172
  %sub174 = fsub float %add70, %31
  %sub175 = fsub float %add74, %32
  %sub176 = fsub float %add78, %33
  %mul177 = fmul float %sub174, %sub174
  %mul178 = fmul float %sub175, %sub175
  %add179 = fadd float %mul177, %mul178
  %mul180 = fmul float %sub176, %sub176
  %add181 = fadd float %add179, %mul180
  %conv = fpext float %add117 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv182 = fptrunc double %div to float
  %conv183 = fpext float %add141 to double
  %call184 = tail call double @sqrt(double %conv183) #2
  %div185 = fdiv double 1.000000e+00, %call184
  %conv186 = fptrunc double %div185 to float
  %conv187 = fpext float %add165 to double
  %call188 = tail call double @sqrt(double %conv187) #2
  %div189 = fdiv double 1.000000e+00, %call188
  %conv190 = fptrunc double %div189 to float
  %conv191 = fpext float %add125 to double
  %call192 = tail call double @sqrt(double %conv191) #2
  %div193 = fdiv double 1.000000e+00, %call192
  %conv194 = fptrunc double %div193 to float
  %conv195 = fpext float %add149 to double
  %call196 = tail call double @sqrt(double %conv195) #2
  %div197 = fdiv double 1.000000e+00, %call196
  %conv198 = fptrunc double %div197 to float
  %conv199 = fpext float %add173 to double
  %call200 = tail call double @sqrt(double %conv199) #2
  %div201 = fdiv double 1.000000e+00, %call200
  %conv202 = fptrunc double %div201 to float
  %conv203 = fpext float %add133 to double
  %call204 = tail call double @sqrt(double %conv203) #2
  %div205 = fdiv double 1.000000e+00, %call204
  %conv206 = fptrunc double %div205 to float
  %conv207 = fpext float %add157 to double
  %call208 = tail call double @sqrt(double %conv207) #2
  %div209 = fdiv double 1.000000e+00, %call208
  %conv210 = fptrunc double %div209 to float
  %conv211 = fpext float %add181 to double
  %call212 = tail call double @sqrt(double %conv211) #2
  %div213 = fdiv double 1.000000e+00, %call212
  %conv214 = fptrunc double %div213 to float
  %mul215 = fmul float %add117, %conv182
  %mul216 = fmul float %conv182, %conv182
  %mul217 = fmul float %mul216, %mul216
  %mul218 = fmul float %mul216, %mul217
  %mul219 = fmul float %4, %mul218
  %mul220 = fmul float %5, %mul215
  %sub222 = fsub float -0.000000e+00, %mul220
  %conv223 = fpext float %sub222 to double
  %call224 = tail call double @exp(double %conv223) #2
  %mul225 = fmul double %conv221, %call224
  %conv226 = fptrunc double %mul225 to float
  %add227 = fadd float %vnbtot.0908, %conv226
  %sub228 = fsub float %add227, %mul219
  %mul229 = fmul float %mul4, %conv182
  %mul230 = fmul float %mul220, %conv226
  %mul231 = fmul float %mul219, 6.000000e+00
  %sub232 = fsub float %mul230, %mul231
  %add233 = fadd float %mul229, %sub232
  %mul234 = fmul float %mul216, %add233
  %add235 = fadd float %vctot.0909, %mul229
  %mul236 = fmul float %sub, %mul234
  %mul237 = fmul float %sub111, %mul234
  %mul238 = fmul float %sub112, %mul234
  %add239 = fadd float %fix1.0907, %mul236
  %add240 = fadd float %fiy1.0906, %mul237
  %add241 = fadd float %fiz1.0905, %mul238
  %arrayidx243 = getelementptr inbounds float* %faction, i64 %idxprom85
  %34 = load float* %arrayidx243, align 4, !tbaa !3
  %sub244 = fsub float %34, %mul236
  %arrayidx247 = getelementptr inbounds float* %faction, i64 %idxprom88
  %35 = load float* %arrayidx247, align 4, !tbaa !3
  %sub248 = fsub float %35, %mul237
  %arrayidx251 = getelementptr inbounds float* %faction, i64 %idxprom91
  %36 = load float* %arrayidx251, align 4, !tbaa !3
  %sub252 = fsub float %36, %mul238
  %mul253 = fmul float %conv194, %conv194
  %mul254 = fmul float %mul6, %conv194
  %mul255 = fmul float %mul254, %mul253
  %add256 = fadd float %add235, %mul254
  %mul257 = fmul float %sub118, %mul255
  %mul258 = fmul float %sub119, %mul255
  %mul259 = fmul float %sub120, %mul255
  %add260 = fadd float %mul257, %add239
  %add261 = fadd float %mul258, %add240
  %add262 = fadd float %mul259, %add241
  %arrayidx265 = getelementptr inbounds float* %faction, i64 %idxprom94
  %37 = load float* %arrayidx265, align 4, !tbaa !3
  %sub266 = fsub float %37, %mul257
  %arrayidx269 = getelementptr inbounds float* %faction, i64 %idxprom97
  %38 = load float* %arrayidx269, align 4, !tbaa !3
  %sub270 = fsub float %38, %mul258
  %arrayidx273 = getelementptr inbounds float* %faction, i64 %idxprom100
  %39 = load float* %arrayidx273, align 4, !tbaa !3
  %sub274 = fsub float %39, %mul259
  %mul275 = fmul float %conv206, %conv206
  %mul276 = fmul float %mul6, %conv206
  %mul277 = fmul float %mul276, %mul275
  %add278 = fadd float %add256, %mul276
  %mul279 = fmul float %sub126, %mul277
  %mul280 = fmul float %sub127, %mul277
  %mul281 = fmul float %sub128, %mul277
  %add282 = fadd float %mul279, %add260
  %add283 = fadd float %mul280, %add261
  %add284 = fadd float %mul281, %add262
  %arrayidx287 = getelementptr inbounds float* %faction, i64 %idxprom103
  %40 = load float* %arrayidx287, align 4, !tbaa !3
  %sub288 = fsub float %40, %mul279
  %arrayidx291 = getelementptr inbounds float* %faction, i64 %idxprom106
  %41 = load float* %arrayidx291, align 4, !tbaa !3
  %sub292 = fsub float %41, %mul280
  %arrayidx295 = getelementptr inbounds float* %faction, i64 %idxprom109
  %42 = load float* %arrayidx295, align 4, !tbaa !3
  %sub296 = fsub float %42, %mul281
  %mul297 = fmul float %conv186, %conv186
  %mul298 = fmul float %mul6, %conv186
  %mul299 = fmul float %mul298, %mul297
  %add300 = fadd float %mul298, %add278
  %mul301 = fmul float %sub134, %mul299
  %mul302 = fmul float %sub135, %mul299
  %mul303 = fmul float %sub136, %mul299
  %add304 = fadd float %fix2.0904, %mul301
  %add305 = fadd float %fiy2.0903, %mul302
  %add306 = fadd float %fiz2.0902, %mul303
  %sub307 = fsub float %sub244, %mul301
  %sub308 = fsub float %sub248, %mul302
  %sub309 = fsub float %sub252, %mul303
  %mul310 = fmul float %conv198, %conv198
  %mul311 = fmul float %mul8, %conv198
  %mul312 = fmul float %mul311, %mul310
  %add313 = fadd float %mul311, %add300
  %mul314 = fmul float %sub142, %mul312
  %mul315 = fmul float %sub143, %mul312
  %mul316 = fmul float %sub144, %mul312
  %add317 = fadd float %add304, %mul314
  %add318 = fadd float %add305, %mul315
  %add319 = fadd float %add306, %mul316
  %sub320 = fsub float %sub266, %mul314
  %sub321 = fsub float %sub270, %mul315
  %sub322 = fsub float %sub274, %mul316
  %mul323 = fmul float %conv210, %conv210
  %mul324 = fmul float %mul8, %conv210
  %mul325 = fmul float %mul324, %mul323
  %add326 = fadd float %mul324, %add313
  %mul327 = fmul float %sub150, %mul325
  %mul328 = fmul float %sub151, %mul325
  %mul329 = fmul float %sub152, %mul325
  %add330 = fadd float %add317, %mul327
  %add331 = fadd float %add318, %mul328
  %add332 = fadd float %add319, %mul329
  %sub333 = fsub float %sub288, %mul327
  %sub334 = fsub float %sub292, %mul328
  %sub335 = fsub float %sub296, %mul329
  %mul336 = fmul float %conv190, %conv190
  %mul337 = fmul float %mul6, %conv190
  %mul338 = fmul float %mul337, %mul336
  %add339 = fadd float %mul337, %add326
  %mul340 = fmul float %sub158, %mul338
  %mul341 = fmul float %sub159, %mul338
  %mul342 = fmul float %sub160, %mul338
  %add343 = fadd float %fix3.0901, %mul340
  %add344 = fadd float %fiy3.0900, %mul341
  %add345 = fadd float %fiz3.0899, %mul342
  %sub346 = fsub float %sub307, %mul340
  store float %sub346, float* %arrayidx243, align 4, !tbaa !3
  %sub349 = fsub float %sub308, %mul341
  store float %sub349, float* %arrayidx247, align 4, !tbaa !3
  %sub353 = fsub float %sub309, %mul342
  store float %sub353, float* %arrayidx251, align 4, !tbaa !3
  %mul357 = fmul float %conv202, %conv202
  %mul358 = fmul float %mul8, %conv202
  %mul359 = fmul float %mul358, %mul357
  %add360 = fadd float %mul358, %add339
  %mul361 = fmul float %sub166, %mul359
  %mul362 = fmul float %sub167, %mul359
  %mul363 = fmul float %sub168, %mul359
  %add364 = fadd float %add343, %mul361
  %add365 = fadd float %add344, %mul362
  %add366 = fadd float %add345, %mul363
  %sub367 = fsub float %sub320, %mul361
  store float %sub367, float* %arrayidx265, align 4, !tbaa !3
  %sub371 = fsub float %sub321, %mul362
  store float %sub371, float* %arrayidx269, align 4, !tbaa !3
  %sub375 = fsub float %sub322, %mul363
  store float %sub375, float* %arrayidx273, align 4, !tbaa !3
  %mul379 = fmul float %conv214, %conv214
  %mul380 = fmul float %mul8, %conv214
  %mul381 = fmul float %mul380, %mul379
  %add382 = fadd float %mul380, %add360
  %mul383 = fmul float %sub174, %mul381
  %mul384 = fmul float %sub175, %mul381
  %mul385 = fmul float %sub176, %mul381
  %add386 = fadd float %add364, %mul383
  %add387 = fadd float %add365, %mul384
  %add388 = fadd float %add366, %mul385
  %sub389 = fsub float %sub333, %mul383
  store float %sub389, float* %arrayidx287, align 4, !tbaa !3
  %sub393 = fsub float %sub334, %mul384
  store float %sub393, float* %arrayidx291, align 4, !tbaa !3
  %sub397 = fsub float %sub335, %mul385
  store float %sub397, float* %arrayidx295, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %43 = trunc i64 %indvars.iv.next to i32
  %cmp80 = icmp slt i32 %43, %13
  br i1 %cmp80, label %for.body81, label %for.end

for.end:                                          ; preds = %for.body81, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add382, %for.body81 ]
  %vnbtot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %sub228, %for.body81 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add282, %for.body81 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add283, %for.body81 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add284, %for.body81 ]
  %fix2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add330, %for.body81 ]
  %fiy2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add331, %for.body81 ]
  %fiz2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add332, %for.body81 ]
  %fix3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add386, %for.body81 ]
  %fiy3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add387, %for.body81 ]
  %fiz3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add388, %for.body81 ]
  %arrayidx402 = getelementptr inbounds float* %faction, i64 %idxprom44
  %44 = load float* %arrayidx402, align 4, !tbaa !3
  %add403 = fadd float %fix1.0.lcssa, %44
  store float %add403, float* %arrayidx402, align 4, !tbaa !3
  %arrayidx408 = getelementptr inbounds float* %faction, i64 %idxprom48
  %45 = load float* %arrayidx408, align 4, !tbaa !3
  %add409 = fadd float %fiy1.0.lcssa, %45
  store float %add409, float* %arrayidx408, align 4, !tbaa !3
  %arrayidx415 = getelementptr inbounds float* %faction, i64 %idxprom52
  %46 = load float* %arrayidx415, align 4, !tbaa !3
  %add416 = fadd float %fiz1.0.lcssa, %46
  store float %add416, float* %arrayidx415, align 4, !tbaa !3
  %arrayidx422 = getelementptr inbounds float* %faction, i64 %idxprom56
  %47 = load float* %arrayidx422, align 4, !tbaa !3
  %add423 = fadd float %fix2.0.lcssa, %47
  store float %add423, float* %arrayidx422, align 4, !tbaa !3
  %arrayidx429 = getelementptr inbounds float* %faction, i64 %idxprom60
  %48 = load float* %arrayidx429, align 4, !tbaa !3
  %add430 = fadd float %fiy2.0.lcssa, %48
  store float %add430, float* %arrayidx429, align 4, !tbaa !3
  %arrayidx436 = getelementptr inbounds float* %faction, i64 %idxprom64
  %49 = load float* %arrayidx436, align 4, !tbaa !3
  %add437 = fadd float %fiz2.0.lcssa, %49
  store float %add437, float* %arrayidx436, align 4, !tbaa !3
  %arrayidx443 = getelementptr inbounds float* %faction, i64 %idxprom68
  %50 = load float* %arrayidx443, align 4, !tbaa !3
  %add444 = fadd float %fix3.0.lcssa, %50
  store float %add444, float* %arrayidx443, align 4, !tbaa !3
  %arrayidx450 = getelementptr inbounds float* %faction, i64 %idxprom72
  %51 = load float* %arrayidx450, align 4, !tbaa !3
  %add451 = fadd float %fiy3.0.lcssa, %51
  store float %add451, float* %arrayidx450, align 4, !tbaa !3
  %arrayidx457 = getelementptr inbounds float* %faction, i64 %idxprom76
  %52 = load float* %arrayidx457, align 4, !tbaa !3
  %add458 = fadd float %fiz3.0.lcssa, %52
  store float %add458, float* %arrayidx457, align 4, !tbaa !3
  %arrayidx463 = getelementptr inbounds float* %fshift, i64 %idxprom28
  %53 = load float* %arrayidx463, align 4, !tbaa !3
  %add464 = fadd float %fix1.0.lcssa, %53
  %add465 = fadd float %fix2.0.lcssa, %add464
  %add466 = fadd float %fix3.0.lcssa, %add465
  store float %add466, float* %arrayidx463, align 4, !tbaa !3
  %arrayidx471 = getelementptr inbounds float* %fshift, i64 %idxprom31
  %54 = load float* %arrayidx471, align 4, !tbaa !3
  %add472 = fadd float %fiy1.0.lcssa, %54
  %add473 = fadd float %fiy2.0.lcssa, %add472
  %add474 = fadd float %fiy3.0.lcssa, %add473
  store float %add474, float* %arrayidx471, align 4, !tbaa !3
  %arrayidx480 = getelementptr inbounds float* %fshift, i64 %idxprom34
  %55 = load float* %arrayidx480, align 4, !tbaa !3
  %add481 = fadd float %fiz1.0.lcssa, %55
  %add482 = fadd float %fiz2.0.lcssa, %add481
  %add483 = fadd float %fiz3.0.lcssa, %add482
  store float %add483, float* %arrayidx480, align 4, !tbaa !3
  %arrayidx488 = getelementptr inbounds i32* %gid, i64 %indvars.iv923
  %56 = load i32* %arrayidx488, align 4, !tbaa !0
  %idxprom489 = sext i32 %56 to i64
  %arrayidx490 = getelementptr inbounds float* %Vc, i64 %idxprom489
  %57 = load float* %arrayidx490, align 4, !tbaa !3
  %add491 = fadd float %vctot.0.lcssa, %57
  store float %add491, float* %arrayidx490, align 4, !tbaa !3
  %arrayidx495 = getelementptr inbounds float* %Vnb, i64 %idxprom489
  %58 = load float* %arrayidx495, align 4, !tbaa !3
  %add496 = fadd float %vnbtot.0.lcssa, %58
  store float %add496, float* %arrayidx495, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next924 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end501, label %for.end.for.body_crit_edge

for.end.for.body_crit_edge:                       ; preds = %for.end
  %arrayidx37.phi.trans.insert = getelementptr inbounds i32* %iinr, i64 %indvars.iv.next924
  %.pre = load i32* %arrayidx37.phi.trans.insert, align 4, !tbaa !0
  br label %for.body

for.end501:                                       ; preds = %for.end, %entry
  ret void
}
