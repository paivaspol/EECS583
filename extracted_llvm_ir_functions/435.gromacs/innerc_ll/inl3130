define void @inl3130(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc, i32* nocapture %type, i32 %ntype, float* nocapture %nbfp, float* nocapture %Vnb, float %tabscale, float* %VFtab) #0 {
entry:
  %0 = load i32* %iinr, align 4, !tbaa !0
  %idxprom = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds float* %charge, i64 %idxprom
  %1 = load float* %arrayidx1, align 4, !tbaa !3
  %add = add nsw i32 %0, 1
  %idxprom2 = sext i32 %add to i64
  %arrayidx3 = getelementptr inbounds float* %charge, i64 %idxprom2
  %2 = load float* %arrayidx3, align 4, !tbaa !3
  %mul = fmul float %1, %facel
  %mul4 = fmul float %1, %mul
  %mul6 = fmul float %mul, %2
  %mul7 = fmul float %2, %facel
  %mul8 = fmul float %2, %mul7
  %mul9 = shl i32 %ntype, 1
  %arrayidx11 = getelementptr inbounds i32* %type, i64 %idxprom
  %3 = load i32* %arrayidx11, align 4, !tbaa !0
  %mul12 = mul nsw i32 %mul9, %3
  %mul15 = shl nsw i32 %3, 1
  %add16 = add nsw i32 %mul12, %mul15
  %idxprom17 = sext i32 %add16 to i64
  %arrayidx18 = getelementptr inbounds float* %nbfp, i64 %idxprom17
  %4 = load float* %arrayidx18, align 4, !tbaa !3
  %add191412 = or i32 %add16, 1
  %idxprom20 = sext i32 %add191412 to i64
  %arrayidx21 = getelementptr inbounds float* %nbfp, i64 %idxprom20
  %5 = load float* %arrayidx21, align 4, !tbaa !3
  %cmp1463 = icmp sgt i32 %nri, 0
  br i1 %cmp1463, label %for.body, label %for.end754

for.body:                                         ; preds = %entry, %for.end.for.body_crit_edge
  %6 = phi i32 [ %.pre, %for.end.for.body_crit_edge ], [ %0, %entry ]
  %indvars.iv1465 = phi i64 [ %indvars.iv.next1466, %for.end.for.body_crit_edge ], [ 0, %entry ]
  %arrayidx23 = getelementptr inbounds i32* %shift, i64 %indvars.iv1465
  %7 = load i32* %arrayidx23, align 4, !tbaa !0
  %mul24 = mul nsw i32 %7, 3
  %idxprom25 = sext i32 %mul24 to i64
  %arrayidx26 = getelementptr inbounds float* %shiftvec, i64 %idxprom25
  %8 = load float* %arrayidx26, align 4, !tbaa !3
  %add27 = add nsw i32 %mul24, 1
  %idxprom28 = sext i32 %add27 to i64
  %arrayidx29 = getelementptr inbounds float* %shiftvec, i64 %idxprom28
  %9 = load float* %arrayidx29, align 4, !tbaa !3
  %add30 = add nsw i32 %mul24, 2
  %idxprom31 = sext i32 %add30 to i64
  %arrayidx32 = getelementptr inbounds float* %shiftvec, i64 %idxprom31
  %10 = load float* %arrayidx32, align 4, !tbaa !3
  %mul35 = mul nsw i32 %6, 3
  %arrayidx37 = getelementptr inbounds i32* %jindex, i64 %indvars.iv1465
  %11 = load i32* %arrayidx37, align 4, !tbaa !0
  %indvars.iv.next1466 = add i64 %indvars.iv1465, 1
  %arrayidx40 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next1466
  %12 = load i32* %arrayidx40, align 4, !tbaa !0
  %idxprom41 = sext i32 %mul35 to i64
  %arrayidx42 = getelementptr inbounds float* %pos, i64 %idxprom41
  %13 = load float* %arrayidx42, align 4, !tbaa !3
  %add43 = fadd float %8, %13
  %add44 = add nsw i32 %mul35, 1
  %idxprom45 = sext i32 %add44 to i64
  %arrayidx46 = getelementptr inbounds float* %pos, i64 %idxprom45
  %14 = load float* %arrayidx46, align 4, !tbaa !3
  %add47 = fadd float %9, %14
  %add48 = add nsw i32 %mul35, 2
  %idxprom49 = sext i32 %add48 to i64
  %arrayidx50 = getelementptr inbounds float* %pos, i64 %idxprom49
  %15 = load float* %arrayidx50, align 4, !tbaa !3
  %add51 = fadd float %10, %15
  %add52 = add nsw i32 %mul35, 3
  %idxprom53 = sext i32 %add52 to i64
  %arrayidx54 = getelementptr inbounds float* %pos, i64 %idxprom53
  %16 = load float* %arrayidx54, align 4, !tbaa !3
  %add55 = fadd float %8, %16
  %add56 = add nsw i32 %mul35, 4
  %idxprom57 = sext i32 %add56 to i64
  %arrayidx58 = getelementptr inbounds float* %pos, i64 %idxprom57
  %17 = load float* %arrayidx58, align 4, !tbaa !3
  %add59 = fadd float %9, %17
  %add60 = add nsw i32 %mul35, 5
  %idxprom61 = sext i32 %add60 to i64
  %arrayidx62 = getelementptr inbounds float* %pos, i64 %idxprom61
  %18 = load float* %arrayidx62, align 4, !tbaa !3
  %add63 = fadd float %10, %18
  %add64 = add nsw i32 %mul35, 6
  %idxprom65 = sext i32 %add64 to i64
  %arrayidx66 = getelementptr inbounds float* %pos, i64 %idxprom65
  %19 = load float* %arrayidx66, align 4, !tbaa !3
  %add67 = fadd float %8, %19
  %add68 = add nsw i32 %mul35, 7
  %idxprom69 = sext i32 %add68 to i64
  %arrayidx70 = getelementptr inbounds float* %pos, i64 %idxprom69
  %20 = load float* %arrayidx70, align 4, !tbaa !3
  %add71 = fadd float %9, %20
  %add72 = add nsw i32 %mul35, 8
  %idxprom73 = sext i32 %add72 to i64
  %arrayidx74 = getelementptr inbounds float* %pos, i64 %idxprom73
  %21 = load float* %arrayidx74, align 4, !tbaa !3
  %add75 = fadd float %10, %21
  %cmp771440 = icmp slt i32 %11, %12
  br i1 %cmp771440, label %for.body78.lr.ph, label %for.end

for.body78.lr.ph:                                 ; preds = %for.body
  %22 = sext i32 %11 to i64
  br label %for.body78

for.body78:                                       ; preds = %for.body78.lr.ph, %for.body78
  %indvars.iv = phi i64 [ %22, %for.body78.lr.ph ], [ %indvars.iv.next, %for.body78 ]
  %vctot.01451 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add635, %for.body78 ]
  %vnbtot.01450 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %sub220, %for.body78 ]
  %fix1.01449 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add361, %for.body78 ]
  %fiy1.01448 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add362, %for.body78 ]
  %fiz1.01447 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add363, %for.body78 ]
  %fix2.01446 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add496, %for.body78 ]
  %fiy2.01445 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add497, %for.body78 ]
  %fiz2.01444 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add498, %for.body78 ]
  %fix3.01443 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add639, %for.body78 ]
  %fiy3.01442 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add640, %for.body78 ]
  %fiz3.01441 = phi float [ 0.000000e+00, %for.body78.lr.ph ], [ %add641, %for.body78 ]
  %arrayidx80 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %23 = load i32* %arrayidx80, align 4, !tbaa !0
  %mul81 = mul nsw i32 %23, 3
  %idxprom82 = sext i32 %mul81 to i64
  %arrayidx83 = getelementptr inbounds float* %pos, i64 %idxprom82
  %24 = load float* %arrayidx83, align 4, !tbaa !3
  %add84 = add nsw i32 %mul81, 1
  %idxprom85 = sext i32 %add84 to i64
  %arrayidx86 = getelementptr inbounds float* %pos, i64 %idxprom85
  %25 = load float* %arrayidx86, align 4, !tbaa !3
  %add87 = add nsw i32 %mul81, 2
  %idxprom88 = sext i32 %add87 to i64
  %arrayidx89 = getelementptr inbounds float* %pos, i64 %idxprom88
  %26 = load float* %arrayidx89, align 4, !tbaa !3
  %add90 = add nsw i32 %mul81, 3
  %idxprom91 = sext i32 %add90 to i64
  %arrayidx92 = getelementptr inbounds float* %pos, i64 %idxprom91
  %27 = load float* %arrayidx92, align 4, !tbaa !3
  %add93 = add nsw i32 %mul81, 4
  %idxprom94 = sext i32 %add93 to i64
  %arrayidx95 = getelementptr inbounds float* %pos, i64 %idxprom94
  %28 = load float* %arrayidx95, align 4, !tbaa !3
  %add96 = add nsw i32 %mul81, 5
  %idxprom97 = sext i32 %add96 to i64
  %arrayidx98 = getelementptr inbounds float* %pos, i64 %idxprom97
  %29 = load float* %arrayidx98, align 4, !tbaa !3
  %add99 = add nsw i32 %mul81, 6
  %idxprom100 = sext i32 %add99 to i64
  %arrayidx101 = getelementptr inbounds float* %pos, i64 %idxprom100
  %30 = load float* %arrayidx101, align 4, !tbaa !3
  %add102 = add nsw i32 %mul81, 7
  %idxprom103 = sext i32 %add102 to i64
  %arrayidx104 = getelementptr inbounds float* %pos, i64 %idxprom103
  %31 = load float* %arrayidx104, align 4, !tbaa !3
  %add105 = add nsw i32 %mul81, 8
  %idxprom106 = sext i32 %add105 to i64
  %arrayidx107 = getelementptr inbounds float* %pos, i64 %idxprom106
  %32 = load float* %arrayidx107, align 4, !tbaa !3
  %sub = fsub float %add43, %24
  %sub108 = fsub float %add47, %25
  %sub109 = fsub float %add51, %26
  %mul110 = fmul float %sub, %sub
  %mul111 = fmul float %sub108, %sub108
  %add112 = fadd float %mul110, %mul111
  %mul113 = fmul float %sub109, %sub109
  %add114 = fadd float %add112, %mul113
  %sub115 = fsub float %add43, %27
  %sub116 = fsub float %add47, %28
  %sub117 = fsub float %add51, %29
  %mul118 = fmul float %sub115, %sub115
  %mul119 = fmul float %sub116, %sub116
  %add120 = fadd float %mul118, %mul119
  %mul121 = fmul float %sub117, %sub117
  %add122 = fadd float %add120, %mul121
  %sub123 = fsub float %add43, %30
  %sub124 = fsub float %add47, %31
  %sub125 = fsub float %add51, %32
  %mul126 = fmul float %sub123, %sub123
  %mul127 = fmul float %sub124, %sub124
  %add128 = fadd float %mul126, %mul127
  %mul129 = fmul float %sub125, %sub125
  %add130 = fadd float %add128, %mul129
  %sub131 = fsub float %add55, %24
  %sub132 = fsub float %add59, %25
  %sub133 = fsub float %add63, %26
  %mul134 = fmul float %sub131, %sub131
  %mul135 = fmul float %sub132, %sub132
  %add136 = fadd float %mul134, %mul135
  %mul137 = fmul float %sub133, %sub133
  %add138 = fadd float %add136, %mul137
  %sub139 = fsub float %add55, %27
  %sub140 = fsub float %add59, %28
  %sub141 = fsub float %add63, %29
  %mul142 = fmul float %sub139, %sub139
  %mul143 = fmul float %sub140, %sub140
  %add144 = fadd float %mul142, %mul143
  %mul145 = fmul float %sub141, %sub141
  %add146 = fadd float %add144, %mul145
  %sub147 = fsub float %add55, %30
  %sub148 = fsub float %add59, %31
  %sub149 = fsub float %add63, %32
  %mul150 = fmul float %sub147, %sub147
  %mul151 = fmul float %sub148, %sub148
  %add152 = fadd float %mul150, %mul151
  %mul153 = fmul float %sub149, %sub149
  %add154 = fadd float %add152, %mul153
  %sub155 = fsub float %add67, %24
  %sub156 = fsub float %add71, %25
  %sub157 = fsub float %add75, %26
  %mul158 = fmul float %sub155, %sub155
  %mul159 = fmul float %sub156, %sub156
  %add160 = fadd float %mul158, %mul159
  %mul161 = fmul float %sub157, %sub157
  %add162 = fadd float %add160, %mul161
  %sub163 = fsub float %add67, %27
  %sub164 = fsub float %add71, %28
  %sub165 = fsub float %add75, %29
  %mul166 = fmul float %sub163, %sub163
  %mul167 = fmul float %sub164, %sub164
  %add168 = fadd float %mul166, %mul167
  %mul169 = fmul float %sub165, %sub165
  %add170 = fadd float %add168, %mul169
  %sub171 = fsub float %add67, %30
  %sub172 = fsub float %add71, %31
  %sub173 = fsub float %add75, %32
  %mul174 = fmul float %sub171, %sub171
  %mul175 = fmul float %sub172, %sub172
  %add176 = fadd float %mul174, %mul175
  %mul177 = fmul float %sub173, %sub173
  %add178 = fadd float %add176, %mul177
  %conv = fpext float %add114 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv179 = fptrunc double %div to float
  %conv180 = fpext float %add138 to double
  %call181 = tail call double @sqrt(double %conv180) #2
  %div182 = fdiv double 1.000000e+00, %call181
  %conv183 = fptrunc double %div182 to float
  %conv184 = fpext float %add162 to double
  %call185 = tail call double @sqrt(double %conv184) #2
  %div186 = fdiv double 1.000000e+00, %call185
  %conv187 = fptrunc double %div186 to float
  %conv188 = fpext float %add122 to double
  %call189 = tail call double @sqrt(double %conv188) #2
  %div190 = fdiv double 1.000000e+00, %call189
  %conv191 = fptrunc double %div190 to float
  %conv192 = fpext float %add146 to double
  %call193 = tail call double @sqrt(double %conv192) #2
  %div194 = fdiv double 1.000000e+00, %call193
  %conv195 = fptrunc double %div194 to float
  %conv196 = fpext float %add170 to double
  %call197 = tail call double @sqrt(double %conv196) #2
  %div198 = fdiv double 1.000000e+00, %call197
  %conv199 = fptrunc double %div198 to float
  %conv200 = fpext float %add130 to double
  %call201 = tail call double @sqrt(double %conv200) #2
  %div202 = fdiv double 1.000000e+00, %call201
  %conv203 = fptrunc double %div202 to float
  %conv204 = fpext float %add154 to double
  %call205 = tail call double @sqrt(double %conv204) #2
  %div206 = fdiv double 1.000000e+00, %call205
  %conv207 = fptrunc double %div206 to float
  %conv208 = fpext float %add178 to double
  %call209 = tail call double @sqrt(double %conv208) #2
  %div210 = fdiv double 1.000000e+00, %call209
  %conv211 = fptrunc double %div210 to float
  %mul212 = fmul float %add114, %conv179
  %mul213 = fmul float %conv179, %conv179
  %mul214 = fmul float %mul213, %mul213
  %mul215 = fmul float %mul213, %mul214
  %mul216 = fmul float %4, %mul215
  %mul217 = fmul float %5, %mul215
  %mul218 = fmul float %mul215, %mul217
  %add219 = fadd float %vnbtot.01450, %mul218
  %sub220 = fsub float %add219, %mul216
  %mul221 = fmul float %mul212, %tabscale
  %conv222 = fptosi float %mul221 to i32
  %conv223 = sitofp i32 %conv222 to float
  %sub224 = fsub float %mul221, %conv223
  %mul225 = fmul float %sub224, %sub224
  %mul226 = shl nsw i32 %conv222, 2
  %idxprom227 = sext i32 %mul226 to i64
  %arrayidx228 = getelementptr inbounds float* %VFtab, i64 %idxprom227
  %33 = load float* %arrayidx228, align 4, !tbaa !3
  %add2291413 = or i32 %mul226, 1
  %idxprom230 = sext i32 %add2291413 to i64
  %arrayidx231 = getelementptr inbounds float* %VFtab, i64 %idxprom230
  %34 = load float* %arrayidx231, align 4, !tbaa !3
  %add2321414 = or i32 %mul226, 2
  %idxprom233 = sext i32 %add2321414 to i64
  %arrayidx234 = getelementptr inbounds float* %VFtab, i64 %idxprom233
  %35 = load float* %arrayidx234, align 4, !tbaa !3
  %mul235 = fmul float %sub224, %35
  %add2361415 = or i32 %mul226, 3
  %idxprom237 = sext i32 %add2361415 to i64
  %arrayidx238 = getelementptr inbounds float* %VFtab, i64 %idxprom237
  %36 = load float* %arrayidx238, align 4, !tbaa !3
  %mul239 = fmul float %mul225, %36
  %add240 = fadd float %34, %mul235
  %add241 = fadd float %add240, %mul239
  %mul242 = fmul float %sub224, %add241
  %add243 = fadd float %33, %mul242
  %add244 = fadd float %mul235, %add241
  %mul245 = fmul float %mul239, 2.000000e+00
  %add246 = fadd float %mul245, %add244
  %mul247 = fmul float %mul4, %add243
  %mul248 = fmul float %mul4, %add246
  %mul249 = fmul float %mul218, 1.200000e+01
  %mul250 = fmul float %mul216, 6.000000e+00
  %sub251 = fsub float %mul249, %mul250
  %mul252 = fmul float %conv179, %sub251
  %mul253 = fmul float %mul248, %tabscale
  %sub254 = fsub float %mul252, %mul253
  %mul255 = fmul float %conv179, %sub254
  %add256 = fadd float %vctot.01451, %mul247
  %mul257 = fmul float %sub, %mul255
  %mul258 = fmul float %sub108, %mul255
  %mul259 = fmul float %sub109, %mul255
  %add260 = fadd float %fix1.01449, %mul257
  %add261 = fadd float %fiy1.01448, %mul258
  %add262 = fadd float %fiz1.01447, %mul259
  %arrayidx264 = getelementptr inbounds float* %faction, i64 %idxprom82
  %37 = load float* %arrayidx264, align 4, !tbaa !3
  %sub265 = fsub float %37, %mul257
  %arrayidx268 = getelementptr inbounds float* %faction, i64 %idxprom85
  %38 = load float* %arrayidx268, align 4, !tbaa !3
  %sub269 = fsub float %38, %mul258
  %arrayidx272 = getelementptr inbounds float* %faction, i64 %idxprom88
  %39 = load float* %arrayidx272, align 4, !tbaa !3
  %sub273 = fsub float %39, %mul259
  %mul274 = fmul float %add122, %conv191
  %mul275 = fmul float %mul274, %tabscale
  %conv276 = fptosi float %mul275 to i32
  %conv277 = sitofp i32 %conv276 to float
  %sub278 = fsub float %mul275, %conv277
  %mul279 = fmul float %sub278, %sub278
  %mul280 = shl nsw i32 %conv276, 2
  %idxprom281 = sext i32 %mul280 to i64
  %arrayidx282 = getelementptr inbounds float* %VFtab, i64 %idxprom281
  %40 = load float* %arrayidx282, align 4, !tbaa !3
  %add2831416 = or i32 %mul280, 1
  %idxprom284 = sext i32 %add2831416 to i64
  %arrayidx285 = getelementptr inbounds float* %VFtab, i64 %idxprom284
  %41 = load float* %arrayidx285, align 4, !tbaa !3
  %add2861417 = or i32 %mul280, 2
  %idxprom287 = sext i32 %add2861417 to i64
  %arrayidx288 = getelementptr inbounds float* %VFtab, i64 %idxprom287
  %42 = load float* %arrayidx288, align 4, !tbaa !3
  %mul289 = fmul float %sub278, %42
  %add2901418 = or i32 %mul280, 3
  %idxprom291 = sext i32 %add2901418 to i64
  %arrayidx292 = getelementptr inbounds float* %VFtab, i64 %idxprom291
  %43 = load float* %arrayidx292, align 4, !tbaa !3
  %mul293 = fmul float %mul279, %43
  %add294 = fadd float %41, %mul289
  %add295 = fadd float %add294, %mul293
  %mul296 = fmul float %sub278, %add295
  %add297 = fadd float %40, %mul296
  %add298 = fadd float %mul289, %add295
  %mul299 = fmul float %mul293, 2.000000e+00
  %add300 = fadd float %mul299, %add298
  %mul301 = fmul float %mul6, %add297
  %mul302 = fmul float %mul6, %add300
  %mul303 = fmul float %mul302, %tabscale
  %44 = fmul float %conv191, %mul303
  %mul305 = fsub float -0.000000e+00, %44
  %add306 = fadd float %add256, %mul301
  %mul307 = fmul float %sub115, %mul305
  %mul308 = fmul float %sub116, %mul305
  %mul309 = fmul float %sub117, %mul305
  %add310 = fadd float %add260, %mul307
  %add311 = fadd float %add261, %mul308
  %add312 = fadd float %add262, %mul309
  %arrayidx315 = getelementptr inbounds float* %faction, i64 %idxprom91
  %45 = load float* %arrayidx315, align 4, !tbaa !3
  %sub316 = fsub float %45, %mul307
  %arrayidx319 = getelementptr inbounds float* %faction, i64 %idxprom94
  %46 = load float* %arrayidx319, align 4, !tbaa !3
  %sub320 = fsub float %46, %mul308
  %arrayidx323 = getelementptr inbounds float* %faction, i64 %idxprom97
  %47 = load float* %arrayidx323, align 4, !tbaa !3
  %sub324 = fsub float %47, %mul309
  %mul325 = fmul float %add130, %conv203
  %mul326 = fmul float %mul325, %tabscale
  %conv327 = fptosi float %mul326 to i32
  %conv328 = sitofp i32 %conv327 to float
  %sub329 = fsub float %mul326, %conv328
  %mul330 = fmul float %sub329, %sub329
  %mul331 = shl nsw i32 %conv327, 2
  %idxprom332 = sext i32 %mul331 to i64
  %arrayidx333 = getelementptr inbounds float* %VFtab, i64 %idxprom332
  %48 = load float* %arrayidx333, align 4, !tbaa !3
  %add3341419 = or i32 %mul331, 1
  %idxprom335 = sext i32 %add3341419 to i64
  %arrayidx336 = getelementptr inbounds float* %VFtab, i64 %idxprom335
  %49 = load float* %arrayidx336, align 4, !tbaa !3
  %add3371420 = or i32 %mul331, 2
  %idxprom338 = sext i32 %add3371420 to i64
  %arrayidx339 = getelementptr inbounds float* %VFtab, i64 %idxprom338
  %50 = load float* %arrayidx339, align 4, !tbaa !3
  %mul340 = fmul float %sub329, %50
  %add3411421 = or i32 %mul331, 3
  %idxprom342 = sext i32 %add3411421 to i64
  %arrayidx343 = getelementptr inbounds float* %VFtab, i64 %idxprom342
  %51 = load float* %arrayidx343, align 4, !tbaa !3
  %mul344 = fmul float %mul330, %51
  %add345 = fadd float %49, %mul340
  %add346 = fadd float %add345, %mul344
  %mul347 = fmul float %sub329, %add346
  %add348 = fadd float %48, %mul347
  %add349 = fadd float %mul340, %add346
  %mul350 = fmul float %mul344, 2.000000e+00
  %add351 = fadd float %mul350, %add349
  %mul352 = fmul float %mul6, %add348
  %mul353 = fmul float %mul6, %add351
  %mul354 = fmul float %mul353, %tabscale
  %52 = fmul float %conv203, %mul354
  %mul356 = fsub float -0.000000e+00, %52
  %add357 = fadd float %add306, %mul352
  %mul358 = fmul float %sub123, %mul356
  %mul359 = fmul float %sub124, %mul356
  %mul360 = fmul float %sub125, %mul356
  %add361 = fadd float %add310, %mul358
  %add362 = fadd float %add311, %mul359
  %add363 = fadd float %add312, %mul360
  %arrayidx366 = getelementptr inbounds float* %faction, i64 %idxprom100
  %53 = load float* %arrayidx366, align 4, !tbaa !3
  %sub367 = fsub float %53, %mul358
  %arrayidx370 = getelementptr inbounds float* %faction, i64 %idxprom103
  %54 = load float* %arrayidx370, align 4, !tbaa !3
  %sub371 = fsub float %54, %mul359
  %arrayidx374 = getelementptr inbounds float* %faction, i64 %idxprom106
  %55 = load float* %arrayidx374, align 4, !tbaa !3
  %sub375 = fsub float %55, %mul360
  %mul376 = fmul float %add138, %conv183
  %mul377 = fmul float %mul376, %tabscale
  %conv378 = fptosi float %mul377 to i32
  %conv379 = sitofp i32 %conv378 to float
  %sub380 = fsub float %mul377, %conv379
  %mul381 = fmul float %sub380, %sub380
  %mul382 = shl nsw i32 %conv378, 2
  %idxprom383 = sext i32 %mul382 to i64
  %arrayidx384 = getelementptr inbounds float* %VFtab, i64 %idxprom383
  %56 = load float* %arrayidx384, align 4, !tbaa !3
  %add3851422 = or i32 %mul382, 1
  %idxprom386 = sext i32 %add3851422 to i64
  %arrayidx387 = getelementptr inbounds float* %VFtab, i64 %idxprom386
  %57 = load float* %arrayidx387, align 4, !tbaa !3
  %add3881423 = or i32 %mul382, 2
  %idxprom389 = sext i32 %add3881423 to i64
  %arrayidx390 = getelementptr inbounds float* %VFtab, i64 %idxprom389
  %58 = load float* %arrayidx390, align 4, !tbaa !3
  %mul391 = fmul float %sub380, %58
  %add3921424 = or i32 %mul382, 3
  %idxprom393 = sext i32 %add3921424 to i64
  %arrayidx394 = getelementptr inbounds float* %VFtab, i64 %idxprom393
  %59 = load float* %arrayidx394, align 4, !tbaa !3
  %mul395 = fmul float %mul381, %59
  %add396 = fadd float %57, %mul391
  %add397 = fadd float %add396, %mul395
  %mul398 = fmul float %sub380, %add397
  %add399 = fadd float %56, %mul398
  %add400 = fadd float %mul391, %add397
  %mul401 = fmul float %mul395, 2.000000e+00
  %add402 = fadd float %mul401, %add400
  %mul403 = fmul float %mul6, %add399
  %mul404 = fmul float %mul6, %add402
  %mul405 = fmul float %mul404, %tabscale
  %60 = fmul float %conv183, %mul405
  %mul407 = fsub float -0.000000e+00, %60
  %add408 = fadd float %add357, %mul403
  %mul409 = fmul float %sub131, %mul407
  %mul410 = fmul float %sub132, %mul407
  %mul411 = fmul float %sub133, %mul407
  %add412 = fadd float %fix2.01446, %mul409
  %add413 = fadd float %fiy2.01445, %mul410
  %add414 = fadd float %fiz2.01444, %mul411
  %sub415 = fsub float %sub265, %mul409
  %sub416 = fsub float %sub269, %mul410
  %sub417 = fsub float %sub273, %mul411
  %mul418 = fmul float %add146, %conv195
  %mul419 = fmul float %mul418, %tabscale
  %conv420 = fptosi float %mul419 to i32
  %conv421 = sitofp i32 %conv420 to float
  %sub422 = fsub float %mul419, %conv421
  %mul423 = fmul float %sub422, %sub422
  %mul424 = shl nsw i32 %conv420, 2
  %idxprom425 = sext i32 %mul424 to i64
  %arrayidx426 = getelementptr inbounds float* %VFtab, i64 %idxprom425
  %61 = load float* %arrayidx426, align 4, !tbaa !3
  %add4271425 = or i32 %mul424, 1
  %idxprom428 = sext i32 %add4271425 to i64
  %arrayidx429 = getelementptr inbounds float* %VFtab, i64 %idxprom428
  %62 = load float* %arrayidx429, align 4, !tbaa !3
  %add4301426 = or i32 %mul424, 2
  %idxprom431 = sext i32 %add4301426 to i64
  %arrayidx432 = getelementptr inbounds float* %VFtab, i64 %idxprom431
  %63 = load float* %arrayidx432, align 4, !tbaa !3
  %mul433 = fmul float %sub422, %63
  %add4341427 = or i32 %mul424, 3
  %idxprom435 = sext i32 %add4341427 to i64
  %arrayidx436 = getelementptr inbounds float* %VFtab, i64 %idxprom435
  %64 = load float* %arrayidx436, align 4, !tbaa !3
  %mul437 = fmul float %mul423, %64
  %add438 = fadd float %62, %mul433
  %add439 = fadd float %add438, %mul437
  %mul440 = fmul float %sub422, %add439
  %add441 = fadd float %61, %mul440
  %add442 = fadd float %mul433, %add439
  %mul443 = fmul float %mul437, 2.000000e+00
  %add444 = fadd float %mul443, %add442
  %mul445 = fmul float %mul8, %add441
  %mul446 = fmul float %mul8, %add444
  %mul447 = fmul float %mul446, %tabscale
  %65 = fmul float %conv195, %mul447
  %mul449 = fsub float -0.000000e+00, %65
  %add450 = fadd float %add408, %mul445
  %mul451 = fmul float %sub139, %mul449
  %mul452 = fmul float %sub140, %mul449
  %mul453 = fmul float %sub141, %mul449
  %add454 = fadd float %add412, %mul451
  %add455 = fadd float %add413, %mul452
  %add456 = fadd float %add414, %mul453
  %sub457 = fsub float %sub316, %mul451
  %sub458 = fsub float %sub320, %mul452
  %sub459 = fsub float %sub324, %mul453
  %mul460 = fmul float %add154, %conv207
  %mul461 = fmul float %mul460, %tabscale
  %conv462 = fptosi float %mul461 to i32
  %conv463 = sitofp i32 %conv462 to float
  %sub464 = fsub float %mul461, %conv463
  %mul465 = fmul float %sub464, %sub464
  %mul466 = shl nsw i32 %conv462, 2
  %idxprom467 = sext i32 %mul466 to i64
  %arrayidx468 = getelementptr inbounds float* %VFtab, i64 %idxprom467
  %66 = load float* %arrayidx468, align 4, !tbaa !3
  %add4691428 = or i32 %mul466, 1
  %idxprom470 = sext i32 %add4691428 to i64
  %arrayidx471 = getelementptr inbounds float* %VFtab, i64 %idxprom470
  %67 = load float* %arrayidx471, align 4, !tbaa !3
  %add4721429 = or i32 %mul466, 2
  %idxprom473 = sext i32 %add4721429 to i64
  %arrayidx474 = getelementptr inbounds float* %VFtab, i64 %idxprom473
  %68 = load float* %arrayidx474, align 4, !tbaa !3
  %mul475 = fmul float %sub464, %68
  %add4761430 = or i32 %mul466, 3
  %idxprom477 = sext i32 %add4761430 to i64
  %arrayidx478 = getelementptr inbounds float* %VFtab, i64 %idxprom477
  %69 = load float* %arrayidx478, align 4, !tbaa !3
  %mul479 = fmul float %mul465, %69
  %add480 = fadd float %67, %mul475
  %add481 = fadd float %add480, %mul479
  %mul482 = fmul float %sub464, %add481
  %add483 = fadd float %66, %mul482
  %add484 = fadd float %mul475, %add481
  %mul485 = fmul float %mul479, 2.000000e+00
  %add486 = fadd float %mul485, %add484
  %mul487 = fmul float %mul8, %add483
  %mul488 = fmul float %mul8, %add486
  %mul489 = fmul float %mul488, %tabscale
  %70 = fmul float %conv207, %mul489
  %mul491 = fsub float -0.000000e+00, %70
  %add492 = fadd float %add450, %mul487
  %mul493 = fmul float %sub147, %mul491
  %mul494 = fmul float %sub148, %mul491
  %mul495 = fmul float %sub149, %mul491
  %add496 = fadd float %add454, %mul493
  %add497 = fadd float %add455, %mul494
  %add498 = fadd float %add456, %mul495
  %sub499 = fsub float %sub367, %mul493
  %sub500 = fsub float %sub371, %mul494
  %sub501 = fsub float %sub375, %mul495
  %mul502 = fmul float %add162, %conv187
  %mul503 = fmul float %mul502, %tabscale
  %conv504 = fptosi float %mul503 to i32
  %conv505 = sitofp i32 %conv504 to float
  %sub506 = fsub float %mul503, %conv505
  %mul507 = fmul float %sub506, %sub506
  %mul508 = shl nsw i32 %conv504, 2
  %idxprom509 = sext i32 %mul508 to i64
  %arrayidx510 = getelementptr inbounds float* %VFtab, i64 %idxprom509
  %71 = load float* %arrayidx510, align 4, !tbaa !3
  %add5111431 = or i32 %mul508, 1
  %idxprom512 = sext i32 %add5111431 to i64
  %arrayidx513 = getelementptr inbounds float* %VFtab, i64 %idxprom512
  %72 = load float* %arrayidx513, align 4, !tbaa !3
  %add5141432 = or i32 %mul508, 2
  %idxprom515 = sext i32 %add5141432 to i64
  %arrayidx516 = getelementptr inbounds float* %VFtab, i64 %idxprom515
  %73 = load float* %arrayidx516, align 4, !tbaa !3
  %mul517 = fmul float %sub506, %73
  %add5181433 = or i32 %mul508, 3
  %idxprom519 = sext i32 %add5181433 to i64
  %arrayidx520 = getelementptr inbounds float* %VFtab, i64 %idxprom519
  %74 = load float* %arrayidx520, align 4, !tbaa !3
  %mul521 = fmul float %mul507, %74
  %add522 = fadd float %72, %mul517
  %add523 = fadd float %add522, %mul521
  %mul524 = fmul float %sub506, %add523
  %add525 = fadd float %71, %mul524
  %add526 = fadd float %mul517, %add523
  %mul527 = fmul float %mul521, 2.000000e+00
  %add528 = fadd float %mul527, %add526
  %mul529 = fmul float %mul6, %add525
  %mul530 = fmul float %mul6, %add528
  %mul531 = fmul float %mul530, %tabscale
  %75 = fmul float %conv187, %mul531
  %mul533 = fsub float -0.000000e+00, %75
  %add534 = fadd float %add492, %mul529
  %mul535 = fmul float %sub155, %mul533
  %mul536 = fmul float %sub156, %mul533
  %mul537 = fmul float %sub157, %mul533
  %add538 = fadd float %fix3.01443, %mul535
  %add539 = fadd float %fiy3.01442, %mul536
  %add540 = fadd float %fiz3.01441, %mul537
  %sub541 = fsub float %sub415, %mul535
  store float %sub541, float* %arrayidx264, align 4, !tbaa !3
  %sub544 = fsub float %sub416, %mul536
  store float %sub544, float* %arrayidx268, align 4, !tbaa !3
  %sub548 = fsub float %sub417, %mul537
  store float %sub548, float* %arrayidx272, align 4, !tbaa !3
  %mul552 = fmul float %add170, %conv199
  %mul553 = fmul float %mul552, %tabscale
  %conv554 = fptosi float %mul553 to i32
  %conv555 = sitofp i32 %conv554 to float
  %sub556 = fsub float %mul553, %conv555
  %mul557 = fmul float %sub556, %sub556
  %mul558 = shl nsw i32 %conv554, 2
  %idxprom559 = sext i32 %mul558 to i64
  %arrayidx560 = getelementptr inbounds float* %VFtab, i64 %idxprom559
  %76 = load float* %arrayidx560, align 4, !tbaa !3
  %add5611434 = or i32 %mul558, 1
  %idxprom562 = sext i32 %add5611434 to i64
  %arrayidx563 = getelementptr inbounds float* %VFtab, i64 %idxprom562
  %77 = load float* %arrayidx563, align 4, !tbaa !3
  %add5641435 = or i32 %mul558, 2
  %idxprom565 = sext i32 %add5641435 to i64
  %arrayidx566 = getelementptr inbounds float* %VFtab, i64 %idxprom565
  %78 = load float* %arrayidx566, align 4, !tbaa !3
  %mul567 = fmul float %sub556, %78
  %add5681436 = or i32 %mul558, 3
  %idxprom569 = sext i32 %add5681436 to i64
  %arrayidx570 = getelementptr inbounds float* %VFtab, i64 %idxprom569
  %79 = load float* %arrayidx570, align 4, !tbaa !3
  %mul571 = fmul float %mul557, %79
  %add572 = fadd float %77, %mul567
  %add573 = fadd float %add572, %mul571
  %mul574 = fmul float %sub556, %add573
  %add575 = fadd float %76, %mul574
  %add576 = fadd float %mul567, %add573
  %mul577 = fmul float %mul571, 2.000000e+00
  %add578 = fadd float %mul577, %add576
  %mul579 = fmul float %mul8, %add575
  %mul580 = fmul float %mul8, %add578
  %mul581 = fmul float %mul580, %tabscale
  %80 = fmul float %conv199, %mul581
  %mul583 = fsub float -0.000000e+00, %80
  %add584 = fadd float %add534, %mul579
  %mul585 = fmul float %sub163, %mul583
  %mul586 = fmul float %sub164, %mul583
  %mul587 = fmul float %sub165, %mul583
  %add588 = fadd float %add538, %mul585
  %add589 = fadd float %add539, %mul586
  %add590 = fadd float %add540, %mul587
  %sub591 = fsub float %sub457, %mul585
  store float %sub591, float* %arrayidx315, align 4, !tbaa !3
  %sub595 = fsub float %sub458, %mul586
  store float %sub595, float* %arrayidx319, align 4, !tbaa !3
  %sub599 = fsub float %sub459, %mul587
  store float %sub599, float* %arrayidx323, align 4, !tbaa !3
  %mul603 = fmul float %add178, %conv211
  %mul604 = fmul float %mul603, %tabscale
  %conv605 = fptosi float %mul604 to i32
  %conv606 = sitofp i32 %conv605 to float
  %sub607 = fsub float %mul604, %conv606
  %mul608 = fmul float %sub607, %sub607
  %mul609 = shl nsw i32 %conv605, 2
  %idxprom610 = sext i32 %mul609 to i64
  %arrayidx611 = getelementptr inbounds float* %VFtab, i64 %idxprom610
  %81 = load float* %arrayidx611, align 4, !tbaa !3
  %add6121437 = or i32 %mul609, 1
  %idxprom613 = sext i32 %add6121437 to i64
  %arrayidx614 = getelementptr inbounds float* %VFtab, i64 %idxprom613
  %82 = load float* %arrayidx614, align 4, !tbaa !3
  %add6151438 = or i32 %mul609, 2
  %idxprom616 = sext i32 %add6151438 to i64
  %arrayidx617 = getelementptr inbounds float* %VFtab, i64 %idxprom616
  %83 = load float* %arrayidx617, align 4, !tbaa !3
  %mul618 = fmul float %sub607, %83
  %add6191439 = or i32 %mul609, 3
  %idxprom620 = sext i32 %add6191439 to i64
  %arrayidx621 = getelementptr inbounds float* %VFtab, i64 %idxprom620
  %84 = load float* %arrayidx621, align 4, !tbaa !3
  %mul622 = fmul float %mul608, %84
  %add623 = fadd float %82, %mul618
  %add624 = fadd float %add623, %mul622
  %mul625 = fmul float %sub607, %add624
  %add626 = fadd float %81, %mul625
  %add627 = fadd float %mul618, %add624
  %mul628 = fmul float %mul622, 2.000000e+00
  %add629 = fadd float %mul628, %add627
  %mul630 = fmul float %mul8, %add626
  %mul631 = fmul float %mul8, %add629
  %mul632 = fmul float %mul631, %tabscale
  %85 = fmul float %conv211, %mul632
  %mul634 = fsub float -0.000000e+00, %85
  %add635 = fadd float %add584, %mul630
  %mul636 = fmul float %sub171, %mul634
  %mul637 = fmul float %sub172, %mul634
  %mul638 = fmul float %sub173, %mul634
  %add639 = fadd float %add588, %mul636
  %add640 = fadd float %add589, %mul637
  %add641 = fadd float %add590, %mul638
  %sub642 = fsub float %sub499, %mul636
  store float %sub642, float* %arrayidx366, align 4, !tbaa !3
  %sub646 = fsub float %sub500, %mul637
  store float %sub646, float* %arrayidx370, align 4, !tbaa !3
  %sub650 = fsub float %sub501, %mul638
  store float %sub650, float* %arrayidx374, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %86 = trunc i64 %indvars.iv.next to i32
  %cmp77 = icmp slt i32 %86, %12
  br i1 %cmp77, label %for.body78, label %for.end

for.end:                                          ; preds = %for.body78, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add635, %for.body78 ]
  %vnbtot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %sub220, %for.body78 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add361, %for.body78 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add362, %for.body78 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add363, %for.body78 ]
  %fix2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add496, %for.body78 ]
  %fiy2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add497, %for.body78 ]
  %fiz2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add498, %for.body78 ]
  %fix3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add639, %for.body78 ]
  %fiy3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add640, %for.body78 ]
  %fiz3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add641, %for.body78 ]
  %arrayidx655 = getelementptr inbounds float* %faction, i64 %idxprom41
  %87 = load float* %arrayidx655, align 4, !tbaa !3
  %add656 = fadd float %fix1.0.lcssa, %87
  store float %add656, float* %arrayidx655, align 4, !tbaa !3
  %arrayidx661 = getelementptr inbounds float* %faction, i64 %idxprom45
  %88 = load float* %arrayidx661, align 4, !tbaa !3
  %add662 = fadd float %fiy1.0.lcssa, %88
  store float %add662, float* %arrayidx661, align 4, !tbaa !3
  %arrayidx668 = getelementptr inbounds float* %faction, i64 %idxprom49
  %89 = load float* %arrayidx668, align 4, !tbaa !3
  %add669 = fadd float %fiz1.0.lcssa, %89
  store float %add669, float* %arrayidx668, align 4, !tbaa !3
  %arrayidx675 = getelementptr inbounds float* %faction, i64 %idxprom53
  %90 = load float* %arrayidx675, align 4, !tbaa !3
  %add676 = fadd float %fix2.0.lcssa, %90
  store float %add676, float* %arrayidx675, align 4, !tbaa !3
  %arrayidx682 = getelementptr inbounds float* %faction, i64 %idxprom57
  %91 = load float* %arrayidx682, align 4, !tbaa !3
  %add683 = fadd float %fiy2.0.lcssa, %91
  store float %add683, float* %arrayidx682, align 4, !tbaa !3
  %arrayidx689 = getelementptr inbounds float* %faction, i64 %idxprom61
  %92 = load float* %arrayidx689, align 4, !tbaa !3
  %add690 = fadd float %fiz2.0.lcssa, %92
  store float %add690, float* %arrayidx689, align 4, !tbaa !3
  %arrayidx696 = getelementptr inbounds float* %faction, i64 %idxprom65
  %93 = load float* %arrayidx696, align 4, !tbaa !3
  %add697 = fadd float %fix3.0.lcssa, %93
  store float %add697, float* %arrayidx696, align 4, !tbaa !3
  %arrayidx703 = getelementptr inbounds float* %faction, i64 %idxprom69
  %94 = load float* %arrayidx703, align 4, !tbaa !3
  %add704 = fadd float %fiy3.0.lcssa, %94
  store float %add704, float* %arrayidx703, align 4, !tbaa !3
  %arrayidx710 = getelementptr inbounds float* %faction, i64 %idxprom73
  %95 = load float* %arrayidx710, align 4, !tbaa !3
  %add711 = fadd float %fiz3.0.lcssa, %95
  store float %add711, float* %arrayidx710, align 4, !tbaa !3
  %arrayidx716 = getelementptr inbounds float* %fshift, i64 %idxprom25
  %96 = load float* %arrayidx716, align 4, !tbaa !3
  %add717 = fadd float %fix1.0.lcssa, %96
  %add718 = fadd float %fix2.0.lcssa, %add717
  %add719 = fadd float %fix3.0.lcssa, %add718
  store float %add719, float* %arrayidx716, align 4, !tbaa !3
  %arrayidx724 = getelementptr inbounds float* %fshift, i64 %idxprom28
  %97 = load float* %arrayidx724, align 4, !tbaa !3
  %add725 = fadd float %fiy1.0.lcssa, %97
  %add726 = fadd float %fiy2.0.lcssa, %add725
  %add727 = fadd float %fiy3.0.lcssa, %add726
  store float %add727, float* %arrayidx724, align 4, !tbaa !3
  %arrayidx733 = getelementptr inbounds float* %fshift, i64 %idxprom31
  %98 = load float* %arrayidx733, align 4, !tbaa !3
  %add734 = fadd float %fiz1.0.lcssa, %98
  %add735 = fadd float %fiz2.0.lcssa, %add734
  %add736 = fadd float %fiz3.0.lcssa, %add735
  store float %add736, float* %arrayidx733, align 4, !tbaa !3
  %arrayidx741 = getelementptr inbounds i32* %gid, i64 %indvars.iv1465
  %99 = load i32* %arrayidx741, align 4, !tbaa !0
  %idxprom742 = sext i32 %99 to i64
  %arrayidx743 = getelementptr inbounds float* %Vc, i64 %idxprom742
  %100 = load float* %arrayidx743, align 4, !tbaa !3
  %add744 = fadd float %vctot.0.lcssa, %100
  store float %add744, float* %arrayidx743, align 4, !tbaa !3
  %arrayidx748 = getelementptr inbounds float* %Vnb, i64 %idxprom742
  %101 = load float* %arrayidx748, align 4, !tbaa !3
  %add749 = fadd float %vnbtot.0.lcssa, %101
  store float %add749, float* %arrayidx748, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next1466 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end754, label %for.end.for.body_crit_edge

for.end.for.body_crit_edge:                       ; preds = %for.end
  %arrayidx34.phi.trans.insert = getelementptr inbounds i32* %iinr, i64 %indvars.iv.next1466
  %.pre = load i32* %arrayidx34.phi.trans.insert, align 4, !tbaa !0
  br label %for.body

for.end754:                                       ; preds = %for.end, %entry
  ret void
}
