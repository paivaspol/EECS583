define void @inl3030(i32 %nri, i32* nocapture %iinr, i32* nocapture %jindex, i32* nocapture %jjnr, i32* nocapture %shift, float* nocapture %shiftvec, float* nocapture %fshift, i32* nocapture %gid, float* nocapture %pos, float* nocapture %faction, float* nocapture %charge, float %facel, float* nocapture %Vc, float %tabscale, float* %VFtab) #0 {
entry:
  %0 = load i32* %iinr, align 4, !tbaa !0
  %idxprom = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds float* %charge, i64 %idxprom
  %1 = load float* %arrayidx1, align 4, !tbaa !3
  %add = add nsw i32 %0, 1
  %idxprom2 = sext i32 %add to i64
  %arrayidx3 = getelementptr inbounds float* %charge, i64 %idxprom2
  %2 = load float* %arrayidx3, align 4, !tbaa !3
  %mul = fmul float %1, %facel
  %mul4 = fmul float %1, %mul
  %mul6 = fmul float %mul, %2
  %mul7 = fmul float %2, %facel
  %mul8 = fmul float %2, %mul7
  %cmp1412 = icmp sgt i32 %nri, 0
  br i1 %cmp1412, label %for.body, label %for.end724

for.body:                                         ; preds = %entry, %for.end.for.body_crit_edge
  %3 = phi i32 [ %.pre, %for.end.for.body_crit_edge ], [ %0, %entry ]
  %indvars.iv1414 = phi i64 [ %indvars.iv.next1415, %for.end.for.body_crit_edge ], [ 0, %entry ]
  %arrayidx10 = getelementptr inbounds i32* %shift, i64 %indvars.iv1414
  %4 = load i32* %arrayidx10, align 4, !tbaa !0
  %mul11 = mul nsw i32 %4, 3
  %idxprom12 = sext i32 %mul11 to i64
  %arrayidx13 = getelementptr inbounds float* %shiftvec, i64 %idxprom12
  %5 = load float* %arrayidx13, align 4, !tbaa !3
  %add14 = add nsw i32 %mul11, 1
  %idxprom15 = sext i32 %add14 to i64
  %arrayidx16 = getelementptr inbounds float* %shiftvec, i64 %idxprom15
  %6 = load float* %arrayidx16, align 4, !tbaa !3
  %add17 = add nsw i32 %mul11, 2
  %idxprom18 = sext i32 %add17 to i64
  %arrayidx19 = getelementptr inbounds float* %shiftvec, i64 %idxprom18
  %7 = load float* %arrayidx19, align 4, !tbaa !3
  %mul22 = mul nsw i32 %3, 3
  %arrayidx24 = getelementptr inbounds i32* %jindex, i64 %indvars.iv1414
  %8 = load i32* %arrayidx24, align 4, !tbaa !0
  %indvars.iv.next1415 = add i64 %indvars.iv1414, 1
  %arrayidx27 = getelementptr inbounds i32* %jindex, i64 %indvars.iv.next1415
  %9 = load i32* %arrayidx27, align 4, !tbaa !0
  %idxprom28 = sext i32 %mul22 to i64
  %arrayidx29 = getelementptr inbounds float* %pos, i64 %idxprom28
  %10 = load float* %arrayidx29, align 4, !tbaa !3
  %add30 = fadd float %5, %10
  %add31 = add nsw i32 %mul22, 1
  %idxprom32 = sext i32 %add31 to i64
  %arrayidx33 = getelementptr inbounds float* %pos, i64 %idxprom32
  %11 = load float* %arrayidx33, align 4, !tbaa !3
  %add34 = fadd float %6, %11
  %add35 = add nsw i32 %mul22, 2
  %idxprom36 = sext i32 %add35 to i64
  %arrayidx37 = getelementptr inbounds float* %pos, i64 %idxprom36
  %12 = load float* %arrayidx37, align 4, !tbaa !3
  %add38 = fadd float %7, %12
  %add39 = add nsw i32 %mul22, 3
  %idxprom40 = sext i32 %add39 to i64
  %arrayidx41 = getelementptr inbounds float* %pos, i64 %idxprom40
  %13 = load float* %arrayidx41, align 4, !tbaa !3
  %add42 = fadd float %5, %13
  %add43 = add nsw i32 %mul22, 4
  %idxprom44 = sext i32 %add43 to i64
  %arrayidx45 = getelementptr inbounds float* %pos, i64 %idxprom44
  %14 = load float* %arrayidx45, align 4, !tbaa !3
  %add46 = fadd float %6, %14
  %add47 = add nsw i32 %mul22, 5
  %idxprom48 = sext i32 %add47 to i64
  %arrayidx49 = getelementptr inbounds float* %pos, i64 %idxprom48
  %15 = load float* %arrayidx49, align 4, !tbaa !3
  %add50 = fadd float %7, %15
  %add51 = add nsw i32 %mul22, 6
  %idxprom52 = sext i32 %add51 to i64
  %arrayidx53 = getelementptr inbounds float* %pos, i64 %idxprom52
  %16 = load float* %arrayidx53, align 4, !tbaa !3
  %add54 = fadd float %5, %16
  %add55 = add nsw i32 %mul22, 7
  %idxprom56 = sext i32 %add55 to i64
  %arrayidx57 = getelementptr inbounds float* %pos, i64 %idxprom56
  %17 = load float* %arrayidx57, align 4, !tbaa !3
  %add58 = fadd float %6, %17
  %add59 = add nsw i32 %mul22, 8
  %idxprom60 = sext i32 %add59 to i64
  %arrayidx61 = getelementptr inbounds float* %pos, i64 %idxprom60
  %18 = load float* %arrayidx61, align 4, !tbaa !3
  %add62 = fadd float %7, %18
  %cmp641391 = icmp slt i32 %8, %9
  br i1 %cmp641391, label %for.body65.lr.ph, label %for.end

for.body65.lr.ph:                                 ; preds = %for.body
  %19 = sext i32 %8 to i64
  br label %for.body65

for.body65:                                       ; preds = %for.body65.lr.ph, %for.body65
  %indvars.iv = phi i64 [ %19, %for.body65.lr.ph ], [ %indvars.iv.next, %for.body65 ]
  %vctot.01401 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add610, %for.body65 ]
  %fix1.01400 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add336, %for.body65 ]
  %fiy1.01399 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add337, %for.body65 ]
  %fiz1.01398 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add338, %for.body65 ]
  %fix2.01397 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add471, %for.body65 ]
  %fiy2.01396 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add472, %for.body65 ]
  %fiz2.01395 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add473, %for.body65 ]
  %fix3.01394 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add614, %for.body65 ]
  %fiy3.01393 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add615, %for.body65 ]
  %fiz3.01392 = phi float [ 0.000000e+00, %for.body65.lr.ph ], [ %add616, %for.body65 ]
  %arrayidx67 = getelementptr inbounds i32* %jjnr, i64 %indvars.iv
  %20 = load i32* %arrayidx67, align 4, !tbaa !0
  %mul68 = mul nsw i32 %20, 3
  %idxprom69 = sext i32 %mul68 to i64
  %arrayidx70 = getelementptr inbounds float* %pos, i64 %idxprom69
  %21 = load float* %arrayidx70, align 4, !tbaa !3
  %add71 = add nsw i32 %mul68, 1
  %idxprom72 = sext i32 %add71 to i64
  %arrayidx73 = getelementptr inbounds float* %pos, i64 %idxprom72
  %22 = load float* %arrayidx73, align 4, !tbaa !3
  %add74 = add nsw i32 %mul68, 2
  %idxprom75 = sext i32 %add74 to i64
  %arrayidx76 = getelementptr inbounds float* %pos, i64 %idxprom75
  %23 = load float* %arrayidx76, align 4, !tbaa !3
  %add77 = add nsw i32 %mul68, 3
  %idxprom78 = sext i32 %add77 to i64
  %arrayidx79 = getelementptr inbounds float* %pos, i64 %idxprom78
  %24 = load float* %arrayidx79, align 4, !tbaa !3
  %add80 = add nsw i32 %mul68, 4
  %idxprom81 = sext i32 %add80 to i64
  %arrayidx82 = getelementptr inbounds float* %pos, i64 %idxprom81
  %25 = load float* %arrayidx82, align 4, !tbaa !3
  %add83 = add nsw i32 %mul68, 5
  %idxprom84 = sext i32 %add83 to i64
  %arrayidx85 = getelementptr inbounds float* %pos, i64 %idxprom84
  %26 = load float* %arrayidx85, align 4, !tbaa !3
  %add86 = add nsw i32 %mul68, 6
  %idxprom87 = sext i32 %add86 to i64
  %arrayidx88 = getelementptr inbounds float* %pos, i64 %idxprom87
  %27 = load float* %arrayidx88, align 4, !tbaa !3
  %add89 = add nsw i32 %mul68, 7
  %idxprom90 = sext i32 %add89 to i64
  %arrayidx91 = getelementptr inbounds float* %pos, i64 %idxprom90
  %28 = load float* %arrayidx91, align 4, !tbaa !3
  %add92 = add nsw i32 %mul68, 8
  %idxprom93 = sext i32 %add92 to i64
  %arrayidx94 = getelementptr inbounds float* %pos, i64 %idxprom93
  %29 = load float* %arrayidx94, align 4, !tbaa !3
  %sub = fsub float %add30, %21
  %sub95 = fsub float %add34, %22
  %sub96 = fsub float %add38, %23
  %mul97 = fmul float %sub, %sub
  %mul98 = fmul float %sub95, %sub95
  %add99 = fadd float %mul97, %mul98
  %mul100 = fmul float %sub96, %sub96
  %add101 = fadd float %add99, %mul100
  %sub102 = fsub float %add30, %24
  %sub103 = fsub float %add34, %25
  %sub104 = fsub float %add38, %26
  %mul105 = fmul float %sub102, %sub102
  %mul106 = fmul float %sub103, %sub103
  %add107 = fadd float %mul105, %mul106
  %mul108 = fmul float %sub104, %sub104
  %add109 = fadd float %add107, %mul108
  %sub110 = fsub float %add30, %27
  %sub111 = fsub float %add34, %28
  %sub112 = fsub float %add38, %29
  %mul113 = fmul float %sub110, %sub110
  %mul114 = fmul float %sub111, %sub111
  %add115 = fadd float %mul113, %mul114
  %mul116 = fmul float %sub112, %sub112
  %add117 = fadd float %add115, %mul116
  %sub118 = fsub float %add42, %21
  %sub119 = fsub float %add46, %22
  %sub120 = fsub float %add50, %23
  %mul121 = fmul float %sub118, %sub118
  %mul122 = fmul float %sub119, %sub119
  %add123 = fadd float %mul121, %mul122
  %mul124 = fmul float %sub120, %sub120
  %add125 = fadd float %add123, %mul124
  %sub126 = fsub float %add42, %24
  %sub127 = fsub float %add46, %25
  %sub128 = fsub float %add50, %26
  %mul129 = fmul float %sub126, %sub126
  %mul130 = fmul float %sub127, %sub127
  %add131 = fadd float %mul129, %mul130
  %mul132 = fmul float %sub128, %sub128
  %add133 = fadd float %add131, %mul132
  %sub134 = fsub float %add42, %27
  %sub135 = fsub float %add46, %28
  %sub136 = fsub float %add50, %29
  %mul137 = fmul float %sub134, %sub134
  %mul138 = fmul float %sub135, %sub135
  %add139 = fadd float %mul137, %mul138
  %mul140 = fmul float %sub136, %sub136
  %add141 = fadd float %add139, %mul140
  %sub142 = fsub float %add54, %21
  %sub143 = fsub float %add58, %22
  %sub144 = fsub float %add62, %23
  %mul145 = fmul float %sub142, %sub142
  %mul146 = fmul float %sub143, %sub143
  %add147 = fadd float %mul145, %mul146
  %mul148 = fmul float %sub144, %sub144
  %add149 = fadd float %add147, %mul148
  %sub150 = fsub float %add54, %24
  %sub151 = fsub float %add58, %25
  %sub152 = fsub float %add62, %26
  %mul153 = fmul float %sub150, %sub150
  %mul154 = fmul float %sub151, %sub151
  %add155 = fadd float %mul153, %mul154
  %mul156 = fmul float %sub152, %sub152
  %add157 = fadd float %add155, %mul156
  %sub158 = fsub float %add54, %27
  %sub159 = fsub float %add58, %28
  %sub160 = fsub float %add62, %29
  %mul161 = fmul float %sub158, %sub158
  %mul162 = fmul float %sub159, %sub159
  %add163 = fadd float %mul161, %mul162
  %mul164 = fmul float %sub160, %sub160
  %add165 = fadd float %add163, %mul164
  %conv = fpext float %add101 to double
  %call = tail call double @sqrt(double %conv) #2
  %div = fdiv double 1.000000e+00, %call
  %conv166 = fptrunc double %div to float
  %conv167 = fpext float %add125 to double
  %call168 = tail call double @sqrt(double %conv167) #2
  %div169 = fdiv double 1.000000e+00, %call168
  %conv170 = fptrunc double %div169 to float
  %conv171 = fpext float %add149 to double
  %call172 = tail call double @sqrt(double %conv171) #2
  %div173 = fdiv double 1.000000e+00, %call172
  %conv174 = fptrunc double %div173 to float
  %conv175 = fpext float %add109 to double
  %call176 = tail call double @sqrt(double %conv175) #2
  %div177 = fdiv double 1.000000e+00, %call176
  %conv178 = fptrunc double %div177 to float
  %conv179 = fpext float %add133 to double
  %call180 = tail call double @sqrt(double %conv179) #2
  %div181 = fdiv double 1.000000e+00, %call180
  %conv182 = fptrunc double %div181 to float
  %conv183 = fpext float %add157 to double
  %call184 = tail call double @sqrt(double %conv183) #2
  %div185 = fdiv double 1.000000e+00, %call184
  %conv186 = fptrunc double %div185 to float
  %conv187 = fpext float %add117 to double
  %call188 = tail call double @sqrt(double %conv187) #2
  %div189 = fdiv double 1.000000e+00, %call188
  %conv190 = fptrunc double %div189 to float
  %conv191 = fpext float %add141 to double
  %call192 = tail call double @sqrt(double %conv191) #2
  %div193 = fdiv double 1.000000e+00, %call192
  %conv194 = fptrunc double %div193 to float
  %conv195 = fpext float %add165 to double
  %call196 = tail call double @sqrt(double %conv195) #2
  %div197 = fdiv double 1.000000e+00, %call196
  %conv198 = fptrunc double %div197 to float
  %mul199 = fmul float %add101, %conv166
  %mul200 = fmul float %mul199, %tabscale
  %conv201 = fptosi float %mul200 to i32
  %conv202 = sitofp i32 %conv201 to float
  %sub203 = fsub float %mul200, %conv202
  %mul204 = fmul float %sub203, %sub203
  %mul205 = shl nsw i32 %conv201, 2
  %idxprom206 = sext i32 %mul205 to i64
  %arrayidx207 = getelementptr inbounds float* %VFtab, i64 %idxprom206
  %30 = load float* %arrayidx207, align 4, !tbaa !3
  %add2081364 = or i32 %mul205, 1
  %idxprom209 = sext i32 %add2081364 to i64
  %arrayidx210 = getelementptr inbounds float* %VFtab, i64 %idxprom209
  %31 = load float* %arrayidx210, align 4, !tbaa !3
  %add2111365 = or i32 %mul205, 2
  %idxprom212 = sext i32 %add2111365 to i64
  %arrayidx213 = getelementptr inbounds float* %VFtab, i64 %idxprom212
  %32 = load float* %arrayidx213, align 4, !tbaa !3
  %mul214 = fmul float %sub203, %32
  %add2151366 = or i32 %mul205, 3
  %idxprom216 = sext i32 %add2151366 to i64
  %arrayidx217 = getelementptr inbounds float* %VFtab, i64 %idxprom216
  %33 = load float* %arrayidx217, align 4, !tbaa !3
  %mul218 = fmul float %mul204, %33
  %add219 = fadd float %31, %mul214
  %add220 = fadd float %add219, %mul218
  %mul221 = fmul float %sub203, %add220
  %add222 = fadd float %30, %mul221
  %add223 = fadd float %mul214, %add220
  %mul224 = fmul float %mul218, 2.000000e+00
  %add225 = fadd float %mul224, %add223
  %mul226 = fmul float %mul4, %add222
  %mul227 = fmul float %mul4, %add225
  %mul228 = fmul float %mul227, %tabscale
  %34 = fmul float %conv166, %mul228
  %mul230 = fsub float -0.000000e+00, %34
  %add231 = fadd float %vctot.01401, %mul226
  %mul232 = fmul float %sub, %mul230
  %mul233 = fmul float %sub95, %mul230
  %mul234 = fmul float %sub96, %mul230
  %add235 = fadd float %fix1.01400, %mul232
  %add236 = fadd float %fiy1.01399, %mul233
  %add237 = fadd float %fiz1.01398, %mul234
  %arrayidx239 = getelementptr inbounds float* %faction, i64 %idxprom69
  %35 = load float* %arrayidx239, align 4, !tbaa !3
  %sub240 = fsub float %35, %mul232
  %arrayidx243 = getelementptr inbounds float* %faction, i64 %idxprom72
  %36 = load float* %arrayidx243, align 4, !tbaa !3
  %sub244 = fsub float %36, %mul233
  %arrayidx247 = getelementptr inbounds float* %faction, i64 %idxprom75
  %37 = load float* %arrayidx247, align 4, !tbaa !3
  %sub248 = fsub float %37, %mul234
  %mul249 = fmul float %add109, %conv178
  %mul250 = fmul float %mul249, %tabscale
  %conv251 = fptosi float %mul250 to i32
  %conv252 = sitofp i32 %conv251 to float
  %sub253 = fsub float %mul250, %conv252
  %mul254 = fmul float %sub253, %sub253
  %mul255 = shl nsw i32 %conv251, 2
  %idxprom256 = sext i32 %mul255 to i64
  %arrayidx257 = getelementptr inbounds float* %VFtab, i64 %idxprom256
  %38 = load float* %arrayidx257, align 4, !tbaa !3
  %add2581367 = or i32 %mul255, 1
  %idxprom259 = sext i32 %add2581367 to i64
  %arrayidx260 = getelementptr inbounds float* %VFtab, i64 %idxprom259
  %39 = load float* %arrayidx260, align 4, !tbaa !3
  %add2611368 = or i32 %mul255, 2
  %idxprom262 = sext i32 %add2611368 to i64
  %arrayidx263 = getelementptr inbounds float* %VFtab, i64 %idxprom262
  %40 = load float* %arrayidx263, align 4, !tbaa !3
  %mul264 = fmul float %sub253, %40
  %add2651369 = or i32 %mul255, 3
  %idxprom266 = sext i32 %add2651369 to i64
  %arrayidx267 = getelementptr inbounds float* %VFtab, i64 %idxprom266
  %41 = load float* %arrayidx267, align 4, !tbaa !3
  %mul268 = fmul float %mul254, %41
  %add269 = fadd float %39, %mul264
  %add270 = fadd float %add269, %mul268
  %mul271 = fmul float %sub253, %add270
  %add272 = fadd float %38, %mul271
  %add273 = fadd float %mul264, %add270
  %mul274 = fmul float %mul268, 2.000000e+00
  %add275 = fadd float %mul274, %add273
  %mul276 = fmul float %mul6, %add272
  %mul277 = fmul float %mul6, %add275
  %mul278 = fmul float %mul277, %tabscale
  %42 = fmul float %conv178, %mul278
  %mul280 = fsub float -0.000000e+00, %42
  %add281 = fadd float %add231, %mul276
  %mul282 = fmul float %sub102, %mul280
  %mul283 = fmul float %sub103, %mul280
  %mul284 = fmul float %sub104, %mul280
  %add285 = fadd float %add235, %mul282
  %add286 = fadd float %add236, %mul283
  %add287 = fadd float %add237, %mul284
  %arrayidx290 = getelementptr inbounds float* %faction, i64 %idxprom78
  %43 = load float* %arrayidx290, align 4, !tbaa !3
  %sub291 = fsub float %43, %mul282
  %arrayidx294 = getelementptr inbounds float* %faction, i64 %idxprom81
  %44 = load float* %arrayidx294, align 4, !tbaa !3
  %sub295 = fsub float %44, %mul283
  %arrayidx298 = getelementptr inbounds float* %faction, i64 %idxprom84
  %45 = load float* %arrayidx298, align 4, !tbaa !3
  %sub299 = fsub float %45, %mul284
  %mul300 = fmul float %add117, %conv190
  %mul301 = fmul float %mul300, %tabscale
  %conv302 = fptosi float %mul301 to i32
  %conv303 = sitofp i32 %conv302 to float
  %sub304 = fsub float %mul301, %conv303
  %mul305 = fmul float %sub304, %sub304
  %mul306 = shl nsw i32 %conv302, 2
  %idxprom307 = sext i32 %mul306 to i64
  %arrayidx308 = getelementptr inbounds float* %VFtab, i64 %idxprom307
  %46 = load float* %arrayidx308, align 4, !tbaa !3
  %add3091370 = or i32 %mul306, 1
  %idxprom310 = sext i32 %add3091370 to i64
  %arrayidx311 = getelementptr inbounds float* %VFtab, i64 %idxprom310
  %47 = load float* %arrayidx311, align 4, !tbaa !3
  %add3121371 = or i32 %mul306, 2
  %idxprom313 = sext i32 %add3121371 to i64
  %arrayidx314 = getelementptr inbounds float* %VFtab, i64 %idxprom313
  %48 = load float* %arrayidx314, align 4, !tbaa !3
  %mul315 = fmul float %sub304, %48
  %add3161372 = or i32 %mul306, 3
  %idxprom317 = sext i32 %add3161372 to i64
  %arrayidx318 = getelementptr inbounds float* %VFtab, i64 %idxprom317
  %49 = load float* %arrayidx318, align 4, !tbaa !3
  %mul319 = fmul float %mul305, %49
  %add320 = fadd float %47, %mul315
  %add321 = fadd float %add320, %mul319
  %mul322 = fmul float %sub304, %add321
  %add323 = fadd float %46, %mul322
  %add324 = fadd float %mul315, %add321
  %mul325 = fmul float %mul319, 2.000000e+00
  %add326 = fadd float %mul325, %add324
  %mul327 = fmul float %mul6, %add323
  %mul328 = fmul float %mul6, %add326
  %mul329 = fmul float %mul328, %tabscale
  %50 = fmul float %conv190, %mul329
  %mul331 = fsub float -0.000000e+00, %50
  %add332 = fadd float %add281, %mul327
  %mul333 = fmul float %sub110, %mul331
  %mul334 = fmul float %sub111, %mul331
  %mul335 = fmul float %sub112, %mul331
  %add336 = fadd float %add285, %mul333
  %add337 = fadd float %add286, %mul334
  %add338 = fadd float %add287, %mul335
  %arrayidx341 = getelementptr inbounds float* %faction, i64 %idxprom87
  %51 = load float* %arrayidx341, align 4, !tbaa !3
  %sub342 = fsub float %51, %mul333
  %arrayidx345 = getelementptr inbounds float* %faction, i64 %idxprom90
  %52 = load float* %arrayidx345, align 4, !tbaa !3
  %sub346 = fsub float %52, %mul334
  %arrayidx349 = getelementptr inbounds float* %faction, i64 %idxprom93
  %53 = load float* %arrayidx349, align 4, !tbaa !3
  %sub350 = fsub float %53, %mul335
  %mul351 = fmul float %add125, %conv170
  %mul352 = fmul float %mul351, %tabscale
  %conv353 = fptosi float %mul352 to i32
  %conv354 = sitofp i32 %conv353 to float
  %sub355 = fsub float %mul352, %conv354
  %mul356 = fmul float %sub355, %sub355
  %mul357 = shl nsw i32 %conv353, 2
  %idxprom358 = sext i32 %mul357 to i64
  %arrayidx359 = getelementptr inbounds float* %VFtab, i64 %idxprom358
  %54 = load float* %arrayidx359, align 4, !tbaa !3
  %add3601373 = or i32 %mul357, 1
  %idxprom361 = sext i32 %add3601373 to i64
  %arrayidx362 = getelementptr inbounds float* %VFtab, i64 %idxprom361
  %55 = load float* %arrayidx362, align 4, !tbaa !3
  %add3631374 = or i32 %mul357, 2
  %idxprom364 = sext i32 %add3631374 to i64
  %arrayidx365 = getelementptr inbounds float* %VFtab, i64 %idxprom364
  %56 = load float* %arrayidx365, align 4, !tbaa !3
  %mul366 = fmul float %sub355, %56
  %add3671375 = or i32 %mul357, 3
  %idxprom368 = sext i32 %add3671375 to i64
  %arrayidx369 = getelementptr inbounds float* %VFtab, i64 %idxprom368
  %57 = load float* %arrayidx369, align 4, !tbaa !3
  %mul370 = fmul float %mul356, %57
  %add371 = fadd float %55, %mul366
  %add372 = fadd float %add371, %mul370
  %mul373 = fmul float %sub355, %add372
  %add374 = fadd float %54, %mul373
  %add375 = fadd float %mul366, %add372
  %mul376 = fmul float %mul370, 2.000000e+00
  %add377 = fadd float %mul376, %add375
  %mul378 = fmul float %mul6, %add374
  %mul379 = fmul float %mul6, %add377
  %mul380 = fmul float %mul379, %tabscale
  %58 = fmul float %conv170, %mul380
  %mul382 = fsub float -0.000000e+00, %58
  %add383 = fadd float %add332, %mul378
  %mul384 = fmul float %sub118, %mul382
  %mul385 = fmul float %sub119, %mul382
  %mul386 = fmul float %sub120, %mul382
  %add387 = fadd float %fix2.01397, %mul384
  %add388 = fadd float %fiy2.01396, %mul385
  %add389 = fadd float %fiz2.01395, %mul386
  %sub390 = fsub float %sub240, %mul384
  %sub391 = fsub float %sub244, %mul385
  %sub392 = fsub float %sub248, %mul386
  %mul393 = fmul float %add133, %conv182
  %mul394 = fmul float %mul393, %tabscale
  %conv395 = fptosi float %mul394 to i32
  %conv396 = sitofp i32 %conv395 to float
  %sub397 = fsub float %mul394, %conv396
  %mul398 = fmul float %sub397, %sub397
  %mul399 = shl nsw i32 %conv395, 2
  %idxprom400 = sext i32 %mul399 to i64
  %arrayidx401 = getelementptr inbounds float* %VFtab, i64 %idxprom400
  %59 = load float* %arrayidx401, align 4, !tbaa !3
  %add4021376 = or i32 %mul399, 1
  %idxprom403 = sext i32 %add4021376 to i64
  %arrayidx404 = getelementptr inbounds float* %VFtab, i64 %idxprom403
  %60 = load float* %arrayidx404, align 4, !tbaa !3
  %add4051377 = or i32 %mul399, 2
  %idxprom406 = sext i32 %add4051377 to i64
  %arrayidx407 = getelementptr inbounds float* %VFtab, i64 %idxprom406
  %61 = load float* %arrayidx407, align 4, !tbaa !3
  %mul408 = fmul float %sub397, %61
  %add4091378 = or i32 %mul399, 3
  %idxprom410 = sext i32 %add4091378 to i64
  %arrayidx411 = getelementptr inbounds float* %VFtab, i64 %idxprom410
  %62 = load float* %arrayidx411, align 4, !tbaa !3
  %mul412 = fmul float %mul398, %62
  %add413 = fadd float %60, %mul408
  %add414 = fadd float %add413, %mul412
  %mul415 = fmul float %sub397, %add414
  %add416 = fadd float %59, %mul415
  %add417 = fadd float %mul408, %add414
  %mul418 = fmul float %mul412, 2.000000e+00
  %add419 = fadd float %mul418, %add417
  %mul420 = fmul float %mul8, %add416
  %mul421 = fmul float %mul8, %add419
  %mul422 = fmul float %mul421, %tabscale
  %63 = fmul float %conv182, %mul422
  %mul424 = fsub float -0.000000e+00, %63
  %add425 = fadd float %add383, %mul420
  %mul426 = fmul float %sub126, %mul424
  %mul427 = fmul float %sub127, %mul424
  %mul428 = fmul float %sub128, %mul424
  %add429 = fadd float %add387, %mul426
  %add430 = fadd float %add388, %mul427
  %add431 = fadd float %add389, %mul428
  %sub432 = fsub float %sub291, %mul426
  %sub433 = fsub float %sub295, %mul427
  %sub434 = fsub float %sub299, %mul428
  %mul435 = fmul float %add141, %conv194
  %mul436 = fmul float %mul435, %tabscale
  %conv437 = fptosi float %mul436 to i32
  %conv438 = sitofp i32 %conv437 to float
  %sub439 = fsub float %mul436, %conv438
  %mul440 = fmul float %sub439, %sub439
  %mul441 = shl nsw i32 %conv437, 2
  %idxprom442 = sext i32 %mul441 to i64
  %arrayidx443 = getelementptr inbounds float* %VFtab, i64 %idxprom442
  %64 = load float* %arrayidx443, align 4, !tbaa !3
  %add4441379 = or i32 %mul441, 1
  %idxprom445 = sext i32 %add4441379 to i64
  %arrayidx446 = getelementptr inbounds float* %VFtab, i64 %idxprom445
  %65 = load float* %arrayidx446, align 4, !tbaa !3
  %add4471380 = or i32 %mul441, 2
  %idxprom448 = sext i32 %add4471380 to i64
  %arrayidx449 = getelementptr inbounds float* %VFtab, i64 %idxprom448
  %66 = load float* %arrayidx449, align 4, !tbaa !3
  %mul450 = fmul float %sub439, %66
  %add4511381 = or i32 %mul441, 3
  %idxprom452 = sext i32 %add4511381 to i64
  %arrayidx453 = getelementptr inbounds float* %VFtab, i64 %idxprom452
  %67 = load float* %arrayidx453, align 4, !tbaa !3
  %mul454 = fmul float %mul440, %67
  %add455 = fadd float %65, %mul450
  %add456 = fadd float %add455, %mul454
  %mul457 = fmul float %sub439, %add456
  %add458 = fadd float %64, %mul457
  %add459 = fadd float %mul450, %add456
  %mul460 = fmul float %mul454, 2.000000e+00
  %add461 = fadd float %mul460, %add459
  %mul462 = fmul float %mul8, %add458
  %mul463 = fmul float %mul8, %add461
  %mul464 = fmul float %mul463, %tabscale
  %68 = fmul float %conv194, %mul464
  %mul466 = fsub float -0.000000e+00, %68
  %add467 = fadd float %add425, %mul462
  %mul468 = fmul float %sub134, %mul466
  %mul469 = fmul float %sub135, %mul466
  %mul470 = fmul float %sub136, %mul466
  %add471 = fadd float %add429, %mul468
  %add472 = fadd float %add430, %mul469
  %add473 = fadd float %add431, %mul470
  %sub474 = fsub float %sub342, %mul468
  %sub475 = fsub float %sub346, %mul469
  %sub476 = fsub float %sub350, %mul470
  %mul477 = fmul float %add149, %conv174
  %mul478 = fmul float %mul477, %tabscale
  %conv479 = fptosi float %mul478 to i32
  %conv480 = sitofp i32 %conv479 to float
  %sub481 = fsub float %mul478, %conv480
  %mul482 = fmul float %sub481, %sub481
  %mul483 = shl nsw i32 %conv479, 2
  %idxprom484 = sext i32 %mul483 to i64
  %arrayidx485 = getelementptr inbounds float* %VFtab, i64 %idxprom484
  %69 = load float* %arrayidx485, align 4, !tbaa !3
  %add4861382 = or i32 %mul483, 1
  %idxprom487 = sext i32 %add4861382 to i64
  %arrayidx488 = getelementptr inbounds float* %VFtab, i64 %idxprom487
  %70 = load float* %arrayidx488, align 4, !tbaa !3
  %add4891383 = or i32 %mul483, 2
  %idxprom490 = sext i32 %add4891383 to i64
  %arrayidx491 = getelementptr inbounds float* %VFtab, i64 %idxprom490
  %71 = load float* %arrayidx491, align 4, !tbaa !3
  %mul492 = fmul float %sub481, %71
  %add4931384 = or i32 %mul483, 3
  %idxprom494 = sext i32 %add4931384 to i64
  %arrayidx495 = getelementptr inbounds float* %VFtab, i64 %idxprom494
  %72 = load float* %arrayidx495, align 4, !tbaa !3
  %mul496 = fmul float %mul482, %72
  %add497 = fadd float %70, %mul492
  %add498 = fadd float %add497, %mul496
  %mul499 = fmul float %sub481, %add498
  %add500 = fadd float %69, %mul499
  %add501 = fadd float %mul492, %add498
  %mul502 = fmul float %mul496, 2.000000e+00
  %add503 = fadd float %mul502, %add501
  %mul504 = fmul float %mul6, %add500
  %mul505 = fmul float %mul6, %add503
  %mul506 = fmul float %mul505, %tabscale
  %73 = fmul float %conv174, %mul506
  %mul508 = fsub float -0.000000e+00, %73
  %add509 = fadd float %add467, %mul504
  %mul510 = fmul float %sub142, %mul508
  %mul511 = fmul float %sub143, %mul508
  %mul512 = fmul float %sub144, %mul508
  %add513 = fadd float %fix3.01394, %mul510
  %add514 = fadd float %fiy3.01393, %mul511
  %add515 = fadd float %fiz3.01392, %mul512
  %sub516 = fsub float %sub390, %mul510
  store float %sub516, float* %arrayidx239, align 4, !tbaa !3
  %sub519 = fsub float %sub391, %mul511
  store float %sub519, float* %arrayidx243, align 4, !tbaa !3
  %sub523 = fsub float %sub392, %mul512
  store float %sub523, float* %arrayidx247, align 4, !tbaa !3
  %mul527 = fmul float %add157, %conv186
  %mul528 = fmul float %mul527, %tabscale
  %conv529 = fptosi float %mul528 to i32
  %conv530 = sitofp i32 %conv529 to float
  %sub531 = fsub float %mul528, %conv530
  %mul532 = fmul float %sub531, %sub531
  %mul533 = shl nsw i32 %conv529, 2
  %idxprom534 = sext i32 %mul533 to i64
  %arrayidx535 = getelementptr inbounds float* %VFtab, i64 %idxprom534
  %74 = load float* %arrayidx535, align 4, !tbaa !3
  %add5361385 = or i32 %mul533, 1
  %idxprom537 = sext i32 %add5361385 to i64
  %arrayidx538 = getelementptr inbounds float* %VFtab, i64 %idxprom537
  %75 = load float* %arrayidx538, align 4, !tbaa !3
  %add5391386 = or i32 %mul533, 2
  %idxprom540 = sext i32 %add5391386 to i64
  %arrayidx541 = getelementptr inbounds float* %VFtab, i64 %idxprom540
  %76 = load float* %arrayidx541, align 4, !tbaa !3
  %mul542 = fmul float %sub531, %76
  %add5431387 = or i32 %mul533, 3
  %idxprom544 = sext i32 %add5431387 to i64
  %arrayidx545 = getelementptr inbounds float* %VFtab, i64 %idxprom544
  %77 = load float* %arrayidx545, align 4, !tbaa !3
  %mul546 = fmul float %mul532, %77
  %add547 = fadd float %75, %mul542
  %add548 = fadd float %add547, %mul546
  %mul549 = fmul float %sub531, %add548
  %add550 = fadd float %74, %mul549
  %add551 = fadd float %mul542, %add548
  %mul552 = fmul float %mul546, 2.000000e+00
  %add553 = fadd float %mul552, %add551
  %mul554 = fmul float %mul8, %add550
  %mul555 = fmul float %mul8, %add553
  %mul556 = fmul float %mul555, %tabscale
  %78 = fmul float %conv186, %mul556
  %mul558 = fsub float -0.000000e+00, %78
  %add559 = fadd float %add509, %mul554
  %mul560 = fmul float %sub150, %mul558
  %mul561 = fmul float %sub151, %mul558
  %mul562 = fmul float %sub152, %mul558
  %add563 = fadd float %add513, %mul560
  %add564 = fadd float %add514, %mul561
  %add565 = fadd float %add515, %mul562
  %sub566 = fsub float %sub432, %mul560
  store float %sub566, float* %arrayidx290, align 4, !tbaa !3
  %sub570 = fsub float %sub433, %mul561
  store float %sub570, float* %arrayidx294, align 4, !tbaa !3
  %sub574 = fsub float %sub434, %mul562
  store float %sub574, float* %arrayidx298, align 4, !tbaa !3
  %mul578 = fmul float %add165, %conv198
  %mul579 = fmul float %mul578, %tabscale
  %conv580 = fptosi float %mul579 to i32
  %conv581 = sitofp i32 %conv580 to float
  %sub582 = fsub float %mul579, %conv581
  %mul583 = fmul float %sub582, %sub582
  %mul584 = shl nsw i32 %conv580, 2
  %idxprom585 = sext i32 %mul584 to i64
  %arrayidx586 = getelementptr inbounds float* %VFtab, i64 %idxprom585
  %79 = load float* %arrayidx586, align 4, !tbaa !3
  %add5871388 = or i32 %mul584, 1
  %idxprom588 = sext i32 %add5871388 to i64
  %arrayidx589 = getelementptr inbounds float* %VFtab, i64 %idxprom588
  %80 = load float* %arrayidx589, align 4, !tbaa !3
  %add5901389 = or i32 %mul584, 2
  %idxprom591 = sext i32 %add5901389 to i64
  %arrayidx592 = getelementptr inbounds float* %VFtab, i64 %idxprom591
  %81 = load float* %arrayidx592, align 4, !tbaa !3
  %mul593 = fmul float %sub582, %81
  %add5941390 = or i32 %mul584, 3
  %idxprom595 = sext i32 %add5941390 to i64
  %arrayidx596 = getelementptr inbounds float* %VFtab, i64 %idxprom595
  %82 = load float* %arrayidx596, align 4, !tbaa !3
  %mul597 = fmul float %mul583, %82
  %add598 = fadd float %80, %mul593
  %add599 = fadd float %add598, %mul597
  %mul600 = fmul float %sub582, %add599
  %add601 = fadd float %79, %mul600
  %add602 = fadd float %mul593, %add599
  %mul603 = fmul float %mul597, 2.000000e+00
  %add604 = fadd float %mul603, %add602
  %mul605 = fmul float %mul8, %add601
  %mul606 = fmul float %mul8, %add604
  %mul607 = fmul float %mul606, %tabscale
  %83 = fmul float %conv198, %mul607
  %mul609 = fsub float -0.000000e+00, %83
  %add610 = fadd float %add559, %mul605
  %mul611 = fmul float %sub158, %mul609
  %mul612 = fmul float %sub159, %mul609
  %mul613 = fmul float %sub160, %mul609
  %add614 = fadd float %add563, %mul611
  %add615 = fadd float %add564, %mul612
  %add616 = fadd float %add565, %mul613
  %sub617 = fsub float %sub474, %mul611
  store float %sub617, float* %arrayidx341, align 4, !tbaa !3
  %sub621 = fsub float %sub475, %mul612
  store float %sub621, float* %arrayidx345, align 4, !tbaa !3
  %sub625 = fsub float %sub476, %mul613
  store float %sub625, float* %arrayidx349, align 4, !tbaa !3
  %indvars.iv.next = add i64 %indvars.iv, 1
  %84 = trunc i64 %indvars.iv.next to i32
  %cmp64 = icmp slt i32 %84, %9
  br i1 %cmp64, label %for.body65, label %for.end

for.end:                                          ; preds = %for.body65, %for.body
  %vctot.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add610, %for.body65 ]
  %fix1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add336, %for.body65 ]
  %fiy1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add337, %for.body65 ]
  %fiz1.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add338, %for.body65 ]
  %fix2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add471, %for.body65 ]
  %fiy2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add472, %for.body65 ]
  %fiz2.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add473, %for.body65 ]
  %fix3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add614, %for.body65 ]
  %fiy3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add615, %for.body65 ]
  %fiz3.0.lcssa = phi float [ 0.000000e+00, %for.body ], [ %add616, %for.body65 ]
  %arrayidx630 = getelementptr inbounds float* %faction, i64 %idxprom28
  %85 = load float* %arrayidx630, align 4, !tbaa !3
  %add631 = fadd float %fix1.0.lcssa, %85
  store float %add631, float* %arrayidx630, align 4, !tbaa !3
  %arrayidx636 = getelementptr inbounds float* %faction, i64 %idxprom32
  %86 = load float* %arrayidx636, align 4, !tbaa !3
  %add637 = fadd float %fiy1.0.lcssa, %86
  store float %add637, float* %arrayidx636, align 4, !tbaa !3
  %arrayidx643 = getelementptr inbounds float* %faction, i64 %idxprom36
  %87 = load float* %arrayidx643, align 4, !tbaa !3
  %add644 = fadd float %fiz1.0.lcssa, %87
  store float %add644, float* %arrayidx643, align 4, !tbaa !3
  %arrayidx650 = getelementptr inbounds float* %faction, i64 %idxprom40
  %88 = load float* %arrayidx650, align 4, !tbaa !3
  %add651 = fadd float %fix2.0.lcssa, %88
  store float %add651, float* %arrayidx650, align 4, !tbaa !3
  %arrayidx657 = getelementptr inbounds float* %faction, i64 %idxprom44
  %89 = load float* %arrayidx657, align 4, !tbaa !3
  %add658 = fadd float %fiy2.0.lcssa, %89
  store float %add658, float* %arrayidx657, align 4, !tbaa !3
  %arrayidx664 = getelementptr inbounds float* %faction, i64 %idxprom48
  %90 = load float* %arrayidx664, align 4, !tbaa !3
  %add665 = fadd float %fiz2.0.lcssa, %90
  store float %add665, float* %arrayidx664, align 4, !tbaa !3
  %arrayidx671 = getelementptr inbounds float* %faction, i64 %idxprom52
  %91 = load float* %arrayidx671, align 4, !tbaa !3
  %add672 = fadd float %fix3.0.lcssa, %91
  store float %add672, float* %arrayidx671, align 4, !tbaa !3
  %arrayidx678 = getelementptr inbounds float* %faction, i64 %idxprom56
  %92 = load float* %arrayidx678, align 4, !tbaa !3
  %add679 = fadd float %fiy3.0.lcssa, %92
  store float %add679, float* %arrayidx678, align 4, !tbaa !3
  %arrayidx685 = getelementptr inbounds float* %faction, i64 %idxprom60
  %93 = load float* %arrayidx685, align 4, !tbaa !3
  %add686 = fadd float %fiz3.0.lcssa, %93
  store float %add686, float* %arrayidx685, align 4, !tbaa !3
  %arrayidx691 = getelementptr inbounds float* %fshift, i64 %idxprom12
  %94 = load float* %arrayidx691, align 4, !tbaa !3
  %add692 = fadd float %fix1.0.lcssa, %94
  %add693 = fadd float %fix2.0.lcssa, %add692
  %add694 = fadd float %fix3.0.lcssa, %add693
  store float %add694, float* %arrayidx691, align 4, !tbaa !3
  %arrayidx699 = getelementptr inbounds float* %fshift, i64 %idxprom15
  %95 = load float* %arrayidx699, align 4, !tbaa !3
  %add700 = fadd float %fiy1.0.lcssa, %95
  %add701 = fadd float %fiy2.0.lcssa, %add700
  %add702 = fadd float %fiy3.0.lcssa, %add701
  store float %add702, float* %arrayidx699, align 4, !tbaa !3
  %arrayidx708 = getelementptr inbounds float* %fshift, i64 %idxprom18
  %96 = load float* %arrayidx708, align 4, !tbaa !3
  %add709 = fadd float %fiz1.0.lcssa, %96
  %add710 = fadd float %fiz2.0.lcssa, %add709
  %add711 = fadd float %fiz3.0.lcssa, %add710
  store float %add711, float* %arrayidx708, align 4, !tbaa !3
  %arrayidx716 = getelementptr inbounds i32* %gid, i64 %indvars.iv1414
  %97 = load i32* %arrayidx716, align 4, !tbaa !0
  %idxprom717 = sext i32 %97 to i64
  %arrayidx718 = getelementptr inbounds float* %Vc, i64 %idxprom717
  %98 = load float* %arrayidx718, align 4, !tbaa !3
  %add719 = fadd float %vctot.0.lcssa, %98
  store float %add719, float* %arrayidx718, align 4, !tbaa !3
  %lftr.wideiv = trunc i64 %indvars.iv.next1415 to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %nri
  br i1 %exitcond, label %for.end724, label %for.end.for.body_crit_edge

for.end.for.body_crit_edge:                       ; preds = %for.end
  %arrayidx21.phi.trans.insert = getelementptr inbounds i32* %iinr, i64 %indvars.iv.next1415
  %.pre = load i32* %arrayidx21.phi.trans.insert, align 4, !tbaa !0
  br label %for.body

for.end724:                                       ; preds = %for.end, %entry
  ret void
}
